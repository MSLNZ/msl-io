{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p><code>msl-io</code> follows the data model used by HDF5 to read and write data files \u2014 where there are Groups and Datasets and each item has Metadata.</p> <p></p> <p>The tree structure is similar to the file-system structure used by operating systems. Groups are analogous to directories (where Root is the root Group) and Datasets are analogous to files.</p> <p>The data files that can be read or written are not restricted to HDF5 files, but any file format that has a Reader implemented can be read and data files can be created using any of the Writers.</p>"},{"location":"#write-a-file","title":"Write a file","text":"<p>Suppose you want to create a new JSON file format. We first create an instance of JSONWriter</p> <pre><code>&gt;&gt;&gt; from msl.io import JSONWriter\n&gt;&gt;&gt; root = JSONWriter()\n</code></pre> <p>then we can add Metadata to the <code>root</code> Root,</p> <pre><code>&gt;&gt;&gt; root.add_metadata(one=1, two=2)\n</code></pre> <p>create a Dataset,</p> <pre><code>&gt;&gt;&gt; dataset1 = root.create_dataset(\"dataset1\", data=[1, 2, 3, 4])\n</code></pre> <p>create a Group,</p> <pre><code>&gt;&gt;&gt; my_group = root.create_group(\"my_group\")\n</code></pre> <p>and create a Dataset in <code>my_group</code></p> <pre><code>&gt;&gt;&gt; dataset2 = my_group.create_dataset(\"dataset2\", data=[[1, 2], [3, 4]], three=3)\n</code></pre> <p>If the parent Groups do not exist, they are created</p> <pre><code>&gt;&gt;&gt; dataset3 = root.create_dataset(\"group1/group2/group3/dataset3\", data=[9, 8, 7])\n</code></pre> <p>Finally, we write the file</p> <pre><code>&gt;&gt;&gt; root.write(file=\"my_file.json\")\n</code></pre> <p>Note</p> <p>The file is not created until you call the write (or save) method.</p>"},{"location":"#read-a-file","title":"Read a file","text":"<p>The read function is available to read a file. Provided that a Reader subclass has been implemented to read the file, the subclass instance is returned. We will now read the file that we created above</p> <pre><code>&gt;&gt;&gt; from msl.io import read\n&gt;&gt;&gt; root = read(\"my_file.json\")\n</code></pre> <p>You can print a tree representation of all Groups and Datasets in the Root by calling the tree method</p> <pre><code>&gt;&gt;&gt; print(root.tree())\n&lt;JSONReader 'my_file.json' (4 groups, 3 datasets, 2 metadata)&gt;\n  &lt;Dataset '/dataset1' shape=(4,) dtype='&lt;f8' (0 metadata)&gt;\n  &lt;Group '/group1' (2 groups, 1 datasets, 0 metadata)&gt;\n    &lt;Group '/group1/group2' (1 groups, 1 datasets, 0 metadata)&gt;\n      &lt;Group '/group1/group2/group3' (0 groups, 1 datasets, 0 metadata)&gt;\n        &lt;Dataset '/group1/group2/group3/dataset3' shape=(3,) dtype='&lt;f8' (0 metadata)&gt;\n  &lt;Group '/my_group' (0 groups, 1 datasets, 0 metadata)&gt;\n    &lt;Dataset '/my_group/dataset2' shape=(2, 2) dtype='&lt;f8' (1 metadata)&gt;\n</code></pre> <p>Since the <code>root</code> item is a Group instance (which operates like a Python dict) you can iterate over the items that are in the file</p> <pre><code>&gt;&gt;&gt; for name, node in root.items():\n...     print(f\"{name!r} -- {node!r}\")\n'/dataset1' -- &lt;Dataset '/dataset1' shape=(4,) dtype='&lt;f8' (0 metadata)&gt;\n'/my_group' -- &lt;Group '/my_group' (0 groups, 1 datasets, 0 metadata)&gt;\n'/my_group/dataset2' -- &lt;Dataset '/my_group/dataset2' shape=(2, 2) dtype='&lt;f8' (1 metadata)&gt;\n'/group1' -- &lt;Group '/group1' (2 groups, 1 datasets, 0 metadata)&gt;\n'/group1/group2' -- &lt;Group '/group1/group2' (1 groups, 1 datasets, 0 metadata)&gt;\n'/group1/group2/group3' -- &lt;Group '/group1/group2/group3' (0 groups, 1 datasets, 0 metadata)&gt;\n'/group1/group2/group3/dataset3' -- &lt;Dataset '/group1/group2/group3/dataset3' shape=(3,) dtype='&lt;f8' (0 metadata)&gt;\n</code></pre> <p>where node will either be a Group or a Dataset.</p> <p>You can iterate only over the Groups that are in the file</p> <pre><code>&gt;&gt;&gt; for group in root.groups():\n...     print(group)\n&lt;Group '/my_group' (0 groups, 1 datasets, 0 metadata)&gt;\n&lt;Group '/group1' (2 groups, 1 datasets, 0 metadata)&gt;\n&lt;Group '/group1/group2' (1 groups, 1 datasets, 0 metadata)&gt;\n&lt;Group '/group1/group2/group3' (0 groups, 1 datasets, 0 metadata)&gt;\n</code></pre> <p>or iterate over the Datasets</p> <pre><code>&gt;&gt;&gt; for dataset in root.datasets():\n...     print(repr(dataset))\n&lt;Dataset '/dataset1' shape=(4,) dtype='&lt;f8' (0 metadata)&gt;\n&lt;Dataset '/my_group/dataset2' shape=(2, 2) dtype='&lt;f8' (1 metadata)&gt;\n&lt;Dataset '/group1/group2/group3/dataset3' shape=(3,) dtype='&lt;f8' (0 metadata)&gt;\n</code></pre> <p>You can access the Metadata of any item through the <code>metadata</code> attribute</p> <pre><code>&gt;&gt;&gt; print(root.metadata)\n&lt;Metadata '/' {'one': 1, 'two': 2}&gt;\n</code></pre> <p>You can access values of the Metadata as attributes</p> <pre><code>&gt;&gt;&gt; dataset2.metadata.three\n3\n</code></pre> <p>or as keys</p> <pre><code>&gt;&gt;&gt; dataset2.metadata[\"three\"]\n3\n</code></pre> <p>When <code>root</code> is returned, it is accessed in read-only mode</p> <pre><code>&gt;&gt;&gt; root.read_only\nTrue\n&gt;&gt;&gt; for name, node in root.items():\n...     print(f\"is {name!r} in read-only mode? {node.read_only}\")\nis '/dataset1' in read-only mode? True\nis '/my_group' in read-only mode? True\nis '/my_group/dataset2' in read-only mode? True\nis '/group1' in read-only mode? True\nis '/group1/group2' in read-only mode? True\nis '/group1/group2/group3' in read-only mode? True\nis '/group1/group2/group3/dataset3' in read-only mode? True\n</code></pre> <p>If you want to edit the Metadata of <code>root</code>, or modify any Groups or Datasets in <code>root</code>, then you must first set the item to be writeable. Setting the read-only mode of <code>root</code> propagates that mode to all items within <code>root</code>. For example,</p> <pre><code>&gt;&gt;&gt; root.read_only = False\n</code></pre> <p>will make <code>root</code> and all sub-Groups and all sub-Datasets within <code>root</code> to be writeable</p> <pre><code>&gt;&gt;&gt; for name, node in root.items():\n...     print(f\"is {name!r} in read-only mode? {node.read_only}\")\nis '/dataset1' in read-only mode? False\nis '/my_group' in read-only mode? False\nis '/my_group/dataset2' in read-only mode? False\nis '/group1' in read-only mode? False\nis '/group1/group2' in read-only mode? False\nis '/group1/group2/group3' in read-only mode? False\nis '/group1/group2/group3/dataset3' in read-only mode? False\n</code></pre> <p>You can make only a specific item (and its descendants) writeable as well. You can make <code>my_group</code> and <code>dataset2</code> to be in read-only mode by the following (recall that <code>root</code> behaves like a Python dict)</p> <pre><code>&gt;&gt;&gt; root[\"my_group\"].read_only = True\n</code></pre> <p>and this will keep <code>root</code>, <code>dataset1</code> and all items contained within the <code>group1</code> Group to be in read-write mode, but change <code>my_group</code> and <code>dataset2</code> to be in read-only mode</p> <pre><code>&gt;&gt;&gt; root.read_only\nFalse\n&gt;&gt;&gt; for name, node in root.items():\n...     print(f\"is {name!r} in read-only mode? {node.read_only}\")\nis '/dataset1' in read-only mode? False\nis '/my_group' in read-only mode? True\nis '/my_group/dataset2' in read-only mode? True\nis '/group1' in read-only mode? False\nis '/group1/group2' in read-only mode? False\nis '/group1/group2/group3' in read-only mode? False\nis '/group1/group2/group3/dataset3' in read-only mode? False\n</code></pre> <p>You can access Groups and Datasets as keys or as class attributes</p> <pre><code>&gt;&gt;&gt; root[\"my_group\"][\"dataset2\"].shape\n(2, 2)\n&gt;&gt;&gt; root.my_group.dataset2.shape\n(2, 2)\n</code></pre> <p>See Accessing Keys as Class Attributes for more information.</p>"},{"location":"#convert-a-file","title":"Convert a file","text":"<p>You can convert between file formats using any of the Writers. Suppose you had a file in the JSON format and you wanted to convert it to the HDF5 format</p> <pre><code>&gt;&gt;&gt; from msl.io import HDF5Writer\n&gt;&gt;&gt; h5 = HDF5Writer(\"my_file.h5\")\n&gt;&gt;&gt; h5.write(root=read(\"my_file.json\"))\n</code></pre>"},{"location":"#read-a-table","title":"Read a table","text":"<p>The read_table function will read tabular data from a file. A table has the following properties:</p> <ol> <li>The first row is a header</li> <li>All rows have the same number of columns</li> <li>All values in a column have the same data type</li> </ol> <p>The returned item is a Dataset with the header provided in the Metadata. You can read a table from a text-based file or from a spreadsheet.</p> <p>For example, suppose a file named my_table.txt contains the following information</p> x y z 1 2 3 4 5 6 7 8 9 <p>You can read this table using the read_table function</p> <pre><code>&gt;&gt;&gt; from msl.io import read_table\n&gt;&gt;&gt; table = read_table(\"my_table.txt\")\n&gt;&gt;&gt; table\n&lt;Dataset 'my_table.txt' shape=(3, 3) dtype='&lt;f8' (1 metadata)&gt;\n&gt;&gt;&gt; table.metadata\n&lt;Metadata 'my_table.txt' {'header': ['x', 'y', 'z']}&gt;\n&gt;&gt;&gt; table.data\narray([[1., 2., 3.],\n       [4., 5., 6.],\n       [7., 8., 9.]])\n</code></pre> <p>and since a Dataset behaves like a numpy ndarray, you can call ndarray attributes directly with the <code>table</code> instance</p> <pre><code>&gt;&gt;&gt; table.shape\n(3, 3)\n&gt;&gt;&gt; print(table.max())\n9.0\n</code></pre> <p>You can define the dtype of each column. For example, to return integer values instead of floating-point numbers you can include the <code>dtype=int</code> keyword argument</p> <pre><code>&gt;&gt;&gt; table = read_table(\"my_table.txt\", dtype=int)\n&gt;&gt;&gt; table.dtype\ndtype('int64')\n</code></pre> <p>and the <code>dtype</code> value is passed to the dtype constructor.</p> <p>However, if you specify the <code>dtype</code> value as a string which starts with the text <code>header</code>, then the values in the header are used as field names for the structured array that is created where each data type is by default a double (note: the header is still included as metadata)</p> <pre><code>&gt;&gt;&gt; table = read_table(\"my_table.txt\", dtype=\"header\")\n&gt;&gt;&gt; table.dtype\ndtype([('x', '&lt;f8'), ('y', '&lt;f8'), ('z', '&lt;f8')])\n&gt;&gt;&gt; table.x\narray([1., 4., 7.])\n</code></pre> <p>You can also specify the data type to use for all columns by specifying <code>header:</code> followed by the data type</p> <pre><code>&gt;&gt;&gt; table = read_table(\"my_table.txt\", dtype=\"header:f4\")\n&gt;&gt;&gt; table.dtype\ndtype([('x', '&lt;f4'), ('y', '&lt;f4'), ('z', '&lt;f4')])\n&gt;&gt;&gt; table.y\narray([2., 5., 8.], dtype=float32)\n</code></pre> <p>or you can specify the data type of each column by separating each data type with a comma</p> <pre><code>&gt;&gt;&gt; table = read_table(\"my_table.txt\", dtype=\"header:f4,f8,H\")\n&gt;&gt;&gt; table.dtype\ndtype([('x', '&lt;f4'), ('y', '&lt;f8'), ('z', '&lt;u2')])\n&gt;&gt;&gt; table.z\narray([3, 6, 9], dtype=uint16)\n</code></pre> <p>See Specifying and constructing data types for the character and string representations of the different data types that numpy supports.</p>"},{"location":"#extension-delimiter-map","title":"Extension-delimiter map","text":"<p>Suppose you wanted to read a table from files that use a <code>;</code> character to separate columns</p> <pre><code>&gt;&gt;&gt; file.name\n'data.xyz'\n&gt;&gt;&gt; print(file.read())\nvalue;uncertainty\n6.317;0.045\n4.362;0.009\n5.328;0.013\n</code></pre> <p>You can add the extension and delimiter to the extension_delimiter_map</p> <pre><code>&gt;&gt;&gt; from msl.io import extension_delimiter_map\n&gt;&gt;&gt; extension_delimiter_map[\".xyz\"] = \";\"\n</code></pre> <p>and then you can read a table from files with the .xyz extension without needing to explicitly specify the delimiter to use every time you read a table</p> <pre><code>&gt;&gt;&gt; table = read_table(file)\n&gt;&gt;&gt; table.metadata\n&lt;Metadata 'data.xyz' {'header': ['value', 'uncertainty']}&gt;\n&gt;&gt;&gt; table.data\narray([[6.317, 0.045],\n       [4.362, 0.009],\n       [5.328, 0.013]])\n</code></pre>"},{"location":"install/","title":"Install","text":"<p><code>msl-io</code> is available for installation via the Python Package Index</p> <pre><code>pip install msl-io\n</code></pre>"},{"location":"install/#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.9+</li> <li>numpy</li> <li>xlrd (bundled with <code>msl-io</code>)</li> </ul>"},{"location":"install/#optional-dependencies","title":"Optional Dependencies","text":"<p>The following packages are not automatically installed when <code>msl-io</code> is installed but may be required to read certain files</p> <ul> <li>h5py</li> <li>google-api-python-client</li> <li>google-auth-httplib2</li> <li>google-auth-oauthlib</li> </ul> <p>To include h5py when installing <code>msl-io</code> run</p> <pre><code>pip install msl-io[h5py]\n</code></pre> <p>To include the Google-API packages run</p> <pre><code>pip install msl-io[google]\n</code></pre>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2018 - 2025, Measurement Standards Laboratory of New Zealand\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#unreleased","title":"unreleased","text":"<ul> <li>RegularTransmittanceReader for reading transmittance files from the MSL spectrophotometer</li> </ul>"},{"location":"release-notes/#020-2025-07-28","title":"0.2.0 (2025-07-28)","text":"<p>Added:</p> <ul> <li>read_table supports <code>dtype=\"header\"</code></li> <li>the dimensions method to ExcelReader</li> <li>the <code>merged</code> keyword argument to Spreadsheet.read</li> <li>ODSReader for reading OpenDocument Spreadsheet files</li> <li>GSheetsReader and ExcelReader can now be used as a context manager</li> <li>PEP-484 type annotations</li> <li>a <code>TEXT</code> member to the GCellType enum</li> <li>support for Python 3.12 and 3.13</li> </ul> <p>Changed:</p> <ul> <li>implement the __init_subclass__ method to register Reader subclasses instead of using the <code>@register</code> decorator (PEP-487)</li> <li>move the static <code>get_bytes</code>, <code>get_lines</code> and <code>get_extension</code> methods from the <code>Reader</code> class to the utils module</li> <li>convert to an implicit namespace package (PEP-420)</li> <li>utils.git_head now returns a GitHead dataclass with the <code>datetime</code> key replaced with a <code>timestamp</code> attribute</li> <li>the names of the keyword arguments to utils.search</li> </ul> <p>Fixed:</p> <ul> <li>specifying only the top-left cell to read_table returned the wrong cells from the spreadsheet</li> </ul> <p>Removed:</p> <ul> <li>the <code>workbook</code> property to ExcelReader</li> <li>support for Python \u2264 3.8</li> </ul>"},{"location":"release-notes/#010-2023-06-16","title":"0.1.0 (2023-06-16)","text":"<p>Initial release.</p>"},{"location":"api/","title":"API Overview","text":"<p>The following functions are available to read a file</p> <ul> <li>read \u2014 Read a file that has a Reader subclass implemented</li> <li>read_table \u2014 Read tabular data</li> </ul> <p>The following classes are for reading cell values (not drawings or charts) in spreadsheets</p> <ul> <li>ExcelReader \u2014 Microsoft Excel</li> <li>GSheetsReader \u2014 Google Sheets</li> <li>ODSReader \u2014 OpenDocument Spreadsheet</li> </ul> <p>These classes are for interacting with a Google account</p> <ul> <li>GDrive \u2014 Google Drive</li> <li>GSheets \u2014 Google Sheets</li> <li>GMail \u2014 Google Mail</li> </ul> <p>There are Readers and Writers and general helper (utility) functions are in the utils module.</p>"},{"location":"api/base/","title":"base","text":""},{"location":"api/base/#msl.io.base.Reader","title":"Reader","text":"<pre><code>Reader(file)\n</code></pre> <p>               Bases: <code>Root</code>, <code>ABC</code></p> <p>Abstract base class for a Reader.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>ReadLike | str</code> <p>The file to read.</p> required Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: ReadLike | str) -&gt; None:\n    \"\"\"Abstract base class for a [Reader][msl-io-readers].\n\n    Args:\n        file: The file to read.\n    \"\"\"\n    super().__init__(file)\n    self._file: ReadLike | str = file\n</code></pre>"},{"location":"api/base/#msl.io.base.Reader.file","title":"file  <code>property</code>","text":"<pre><code>file\n</code></pre> <p>ReadLike | str \u2014 The file object associated with the Reader.</p>"},{"location":"api/base/#msl.io.base.Reader.can_read","title":"can_read  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>can_read(file, **kwargs)\n</code></pre> <p>Whether this Reader can read the file specified by <code>file</code>.</p> <p>You must override this method.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>ReadLike | str</code> <p>The file to check whether the Reader can read it.</p> required <code>kwargs</code> <code>Any</code> <p>Keyword arguments that the Reader class may need when checking if it can read the <code>file</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>Either <code>True</code> (can read) or <code>False</code> (cannot read).</p> Source code in <code>src/msl/io/base.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef can_read(file: ReadLike | str, **kwargs: Any) -&gt; bool:\n    \"\"\"Whether this [Reader][msl.io.base.Reader] can read the file specified by `file`.\n\n    !!! warning \"You must override this method.\"\n\n    Args:\n        file: The file to check whether the [Reader][msl.io.base.Reader] can read it.\n        kwargs: Keyword arguments that the [Reader][msl.io.base.Reader] class may need\n            when checking if it can read the `file`.\n\n    Returns:\n        Either `True` (can read) or `False` (cannot read).\n    \"\"\"\n</code></pre>"},{"location":"api/base/#msl.io.base.Reader.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read(**kwargs)\n</code></pre> <p>Read the file.</p> <p>The file to read can be accessed by the file property.</p> <p>You must override this method.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Keyword arguments that the Reader class may need when reading the file.</p> <code>{}</code> Source code in <code>src/msl/io/base.py</code> <pre><code>@abstractmethod\ndef read(self, **kwargs: Any) -&gt; None:\n    \"\"\"Read the file.\n\n    The file to read can be accessed by the [file][msl.io.base.Reader.file] property.\n\n    !!! warning \"You must override this method.\"\n\n    Args:\n        kwargs: Keyword arguments that the [Reader][msl.io.base.Reader] class may need\n            when reading the file.\n    \"\"\"\n</code></pre>"},{"location":"api/base/#msl.io.base.Root","title":"Root","text":"<pre><code>Root(file, **metadata)\n</code></pre> <p>               Bases: <code>Group</code></p> <p>The root Group.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike | WriteLike | None</code> <p>The file object to associate with the Root.</p> required <code>metadata</code> <code>Any</code> <p>All keyword arguments are used as Metadata.</p> <code>{}</code> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(\n    self,\n    file: PathLike | ReadLike | WriteLike | None,\n    **metadata: Any,\n) -&gt; None:\n    \"\"\"The root [Group][msl.io.node.Group].\n\n    Args:\n        file: The file object to associate with the [Root][msl.io.base.Root].\n        metadata: All keyword arguments are used as [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    super().__init__(name=\"/\", parent=None, read_only=False, **metadata)\n    self._root_file: PathLike | ReadLike | WriteLike | None = file\n</code></pre>"},{"location":"api/base/#msl.io.base.Root.tree","title":"tree","text":"<pre><code>tree(*, indent=2)\n</code></pre> <p>Returns a string representation of the tree structure.</p> <p>Shows all Groups and Datasets that are in Root.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int</code> <p>The amount of indentation to add for each recursive level.</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>The tree structure.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def tree(self, *, indent: int = 2) -&gt; str:\n    \"\"\"Returns a string representation of the [tree structure](https://en.wikipedia.org/wiki/Tree_structure){:target=\"_blank\"}.\n\n    Shows all [Group][msl.io.node.Group]s and [Dataset][msl.io.node.Dataset]s that are in [Root][msl.io.base.Root].\n\n    Args:\n        indent: The amount of indentation to add for each recursive level.\n\n    Returns:\n        The [tree structure](https://en.wikipedia.org/wiki/Tree_structure){:target=\"_blank\"}.\n    \"\"\"\n    return repr(self) + \"\\n\" + \"\\n\".join(\" \" * (indent * k.count(\"/\")) + repr(v) for k, v in sorted(self.items()))\n</code></pre>"},{"location":"api/base/#msl.io.base.Writer","title":"Writer","text":"<pre><code>Writer(file=None, **metadata)\n</code></pre> <p>               Bases: <code>Root</code>, <code>ABC</code></p> <p>Abstract base class for a Writer.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | WriteLike | None</code> <p>The file to write the data to. Can also be specified in the write method.</p> <code>None</code> <code>metadata</code> <code>Any</code> <p>All keyword arguments are used as Metadata.</p> <code>{}</code> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: PathLike | WriteLike | None = None, **metadata: Any) -&gt; None:\n    \"\"\"Abstract base class for a [Writer][msl-io-writers].\n\n    Args:\n        file: The file to write the data to. Can also be specified in the [write][msl.io.base.Writer.write] method.\n        metadata: All keyword arguments are used as [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    super().__init__(file, **metadata)\n    self._file: PathLike | WriteLike | None = file\n    self._context_kwargs: dict[str, Any] = {}\n</code></pre>"},{"location":"api/base/#msl.io.base.Writer.file","title":"file  <code>property</code>","text":"<pre><code>file\n</code></pre> <p>PathLike | WriteLike | None \u2014 The file object associated with the Writer.</p>"},{"location":"api/base/#msl.io.base.Writer.save","title":"save","text":"<pre><code>save(file=None, *, root=None, **kwargs)\n</code></pre> <p>Alias for write.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def save(\n    self,\n    file: PathLike | WriteLike | None = None,\n    *,\n    root: Group | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Alias for [write][msl.io.base.Writer.write].\"\"\"\n    self.write(file=file, root=root, **kwargs)\n</code></pre>"},{"location":"api/base/#msl.io.base.Writer.set_root","title":"set_root","text":"<pre><code>set_root(root)\n</code></pre> <p>Set a new Root for the Writer.</p> <p>Info</p> <p>This will clear the Metadata of the Writer and all Groups and Datasets that the Writer currently contains. The <code>file</code> that was specified when the Writer was created does not change.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Group</code> <p>The new <code>root</code>.</p> required Source code in <code>src/msl/io/base.py</code> <pre><code>def set_root(self, root: Group) -&gt; None:\n    \"\"\"Set a new [Root][msl.io.base.Root] for the [Writer][msl.io.base.Writer].\n\n    !!! info\n        This will clear the [Metadata][msl.io.metadata.Metadata] of the [Writer][msl.io.base.Writer]\n        and all [Group][msl.io.node.Group]s and [Dataset][msl.io.node.Dataset]s that the\n        [Writer][msl.io.base.Writer] currently contains. The `file` that was specified when\n        the [Writer][msl.io.base.Writer] was created does not change.\n\n    Args:\n        root: The new `root`.\n    \"\"\"\n    self.clear()\n    self.metadata.clear()\n    self.add_metadata(**root.metadata)\n    if root:  # only do this if there are Groups and/or Datasets in the new root\n        self.add_group(\"\", root)\n</code></pre>"},{"location":"api/base/#msl.io.base.Writer.update_context_kwargs","title":"update_context_kwargs","text":"<pre><code>update_context_kwargs(**kwargs)\n</code></pre> <p>Update the keyword arguments when used as a context manager.</p> <p>When a Writer is used as a context manager the write method is automatically called when exiting the context manager. You can specify the keyword arguments that will be passed to the write method by calling update_context_kwargs with the appropriate keyword arguments before the context manager exits. You may call this method multiple times.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def update_context_kwargs(self, **kwargs: Any) -&gt; None:\n    \"\"\"Update the keyword arguments when used as a [context manager][with]{:target=\"_blank\"}.\n\n    When a [Writer][msl.io.base.Writer] is used as a [context manager][with]{:target=\"_blank\"}\n    the [write][msl.io.base.Writer.write] method is automatically called when exiting the\n    [context manager][with]{:target=\"_blank\"}. You can specify the keyword arguments\n    that will be passed to the [write][msl.io.base.Writer.write] method by calling\n    [update_context_kwargs][msl.io.base.Writer.update_context_kwargs] with the appropriate\n    keyword arguments before the [context manager][with]{:target=\"_blank\"} exits. You may\n    call this method multiple times.\n    \"\"\"\n    self._context_kwargs.update(**kwargs)\n</code></pre>"},{"location":"api/base/#msl.io.base.Writer.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(file=None, root=None, **kwargs)\n</code></pre> <p>Write to a file.</p> <p>You must override this method.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | WriteLike | None</code> <p>The file to write the <code>root</code> to. If <code>None</code> then uses the <code>file</code> value that was specified when the Writer was instantiated.</p> <code>None</code> <code>root</code> <code>Group | None</code> <p>Write the <code>root</code> object in the file format of this Writer. This argument is useful when converting between different file formats.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments to use when writing the file.</p> <code>{}</code> Source code in <code>src/msl/io/base.py</code> <pre><code>@abstractmethod\ndef write(self, file: PathLike | WriteLike | None = None, root: Group | None = None, **kwargs: Any) -&gt; None:\n    \"\"\"Write to a file.\n\n    !!! warning \"You must override this method.\"\n\n    Args:\n        file: The file to write the `root` to. If `None` then uses the `file` value\n            that was specified when the [Writer][msl.io.base.Writer] was instantiated.\n        root: Write the `root` object in the file format of this [Writer][msl.io.base.Writer].\n            This argument is useful when converting between different file formats.\n        kwargs: Keyword arguments to use when writing the file.\n    \"\"\"\n</code></pre>"},{"location":"api/base/#msl.io.base.read","title":"read","text":"<pre><code>read(file, **kwargs)\n</code></pre> <p>Read a file that has a Reader implemented.</p> <p>See the Overview for an example.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike</code> <p>The file to read.</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are passed to the abstract can_read and read methods.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Reader</code> <p>The data from the file.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def read(file: PathLike | ReadLike, **kwargs: Any) -&gt; Reader:\n    \"\"\"Read a file that has a [Reader][msl-io-readers] implemented.\n\n    !!! example \"See the [Overview][read-a-file] for an example.\"\n\n    Args:\n        file: The file to read.\n        kwargs: All keyword arguments are passed to the abstract [can_read][msl.io.base.Reader.can_read]\n            and [read][msl.io.base.Reader.read] methods.\n\n    Returns:\n        The data from the file.\n    \"\"\"\n    if isinstance(file, (bytes, str, os.PathLike)):\n        file = os.fsdecode(file)\n        readable = is_file_readable(file, strict=True)\n    else:\n        readable = hasattr(file, \"read\")\n\n    if readable:\n        logger.debug(\"finding Reader for %r\", file)\n        for r in _readers:\n            logger.debug(\"checking %s\", r.__name__)\n            try:\n                can_read = r.can_read(file, **kwargs)\n            except Exception as e:  # noqa: BLE001\n                logger.debug(\"%s: %s [%s]\", e.__class__.__name__, e, r.__name__)\n                continue\n\n            if can_read:\n                logger.debug(\"reading file with %s\", r.__name__)\n                root = r(file)\n                root.read(**kwargs)\n                root.read_only = True\n                return root\n\n    msg = f\"No Reader exists to read {file!r}\"\n    raise OSError(msg)\n</code></pre>"},{"location":"api/constants/","title":"constants","text":"<p>Constants used by <code>msl-io</code>.</p>"},{"location":"api/constants/#msl.io.constants.MSL_IO_DIR","title":"MSL_IO_DIR  <code>module-attribute</code>","text":"<pre><code>MSL_IO_DIR = Path(\n    getenv(\"MSL_IO_DIR\", USER_DIR / \".msl\" / \"io\")\n)\n</code></pre> <p>Path \u2014 The default directory where all files that are used by <code>msl-io</code> are located.</p> <p>Can be overwritten by specifying an <code>MSL_IO_DIR</code> environment variable.</p>"},{"location":"api/constants/#msl.io.constants.USER_DIR","title":"USER_DIR  <code>module-attribute</code>","text":"<pre><code>USER_DIR = expanduser()\n</code></pre> <p>Path \u2014 The user's home directory.</p>"},{"location":"api/freezable/","title":"freezable","text":"<p>Freezable objects (can be made read only).</p>"},{"location":"api/freezable/#msl.io.freezable.FreezableMap","title":"FreezableMap","text":"<pre><code>FreezableMap(*, read_only, **kwargs)\n</code></pre> <p>               Bases: <code>MutableMapping[str, VT]</code></p> <p>A key-value map that can be made read only.</p> <p>Parameters:</p> Name Type Description Default <code>read_only</code> <code>bool</code> <p>Whether the mapping is initially in read-only mode (frozen).</p> required <code>kwargs</code> <code>VT</code> <p>All other keyword arguments are used to create the underlying map object.</p> <code>{}</code> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def __init__(self, *, read_only: bool, **kwargs: VT) -&gt; None:\n    \"\"\"A key-value map that can be made read only.\n\n    Args:\n        read_only: Whether the mapping is initially in read-only mode (frozen).\n        kwargs: All other keyword arguments are used to create the underlying map object.\n    \"\"\"\n    self._read_only: bool = bool(read_only)\n    self._mapping: dict[str, VT] = dict(**kwargs)\n</code></pre>"},{"location":"api/freezable/#msl.io.freezable.FreezableMap.read_only","title":"read_only  <code>property</code> <code>writable</code>","text":"<pre><code>read_only\n</code></pre> <p>bool \u2014 Whether the map is in read-only mode.</p>"},{"location":"api/freezable/#msl.io.freezable.FreezableMap.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Maybe remove all items from the map, only if the map is not in read-only mode.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Maybe remove all items from the map, only if the map is not in read-only mode.\"\"\"\n    self._raise_if_read_only()\n    self._mapping.clear()\n</code></pre>"},{"location":"api/freezable/#msl.io.freezable.FreezableMap.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>ItemsView[str, VT] \u2014 Return a view of the map's items.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def items(self) -&gt; ItemsView[str, VT]:\n    \"\"\"[ItemsView][collections.abc.ItemsView][[str][], VT] &amp;mdash; Return a view of the map's items.\"\"\"\n    return ItemsView(self)\n</code></pre>"},{"location":"api/freezable/#msl.io.freezable.FreezableMap.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>KeysView[str] \u2014 Returns a view of the map's keys.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def keys(self) -&gt; KeysView[str]:\n    \"\"\"[KeysView][collections.abc.KeysView][[str][]] &amp;mdash; Returns a view of the map's keys.\"\"\"\n    return KeysView(self)\n</code></pre>"},{"location":"api/freezable/#msl.io.freezable.FreezableMap.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>ValuesView[VT] \u2014 Returns a view of the map's values.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def values(self) -&gt; ValuesView[VT]:\n    \"\"\"[ValuesView][collections.abc.ValuesView][VT] &amp;mdash; Returns a view of the map's values.\"\"\"\n    return ValuesView(self)\n</code></pre>"},{"location":"api/google/","title":"google_api","text":"<p>Wrappers around Google APIs.</p>"},{"location":"api/google/#msl.io.google_api.GCell","title":"GCell","text":"<p>               Bases: <code>NamedTuple</code></p> <p>The information about a Google Sheets cell.</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>Any</code> <p>The value of the cell.</p> <code>type</code> <code>GCellType</code> <p>The data type of <code>value</code>.</p> <code>formatted</code> <code>str</code> <p>The formatted value (i.e., how the <code>value</code> is displayed in the cell).</p>"},{"location":"api/google/#msl.io.google_api.GCellType","title":"GCellType","text":"<p>               Bases: <code>Enum</code></p> <p>The spreadsheet cell data type.</p> <p>Attributes:</p> Name Type Description <code>BOOLEAN</code> <code>str</code> <p><code>\"BOOLEAN\"</code></p> <code>CURRENCY</code> <code>str</code> <p><code>\"CURRENCY\"</code></p> <code>DATE</code> <code>str</code> <p><code>\"DATE\"</code></p> <code>DATE_TIME</code> <code>str</code> <p><code>\"DATE_TIME\"</code></p> <code>EMPTY</code> <code>str</code> <p><code>\"EMPTY\"</code></p> <code>ERROR</code> <code>str</code> <p><code>\"ERROR\"</code></p> <code>NUMBER</code> <code>str</code> <p><code>\"NUMBER\"</code></p> <code>PERCENT</code> <code>str</code> <p><code>\"PERCENT\"</code></p> <code>SCIENTIFIC</code> <code>str</code> <p><code>\"SCIENTIFIC\"</code></p> <code>STRING</code> <code>str</code> <p><code>\"STRING\"</code></p> <code>TEXT</code> <code>str</code> <p><code>\"TEXT\"</code></p> <code>TIME</code> <code>str</code> <p><code>\"TIME\"</code></p> <code>UNKNOWN</code> <code>str</code> <p><code>\"UNKNOWN\"</code></p>"},{"location":"api/google/#msl.io.google_api.GDateTimeOption","title":"GDateTimeOption","text":"<p>               Bases: <code>Enum</code></p> <p>Determines how dates should be returned.</p>"},{"location":"api/google/#msl.io.google_api.GDateTimeOption.FORMATTED_STRING","title":"FORMATTED_STRING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FORMATTED_STRING = 'FORMATTED_STRING'\n</code></pre> <p>Instructs date, time, datetime, and duration fields to be output as strings in their given number format (which is dependent on the spreadsheet locale).</p>"},{"location":"api/google/#msl.io.google_api.GDateTimeOption.SERIAL_NUMBER","title":"SERIAL_NUMBER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SERIAL_NUMBER = 'SERIAL_NUMBER'\n</code></pre> <p>Instructs date, time, datetime, and duration fields to be output as doubles in \"serial number\" format, as popularised by Lotus 1-2-3. The whole number portion of the value (left of the decimal) counts the days since December 30th 1899. The fractional portion (right of the decimal) counts the time as a fraction of the day. For example, January 1st 1900 at noon would be 2.5, 2 because it's 2 days after December 30st 1899, and .5 because noon is half a day. February 1st 1900 at 3pm would be 33.625. This correctly treats the year 1900 as not a leap year.</p>"},{"location":"api/google/#msl.io.google_api.GDrive","title":"GDrive","text":"<pre><code>GDrive(\n    *,\n    account=None,\n    credentials=None,\n    scopes=None,\n    read_only=True,\n)\n</code></pre> <p>               Bases: <code>GoogleAPI</code></p> <p>Interact with Google Drive.</p> <p>Info</p> <p>You must follow the instructions in the prerequisites section for setting up the Drive API before you can use this class. It is also useful to be aware of the refresh token expiration policy.</p> <p>Parameters:</p> Name Type Description Default <code>account</code> <code>str | None</code> <p>Since a person may have multiple Google accounts, and multiple people may run the same code, this parameter decides which token to load to authenticate with the Google API. The value can be any text (or <code>None</code>) that you want to associate with a particular Google account, provided that it contains valid characters for a filename. The value that you chose when you authenticated with your <code>credentials</code> should be used for all future instances of this class to access that particular Google account. You can associate a different value with a Google account at any time (by passing in a different <code>account</code> value), but you may be asked to authenticate with your <code>credentials</code> again, or, alternatively, you can rename the token files located in MSL_IO_DIR to match the new <code>account</code> value.</p> <code>None</code> <code>credentials</code> <code>PathLike | None</code> <p>The path to the client secrets OAuth credential file. This parameter only needs to be specified the first time that you authenticate with a particular Google <code>account</code> or if you delete the token file that was created when you previously authenticated.</p> <code>None</code> <code>scopes</code> <code>list[str] | None</code> <p>The list of scopes to enable for the Google API. See Drive scopes for more details. If not specified, default scopes are chosen based on the value of <code>read_only</code>.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Whether to interact with Google Drive in read-only mode.</p> <code>True</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def __init__(\n    self,\n    *,\n    account: str | None = None,\n    credentials: PathLike | None = None,\n    scopes: list[str] | None = None,\n    read_only: bool = True,\n) -&gt; None:\n    \"\"\"Interact with Google Drive.\n\n    !!! info\n        You must follow the instructions in the prerequisites section for setting up the\n        [Drive API](https://developers.google.com/drive/api/quickstart/python#prerequisites){:target=\"_blank\"}\n        before you can use this class. It is also useful to be aware of the\n        [refresh token expiration](https://developers.google.com/identity/protocols/oauth2#expiration){:target=\"_blank\"}\n        policy.\n\n    Args:\n        account: Since a person may have multiple Google accounts, and multiple people\n            may run the same code, this parameter decides which token to load to authenticate\n            with the Google API. The value can be any text (or `None`) that you want to\n            associate with a particular Google account, provided that it contains valid\n            characters for a filename. The value that you chose when you authenticated with\n            your `credentials` should be used for all future instances of this class to access\n            that particular Google account. You can associate a different value with a Google\n            account at any time (by passing in a different `account` value), but you may be\n            asked to authenticate with your `credentials` again, or, alternatively, you can\n            rename the token files located in [MSL_IO_DIR][msl.io.constants.MSL_IO_DIR]\n            to match the new `account` value.\n        credentials: The path to the *client secrets* OAuth credential file. This parameter only\n            needs to be specified the first time that you authenticate with a particular Google\n            `account` or if you delete the token file that was created when you previously authenticated.\n        scopes: The list of scopes to enable for the Google API. See\n            [Drive scopes](https://developers.google.com/identity/protocols/oauth2/scopes#drive){:target=\"_blank\"}\n            for more details. If not specified, default scopes are chosen based on the value of `read_only`.\n        read_only: Whether to interact with Google Drive in read-only mode.\n    \"\"\"\n    if not scopes:\n        if read_only:\n            scopes = [\n                \"https://www.googleapis.com/auth/drive.readonly\",\n                \"https://www.googleapis.com/auth/drive.metadata.readonly\",\n            ]\n        else:\n            scopes = [\n                \"https://www.googleapis.com/auth/drive\",\n                \"https://www.googleapis.com/auth/drive.metadata\",\n            ]\n\n    c = Path(os.fsdecode(credentials)) if credentials else None\n    super().__init__(\n        service=\"drive\", version=\"v3\", account=account, credentials=c, scopes=scopes, read_only=read_only\n    )\n\n    self._files: Any = self._service.files()\n    self._drives: Any = self._service.drives()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.copy","title":"copy","text":"<pre><code>copy(file_id, folder_id=None, name=None)\n</code></pre> <p>Copy a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The ID of a file to copy. Folders cannot be copied.</p> required <code>folder_id</code> <code>str | None</code> <p>The ID of the destination folder. If not specified then creates a copy in the same folder that the original file is located in. To copy the file to the My Drive root folder then specify <code>'root'</code> as the <code>folder_id</code>.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>The filename of the destination file.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the destination file.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def copy(self, file_id: str, folder_id: str | None = None, name: str | None = None) -&gt; str:\n    \"\"\"Copy a file.\n\n    Args:\n        file_id: The ID of a file to copy. Folders cannot be copied.\n        folder_id: The ID of the destination folder. If not specified then creates\n            a copy in the same folder that the original file is located in. To copy\n            the file to the *My Drive* root folder then specify `'root'` as the `folder_id`.\n        name: The filename of the destination file.\n\n    Returns:\n        The ID of the destination file.\n    \"\"\"\n    response = self._files.copy(\n        fileId=file_id,\n        fields=\"id\",\n        supportsAllDrives=True,\n        body={\n            \"name\": name,\n            \"parents\": [folder_id] if folder_id else None,\n        },\n    ).execute()\n    return str(response[\"id\"])\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.create_folder","title":"create_folder","text":"<pre><code>create_folder(folder, parent_id=None)\n</code></pre> <p>Create a folder.</p> <p>Makes all intermediate-level folders needed to contain the leaf directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PathLike</code> <p>The folder(s) to create, for example, <code>'folder1'</code> or <code>'folder1/folder2/folder3'</code>.</p> required <code>parent_id</code> <code>str | None</code> <p>The ID of the parent folder that <code>folder</code> is relative to. If not specified, <code>folder</code> is relative to the My Drive root folder. If <code>folder</code> is in a Shared drive then you must specify the ID of the parent folder.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the last (right most) folder that was created.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def create_folder(self, folder: PathLike, parent_id: str | None = None) -&gt; str:\n    \"\"\"Create a folder.\n\n    Makes all intermediate-level folders needed to contain the leaf directory.\n\n    Args:\n        folder: The folder(s) to create, for example, `'folder1'` or `'folder1/folder2/folder3'`.\n        parent_id: The ID of the parent folder that `folder` is relative to. If not\n            specified, `folder` is relative to the *My Drive* root folder. If `folder`\n            is in a *Shared drive* then you must specify the ID of the parent folder.\n\n    Returns:\n        The ID of the last (right most) folder that was created.\n    \"\"\"\n    names = GDrive._folder_hierarchy(folder)\n    response = {\"id\": parent_id or \"root\"}\n    for name in names:\n        request = self._files.create(\n            body={\n                \"name\": name,\n                \"mimeType\": GDrive.MIME_TYPE_FOLDER,\n                \"parents\": [response[\"id\"]],\n            },\n            fields=\"id\",\n            supportsAllDrives=True,\n        )\n        response = request.execute()\n    return response[\"id\"]\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.delete","title":"delete","text":"<pre><code>delete(file_or_folder_id)\n</code></pre> <p>Delete a file or a folder.</p> <p>Files that are in read-only mode cannot be deleted.</p> <p>Danger</p> <p>Permanently deletes the file or folder owned by the user without moving it to the trash. If the target is a folder, then all files and sub-folders contained within the folder (that are owned by the user) are also permanently deleted.</p> <p>Parameters:</p> Name Type Description Default <code>file_or_folder_id</code> <code>str</code> <p>The ID of the file or folder to delete.</p> required Source code in <code>src/msl/io/google_api.py</code> <pre><code>def delete(self, file_or_folder_id: str) -&gt; None:\n    \"\"\"Delete a file or a folder.\n\n    Files that are in read-only mode cannot be deleted.\n\n    !!! danger\n        Permanently deletes the file or folder owned by the user without\n        moving it to the trash. If the target is a folder, then all files\n        and sub-folders contained within the folder (that are owned by the\n        user) are also permanently deleted.\n\n    Args:\n        file_or_folder_id: The ID of the file or folder to delete.\n    \"\"\"\n    if self.is_read_only(file_or_folder_id):\n        # The API allows for a file to be deleted if it is in read-only mode,\n        # but we will not allow it to be deleted\n        msg = \"Cannot delete the file since it is in read-only mode\"\n        raise RuntimeError(msg)\n\n    self._files.delete(\n        fileId=file_or_folder_id,\n        supportsAllDrives=True,\n    ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.download","title":"download","text":"<pre><code>download(\n    file_id,\n    *,\n    save_to=None,\n    num_retries=0,\n    chunk_size=DEFAULT_CHUNK_SIZE,\n    callback=None,\n)\n</code></pre> <p>Download a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The ID of the file to download.</p> required <code>save_to</code> <code>PathLike | FileLikeWrite[bytes] | None</code> <p>The location to save the file to. If a directory is specified, the directory must already exist and the file will be saved to that directory using the filename of the remote file. To save the file with a new filename, also specify the new filename. Default is to save the file to the current working directory using the remote filename.</p> <code>None</code> <code>num_retries</code> <code>int</code> <p>The number of times to retry the download. If zero (default) then attempt the request only once.</p> <code>0</code> <code>chunk_size</code> <code>int</code> <p>The file will be downloaded in chunks of this many bytes.</p> <code>DEFAULT_CHUNK_SIZE</code> <code>callback</code> <code>Callable[[MediaDownloadProgress], None] | None</code> <p>The callback function to call after each chunk of the file is downloaded. The <code>callback</code> accepts one positional argument that is of type MediaDownloadProgress.</p> <code>None</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def download(\n    self,\n    file_id: str,\n    *,\n    save_to: PathLike | FileLikeWrite[bytes] | None = None,\n    num_retries: int = 0,\n    chunk_size: int = DEFAULT_CHUNK_SIZE,\n    callback: Callable[[MediaDownloadProgress], None] | None = None,\n) -&gt; None:\n    \"\"\"Download a file.\n\n    Args:\n        file_id: The ID of the file to download.\n        save_to: The location to save the file to. If a directory is specified, the directory\n            must already exist and the file will be saved to that directory using the filename\n            of the remote file. To save the file with a new filename, also specify the new filename.\n            Default is to save the file to the current working directory using the remote filename.\n        num_retries: The number of times to retry the download.\n            If zero (default) then attempt the request only once.\n        chunk_size: The file will be downloaded in chunks of this many bytes.\n        callback: The callback function to call after each chunk of the file is downloaded.\n            The `callback` accepts one positional argument that is of type\n            [MediaDownloadProgress][msl.io.types.MediaDownloadProgress].\n    \"\"\"\n    response = self._files.get(\n        fileId=file_id,\n        fields=\"name\",\n        supportsAllDrives=True,\n    ).execute()\n    filename: str = response[\"name\"]\n\n    file: BufferedWriter | FileLikeWrite[bytes]\n    if save_to is None:\n        file = Path(filename).open(\"wb\")  # noqa: SIM115\n    elif isinstance(save_to, (str, bytes, os.PathLike)):\n        path = Path(os.fsdecode(save_to))\n        file = (path / filename).open(\"wb\") if path.is_dir() else path.open(\"wb\")\n    else:\n        file = save_to\n\n    request = self._files.get_media(fileId=file_id, supportsAllDrives=True)\n    downloader = MediaIoBaseDownload(file, request, chunksize=chunk_size)  # pyright: ignore[reportPossiblyUnboundVariable,reportUnknownVariableType]\n    done = False\n    while not done:\n        status, done = downloader.next_chunk(num_retries=num_retries)  # pyright: ignore[reportUnknownMemberType,reportUnknownVariableType]\n        if callback:\n            callback(status)  # pyright: ignore[reportUnknownArgumentType]\n\n    if file is not save_to:\n        file.close()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.empty_trash","title":"empty_trash","text":"<pre><code>empty_trash()\n</code></pre> <p>Permanently delete all files in the trash.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def empty_trash(self) -&gt; None:\n    \"\"\"Permanently delete all files in the trash.\"\"\"\n    self._files.emptyTrash().execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.file_id","title":"file_id","text":"<pre><code>file_id(file, *, mime_type=None, folder_id=None)\n</code></pre> <p>Get the ID of a Google Drive file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>The path to a Google Drive file.</p> required <code>mime_type</code> <code>str | None</code> <p>The Drive MIME type or media type to use to filter the results.</p> <code>None</code> <code>folder_id</code> <code>str | None</code> <p>The ID of the folder that <code>file</code> is relative to. If not specified, <code>file</code> is relative to the My Drive root folder. If <code>file</code> is in a Shared drive then you must specify the ID of the parent folder.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The file ID.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def file_id(self, file: PathLike, *, mime_type: str | None = None, folder_id: str | None = None) -&gt; str:\n    \"\"\"Get the ID of a Google Drive file.\n\n    Args:\n        file: The path to a Google Drive file.\n        mime_type: The [Drive MIME type](https://developers.google.com/drive/api/guides/mime-types){:target=\"_blank\"}\n            or [media type](https://www.iana.org/assignments/media-types/media-types.xhtml){:target=\"_blank\"} to use\n            to filter the results.\n        folder_id: The ID of the folder that `file` is relative to. If not specified, `file`\n            is relative to the *My Drive* root folder. If `file` is in a *Shared drive* then\n            you must specify the ID of the parent folder.\n\n    Returns:\n        The file ID.\n    \"\"\"\n    folders, name = os.path.split(os.fsdecode(file))\n    folder_id = self.folder_id(folders, parent_id=folder_id)\n\n    q = f'\"{folder_id}\" in parents and name=\"{name}\" and trashed=false'\n    if not mime_type:\n        q += f' and mimeType!=\"{GDrive.MIME_TYPE_FOLDER}\"'\n    else:\n        q += f' and mimeType=\"{mime_type}\"'\n\n    response = self._files.list(\n        q=q,\n        fields=\"files(id,name,mimeType)\",\n        includeItemsFromAllDrives=True,\n        supportsAllDrives=True,\n    ).execute()\n    files = response[\"files\"]\n    if not files:\n        msg = f\"Not a valid Google Drive file {file!r}\"\n        raise OSError(msg)\n    if len(files) &gt; 1:\n        mime_types = \"\\n  \".join(f[\"mimeType\"] for f in files)\n        msg = f\"Multiple files exist for {file!r}. Filter by MIME type:\\n  {mime_types}\"\n        raise OSError(msg)\n\n    first = files[0]\n    assert name == first[\"name\"], \"{name!r} != {first['name']!r}\"  # noqa: S101\n    return str(first[\"id\"])\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.folder_id","title":"folder_id","text":"<pre><code>folder_id(folder, *, parent_id=None)\n</code></pre> <p>Get the ID of a Google Drive folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PathLike</code> <p>The path to a Google Drive file.</p> required <code>parent_id</code> <code>str | None</code> <p>The ID of the parent folder that <code>folder</code> is relative to. If not specified, <code>folder</code> is relative to the My Drive root folder. If <code>folder</code> is in a Shared drive then you must specify the ID of the parent folder.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The folder ID.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def folder_id(self, folder: PathLike, *, parent_id: str | None = None) -&gt; str:\n    \"\"\"Get the ID of a Google Drive folder.\n\n    Args:\n        folder: The path to a Google Drive file.\n        parent_id: The ID of the parent folder that `folder` is relative to. If not\n            specified, `folder` is relative to the *My Drive* root folder. If `folder`\n            is in a *Shared drive* then you must specify the ID of the parent folder.\n\n    Returns:\n        The folder ID.\n    \"\"\"\n    folder_id = parent_id or \"root\"\n    folder = os.fsdecode(folder)\n    names = GDrive._folder_hierarchy(folder)\n    for name in names:\n        q = f'\"{folder_id}\" in parents and name=\"{name}\" and trashed=false and mimeType=\"{GDrive.MIME_TYPE_FOLDER}\"'\n        response = self._files.list(\n            q=q,\n            fields=\"files(id,name)\",\n            includeItemsFromAllDrives=True,\n            supportsAllDrives=True,\n        ).execute()\n        files = response[\"files\"]\n        if not files:\n            msg = f\"Not a valid Google Drive folder {folder!r}\"\n            raise OSError(msg)\n        if len(files) &gt; 1:\n            matches = \"\\n  \".join(str(file) for file in files)\n            msg = f\"Multiple folders exist for {name!r}\\n  {matches}\"\n            raise OSError(msg)\n\n        first = files[0]\n        assert name == first[\"name\"], f\"{name!r} != {first['name']!r}\"  # noqa: S101\n        folder_id = first[\"id\"]\n\n    return folder_id\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.is_file","title":"is_file","text":"<pre><code>is_file(file, *, mime_type=None, folder_id=None)\n</code></pre> <p>Check if a file exists.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>The path to a Google Drive file.</p> required <code>mime_type</code> <code>str | None</code> <p>The Drive MIME type or media type to use to filter the results.</p> <code>None</code> <code>folder_id</code> <code>str | None</code> <p>The ID of the folder that <code>file</code> is relative to. If not specified, <code>file</code> is relative to the My Drive root folder. If <code>file</code> is in a Shared drive then you must specify the ID of the parent folder.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the file exists.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def is_file(self, file: PathLike, *, mime_type: str | None = None, folder_id: str | None = None) -&gt; bool:\n    \"\"\"Check if a file exists.\n\n    Args:\n        file: The path to a Google Drive file.\n        mime_type: The [Drive MIME type](https://developers.google.com/drive/api/guides/mime-types){target=\"_blank\"}\n            or [media type](https://www.iana.org/assignments/media-types/media-types.xhtml){target=\"_blank\"} to use\n            to filter the results.\n        folder_id: The ID of the folder that `file` is relative to. If not specified, `file`\n            is relative to the *My Drive* root folder. If `file` is in a *Shared drive* then\n            you must specify the ID of the parent folder.\n\n    Returns:\n        Whether the file exists.\n    \"\"\"\n    try:\n        _ = self.file_id(file, mime_type=mime_type, folder_id=folder_id)\n    except OSError as err:\n        return str(err).startswith(\"Multiple files\")\n    else:\n        return True\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.is_folder","title":"is_folder","text":"<pre><code>is_folder(folder, parent_id=None)\n</code></pre> <p>Check if a folder exists.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PathLike</code> <p>The path to a Google Drive folder.</p> required <code>parent_id</code> <code>str | None</code> <p>The ID of the parent folder that <code>folder</code> is relative to. If not specified, <code>folder</code> is relative to the My Drive root folder. If <code>folder</code> is in a Shared drive then you must specify the ID of the parent folder.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the folder exists.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def is_folder(self, folder: PathLike, parent_id: str | None = None) -&gt; bool:\n    \"\"\"Check if a folder exists.\n\n    Args:\n        folder: The path to a Google Drive folder.\n        parent_id: The ID of the parent folder that `folder` is relative to. If not\n            specified, `folder` is relative to the *My Drive* root folder. If `folder`\n            is in a *Shared drive* then you must specify the ID of the parent folder.\n\n    Returns:\n        Whether the folder exists.\n    \"\"\"\n    try:\n        _ = self.folder_id(folder, parent_id=parent_id)\n    except OSError as err:\n        return str(err).startswith(\"Multiple folders\")\n    else:\n        return True\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.is_read_only","title":"is_read_only","text":"<pre><code>is_read_only(file_or_folder_id)\n</code></pre> <p>Returns whether the file or folder is accessed in read-only mode.</p> <p>Parameters:</p> Name Type Description Default <code>file_or_folder_id</code> <code>str</code> <p>The ID of a file or folder.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the file or folder is accessed in read-only mode.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def is_read_only(self, file_or_folder_id: str) -&gt; bool:\n    \"\"\"Returns whether the file or folder is accessed in read-only mode.\n\n    Args:\n        file_or_folder_id: The ID of a file or folder.\n\n    Returns:\n        Whether the file or folder is accessed in read-only mode.\n    \"\"\"\n    response = self._files.get(\n        fileId=file_or_folder_id,\n        supportsAllDrives=True,\n        fields=\"contentRestrictions\",\n    ).execute()\n    restrictions = response.get(\"contentRestrictions\")\n    if not restrictions:\n        return False\n    r: bool = restrictions[0][\"readOnly\"]\n    return r\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.move","title":"move","text":"<pre><code>move(source_id, destination_id)\n</code></pre> <p>Move a file or a folder.</p> <p>When moving a file or folder between My Drive and a Shared drive the access permissions will change.</p> <p>Moving a file or folder does not change its ID, only the ID of its parent changes (i.e., <code>source_id</code> will remain the same after the move).</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>The ID of a file or folder to move.</p> required <code>destination_id</code> <code>str</code> <p>The ID of the destination folder. To move the file or folder to the My Drive root folder then specify <code>'root'</code> as the <code>destination_id</code>.</p> required Source code in <code>src/msl/io/google_api.py</code> <pre><code>def move(self, source_id: str, destination_id: str) -&gt; None:\n    \"\"\"Move a file or a folder.\n\n    When moving a file or folder between *My Drive* and a *Shared drive*\n    the access permissions will change.\n\n    Moving a file or folder does not change its ID, only the ID of\n    its *parent* changes (i.e., `source_id` will remain the same\n    after the move).\n\n    Args:\n        source_id: The ID of a file or folder to move.\n        destination_id: The ID of the destination folder. To move the file or folder to the\n            *My Drive* root folder then specify `'root'` as the `destination_id`.\n    \"\"\"\n    params = {\"fileId\": source_id, \"supportsAllDrives\": True}\n    try:\n        self._files.update(addParents=destination_id, **params).execute()\n    except HttpError as e:  # pyright: ignore[reportPossiblyUnboundVariable,reportUnknownVariableType]\n        if \"exactly one parent\" not in str(e):  # pyright: ignore[reportUnknownArgumentType]\n            raise\n\n        # Handle the following error:\n        #   A shared drive item must have exactly one parent\n        response = self._files.get(fields=\"parents\", **params).execute()\n        self._files.update(\n            addParents=destination_id, removeParents=\",\".join(response[\"parents\"]), **params\n        ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.path","title":"path","text":"<pre><code>path(file_or_folder_id)\n</code></pre> <p>Convert an ID to a path.</p> <p>Parameters:</p> Name Type Description Default <code>file_or_folder_id</code> <code>str</code> <p>The ID of a file or folder.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The corresponding path of the ID.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def path(self, file_or_folder_id: str) -&gt; str:\n    \"\"\"Convert an ID to a path.\n\n    Args:\n        file_or_folder_id: The ID of a file or folder.\n\n    Returns:\n        The corresponding path of the ID.\n    \"\"\"\n    names: list[str] = []\n    while True:\n        request = self._files.get(\n            fileId=file_or_folder_id,\n            fields=\"name,parents\",\n            supportsAllDrives=True,\n        )\n        response = request.execute()\n        names.append(response[\"name\"])\n        parents = response.get(\"parents\", [])\n        if not parents:\n            break\n        if len(parents) &gt; 1:\n            msg = \"Multiple parents exist. This case has not been handled yet. Contact developers.\"\n            raise OSError(msg)\n        file_or_folder_id = response[\"parents\"][0]\n    return \"/\".join(names[::-1])\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.read_only","title":"read_only","text":"<pre><code>read_only(file_id, read_only, reason=None)\n</code></pre> <p>Set a file to be in read-only or read-write mode.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The ID of a file.</p> required <code>read_only</code> <code>bool</code> <p>Whether to set the file to be in read-only mode.</p> required <code>reason</code> <code>str | None</code> <p>The reason for putting the file in read-only mode. Only used if <code>read_only</code> is <code>True</code>.</p> <code>None</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def read_only(self, file_id: str, read_only: bool, reason: str | None = None) -&gt; None:  # noqa: FBT001\n    \"\"\"Set a file to be in read-only or read-write mode.\n\n    Args:\n        file_id: The ID of a file.\n        read_only: Whether to set the file to be in read-only mode.\n        reason: The reason for putting the file in read-only mode.\n            Only used if `read_only` is `True`.\n    \"\"\"\n    restrictions: dict[str, bool | str] = {\"readOnly\": read_only}\n    if read_only:\n        restrictions[\"reason\"] = reason or \"\"\n\n        # If `file_id` is already in read-only mode, and it is being set\n        # to read-only mode then the API raises a TimeoutError waiting for\n        # a response. To avoid this error, check the mode and if it is\n        # already in read-only mode we are done.\n        if self.is_read_only(file_id):\n            return\n\n    self._files.update(\n        fileId=file_id, supportsAllDrives=True, body={\"contentRestrictions\": [restrictions]}\n    ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.rename","title":"rename","text":"<pre><code>rename(file_or_folder_id, new_name)\n</code></pre> <p>Rename a file or folder.</p> <p>Renaming a file or folder does not change its ID.</p> <p>Parameters:</p> Name Type Description Default <code>file_or_folder_id</code> <code>str</code> <p>The ID of a file or folder.</p> required <code>new_name</code> <code>str</code> <p>The new name of the file or folder.</p> required Source code in <code>src/msl/io/google_api.py</code> <pre><code>def rename(self, file_or_folder_id: str, new_name: str) -&gt; None:\n    \"\"\"Rename a file or folder.\n\n    Renaming a file or folder does not change its ID.\n\n    Args:\n        file_or_folder_id: The ID of a file or folder.\n        new_name: The new name of the file or folder.\n    \"\"\"\n    self._files.update(\n        fileId=file_or_folder_id,\n        supportsAllDrives=True,\n        body={\"name\": new_name},\n    ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.shared_drives","title":"shared_drives","text":"<pre><code>shared_drives()\n</code></pre> <p>Returns the IDs and names of all <code>Shared drives</code>.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The keys are the IDs of the shared drives and the values are the names of the shared drives.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def shared_drives(self) -&gt; dict[str, str]:\n    \"\"\"Returns the IDs and names of all `Shared drives`.\n\n    Returns:\n        The keys are the IDs of the shared drives and the values are the names of the shared drives.\n    \"\"\"\n    drives: dict[str, str] = {}\n    next_page_token = \"\"\n    while True:\n        response = self._drives.list(pageSize=100, pageToken=next_page_token).execute()\n        drives.update({d[\"id\"]: d[\"name\"] for d in response[\"drives\"]})\n        next_page_token = response.get(\"nextPageToken\")\n        if not next_page_token:\n            break\n    return drives\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GDrive.upload","title":"upload","text":"<pre><code>upload(\n    file,\n    *,\n    folder_id=None,\n    mime_type=None,\n    resumable=False,\n    chunk_size=DEFAULT_CHUNK_SIZE,\n)\n</code></pre> <p>Upload a file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>The file to upload.</p> required <code>folder_id</code> <code>str | None</code> <p>The ID of the folder to upload the file to. If not specified, uploads to the My Drive root folder.</p> <code>None</code> <code>mime_type</code> <code>str | None</code> <p>The Drive MIME type or media type of the file (e.g., <code>'text/csv'</code>). If not specified then a type will be guessed based on the file extension.</p> <code>None</code> <code>resumable</code> <code>bool</code> <p>Whether the upload can be resumed.</p> <code>False</code> <code>chunk_size</code> <code>int</code> <p>The file will be uploaded in chunks of this many bytes. Only used if <code>resumable</code> is <code>True</code>. Specify a value of -1 if the file is to be uploaded in a single chunk. Note that Google App Engine has a 5MB limit per request size, so you should not set <code>chunk_size</code> to be &gt; 5MB or to the value <code>-1</code> if the file size is &gt; 5MB.</p> <code>DEFAULT_CHUNK_SIZE</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the file that was uploaded.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def upload(\n    self,\n    file: PathLike,\n    *,\n    folder_id: str | None = None,\n    mime_type: str | None = None,\n    resumable: bool = False,\n    chunk_size: int = DEFAULT_CHUNK_SIZE,\n) -&gt; str:\n    \"\"\"Upload a file.\n\n    Args:\n        file: The file to upload.\n        folder_id: The ID of the folder to upload the file to. If not specified,\n            uploads to the *My Drive* root folder.\n        mime_type: The [Drive MIME type](https://developers.google.com/drive/api/guides/mime-types){:target=\"_blank\"}\n            or [media type](https://www.iana.org/assignments/media-types/media-types.xhtml){:target=\"_blank\"} of the\n            file (e.g., `'text/csv'`). If not specified then a type will be guessed based on the file extension.\n        resumable: Whether the upload can be resumed.\n        chunk_size: The file will be uploaded in chunks of this many bytes. Only used\n            if `resumable` is `True`. Specify a value of -1 if the file is to be uploaded\n            in a single chunk. Note that Google App Engine has a 5MB limit per request size,\n            so you should not set `chunk_size` to be &amp;gt; 5MB or to the value `-1` if the file size is &amp;gt; 5MB.\n\n    Returns:\n        The ID of the file that was uploaded.\n    \"\"\"\n    parent_id = folder_id or \"root\"\n    file = os.fsdecode(file)\n    filename = Path(file).name\n\n    body = {\"name\": filename, \"parents\": [parent_id]}\n    if mime_type:\n        body[\"mimeType\"] = mime_type\n\n    request = self._files.create(\n        body=body,\n        media_body=MediaFileUpload(file, mimetype=mime_type, chunksize=chunk_size, resumable=resumable),  # pyright: ignore[reportPossiblyUnboundVariable]\n        fields=\"id\",\n        supportsAllDrives=True,\n    )\n    response = request.execute()\n    return str(response[\"id\"])\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GMail","title":"GMail","text":"<pre><code>GMail(account=None, credentials=None, scopes=None)\n</code></pre> <p>               Bases: <code>GoogleAPI</code></p> <p>Interact with Gmail.</p> <p>Info</p> <p>You must follow the instructions in the prerequisites section for setting up the Gmail API before you can use this class. It is also useful to be aware of the refresh token expiration policy.</p> <p>Parameters:</p> Name Type Description Default <code>account</code> <code>str | None</code> <p>Since a person may have multiple Google accounts, and multiple people may run the same code, this parameter decides which token to load to authenticate with the Google API. The value can be any text (or <code>None</code>) that you want to associate with a particular Google account, provided that it contains valid characters for a filename. The value that you chose when you authenticated with your <code>credentials</code> should be used for all future instances of this class to access that particular Google account. You can associate a different value with a Google account at any time (by passing in a different <code>account</code> value), but you may be asked to authenticate with your <code>credentials</code> again, or, alternatively, you can rename the token files located in MSL_IO_DIR to match the new <code>account</code> value.</p> <code>None</code> <code>credentials</code> <code>PathLike | None</code> <p>The path to the client secrets OAuth credential file. This parameter only needs to be specified the first time that you authenticate with a particular Google account or if you delete the token file that was created when you previously authenticated.</p> <code>None</code> <code>scopes</code> <code>list[str] | None</code> <p>The list of scopes to enable for the Google API. See Gmail scopes for more details. If not specified then default scopes are chosen.</p> <code>None</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def __init__(\n    self,\n    account: str | None = None,\n    credentials: PathLike | None = None,\n    scopes: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Interact with Gmail.\n\n    !!! info\n        You must follow the instructions in the prerequisites section for setting up the\n        [Gmail API](https://developers.google.com/gmail/api/quickstart/python#prerequisites){:target=\"_blank\"}\n        before you can use this class. It is also useful to be aware of the\n        [refresh token expiration](https://developers.google.com/identity/protocols/oauth2#expiration){:target=\"_blank\"}\n        policy.\n\n    Args:\n        account: Since a person may have multiple Google accounts, and multiple people\n            may run the same code, this parameter decides which token to load to\n            authenticate with the Google API. The value can be any text (or `None`)\n            that you want to associate with a particular Google account, provided that\n            it contains valid characters for a filename. The value that you chose when\n            you authenticated with your `credentials` should be used for all future\n            instances of this class to access that particular Google account. You can\n            associate a different value with a Google account at any time (by passing\n            in a different `account` value), but you may be asked to authenticate with\n            your `credentials` again, or, alternatively, you can rename the token files\n            located in [MSL_IO_DIR][msl.io.constants.MSL_IO_DIR] to match the new\n            `account` value.\n        credentials: The path to the *client secrets* OAuth credential file. This\n            parameter only needs to be specified the first time that you\n            authenticate with a particular Google account or if you delete\n            the token file that was created when you previously authenticated.\n        scopes: The list of scopes to enable for the Google API. See\n            [Gmail scopes](https://developers.google.com/identity/protocols/oauth2/scopes#gmail){:target=\"_blank\"}\n            for more details. If not specified then default scopes are chosen.\n    \"\"\"\n    if not scopes:\n        scopes = [\"https://www.googleapis.com/auth/gmail.send\", \"https://www.googleapis.com/auth/gmail.metadata\"]\n\n    c = Path(os.fsdecode(credentials)) if credentials else None\n    super().__init__(service=\"gmail\", version=\"v1\", account=account, credentials=c, scopes=scopes, read_only=False)\n\n    self._my_email_address: str | None = None\n    self._users: Any = self._service.users()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GMail.profile","title":"profile","text":"<pre><code>profile()\n</code></pre> <p>Gets the authenticated user's GMail profile.</p> <p>Returns:</p> Type Description <code>Profile</code> <p>The current users GMail profile.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def profile(self) -&gt; Profile:\n    \"\"\"Gets the authenticated user's GMail profile.\n\n    Returns:\n        The current users GMail profile.\n    \"\"\"\n    profile = self._users.getProfile(userId=\"me\").execute()\n    return Profile(\n        email_address=str(profile[\"emailAddress\"]),\n        messages_total=int(profile[\"messagesTotal\"]),\n        threads_total=int(profile[\"threadsTotal\"]),\n        history_id=str(profile[\"historyId\"]),\n    )\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GMail.send","title":"send","text":"<pre><code>send(recipients, sender='me', subject=None, body=None)\n</code></pre> <p>Send an email.</p> <p>See also send_email.</p> <p>Parameters:</p> Name Type Description Default <code>recipients</code> <code>str | MutableSequence[str]</code> <p>The email address(es) of the recipient(s). The value <code>'me'</code> can be used to indicate the authenticated user.</p> required <code>sender</code> <code>str</code> <p>The email address of the sender. The value <code>'me'</code> can be used to indicate the authenticated user.</p> <code>'me'</code> <code>subject</code> <code>str | None</code> <p>The text to include in the subject field.</p> <code>None</code> <code>body</code> <code>str | None</code> <p>The text to include in the body of the email. The text can be enclosed in <code>&lt;html&gt;&lt;/html&gt;</code> tags to use HTML elements to format the message.</p> <code>None</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def send(\n    self,\n    recipients: str | MutableSequence[str],\n    sender: str = \"me\",\n    subject: str | None = None,\n    body: str | None = None,\n) -&gt; None:\n    \"\"\"Send an email.\n\n    !!! note \"See also [send_email][msl.io.utils.send_email].\"\n\n    Args:\n        recipients: The email address(es) of the recipient(s). The value `'me'` can be used\n            to indicate the authenticated user.\n        sender: The email address of the sender. The value `'me'` can be used to indicate\n            the authenticated user.\n        subject: The text to include in the subject field.\n        body: The text to include in the body of the email. The text can be enclosed in\n            `&lt;html&gt;&lt;/html&gt;` tags to use HTML elements to format the message.\n    \"\"\"\n    from base64 import b64encode  # noqa: PLC0415\n    from email.mime.multipart import MIMEMultipart  # noqa: PLC0415\n    from email.mime.text import MIMEText  # noqa: PLC0415\n\n    if isinstance(recipients, str):\n        recipients = [recipients]\n\n    for i in range(len(recipients)):\n        if recipients[i] == \"me\":\n            if self._my_email_address is None:\n                self._my_email_address = str(self.profile()[\"email_address\"])\n            recipients[i] = self._my_email_address\n\n    msg = MIMEMultipart()\n    msg[\"From\"] = sender\n    msg[\"To\"] = \", \".join(recipients)\n    msg[\"Subject\"] = subject or \"(no subject)\"\n\n    text = body or \"\"\n    subtype = \"html\" if text.startswith(\"&lt;html&gt;\") else \"plain\"\n    msg.attach(MIMEText(text, subtype))\n\n    self._users.messages().send(userId=sender, body={\"raw\": b64encode(msg.as_bytes()).decode()}).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets","title":"GSheets","text":"<pre><code>GSheets(\n    *,\n    account=None,\n    credentials=None,\n    scopes=None,\n    read_only=True,\n)\n</code></pre> <p>               Bases: <code>GoogleAPI</code></p> <p>Interact with Google Sheets.</p> <p>Info</p> <p>You must follow the instructions in the prerequisites section for setting up the Sheets API before you can use this class. It is also useful to be aware of the refresh token expiration policy.</p> <p>Parameters:</p> Name Type Description Default <code>account</code> <code>str | None</code> <p>Since a person may have multiple Google accounts, and multiple people may run the same code, this parameter decides which token to load to authenticate with the Google API. The value can be any text (or <code>None</code>) that you want to associate with a particular Google account, provided that it contains valid characters for a filename. The value that you chose when you authenticated with your <code>credentials</code> should be used for all future instances of this class to access that particular Google account. You can associate a different value with a Google account at any time (by passing in a different <code>account</code> value), but you may be asked to authenticate with your <code>credentials</code> again, or, alternatively, you can rename the token files located in MSL_IO_DIR to match the new <code>account</code> value.</p> <code>None</code> <code>credentials</code> <code>PathLike | None</code> <p>The path to the client secrets OAuth credential file. This parameter only needs to be specified the first time that you authenticate with a particular Google account or if you delete the token file that was created when you previously authenticated.</p> <code>None</code> <code>scopes</code> <code>list[str] | None</code> <p>The list of scopes to enable for the Google API. See Sheets scopes for more details. If not specified, default scopes are chosen based on the value of <code>read_only</code>.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Whether to interact with Google Sheets in read-only mode.</p> <code>True</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def __init__(\n    self,\n    *,\n    account: str | None = None,\n    credentials: PathLike | None = None,\n    scopes: list[str] | None = None,\n    read_only: bool = True,\n) -&gt; None:\n    \"\"\"Interact with Google Sheets.\n\n    !!! info\n        You must follow the instructions in the prerequisites section for setting up the\n        [Sheets API](https://developers.google.com/sheets/api/quickstart/python#prerequisites){:target=\"_blank\"}\n        before you can use this class. It is also useful to be aware of the\n        [refresh token expiration](https://developers.google.com/identity/protocols/oauth2#expiration){:target=\"_blank\"}\n        policy.\n\n    Args:\n        account: Since a person may have multiple Google accounts, and multiple people\n            may run the same code, this parameter decides which token to load\n            to authenticate with the Google API. The value can be any text (or\n            `None`) that you want to associate with a particular Google\n            account, provided that it contains valid characters for a filename.\n            The value that you chose when you authenticated with your `credentials`\n            should be used for all future instances of this class to access that\n            particular Google account. You can associate a different value with\n            a Google account at any time (by passing in a different `account`\n            value), but you may be asked to authenticate with your `credentials`\n            again, or, alternatively, you can rename the token files located in\n            [MSL_IO_DIR][msl.io.constants.MSL_IO_DIR] to match the new `account` value.\n        credentials: The path to the *client secrets* OAuth credential file. This\n            parameter only needs to be specified the first time that you\n            authenticate with a particular Google account or if you delete\n            the token file that was created when you previously authenticated.\n        scopes: The list of scopes to enable for the Google API. See\n            [Sheets scopes](https://developers.google.com/identity/protocols/oauth2/scopes#sheets){:target=\"_blank\"}\n            for more details. If not specified, default scopes are chosen based on the value of `read_only`.\n        read_only: Whether to interact with Google Sheets in read-only mode.\n    \"\"\"\n    if not scopes:\n        if read_only:\n            scopes = [\"https://www.googleapis.com/auth/spreadsheets.readonly\"]\n        else:\n            scopes = [\"https://www.googleapis.com/auth/spreadsheets\"]\n\n    c = Path(os.fsdecode(credentials)) if credentials else None\n    super().__init__(\n        service=\"sheets\", version=\"v4\", account=account, credentials=c, scopes=scopes, read_only=read_only\n    )\n\n    self._spreadsheets: Any = self._service.spreadsheets()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.add_sheets","title":"add_sheets","text":"<pre><code>add_sheets(names, spreadsheet_id)\n</code></pre> <p>Add sheets to a spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str | Iterable[str]</code> <p>The name(s) of the new sheet(s) to add.</p> required <code>spreadsheet_id</code> <code>str</code> <p>The ID of the spreadsheet to add the sheet(s) to.</p> required <p>Returns:</p> Type Description <code>dict[int, str]</code> <p>The keys are the IDs of the new sheets and the values are the names.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def add_sheets(self, names: str | Iterable[str], spreadsheet_id: str) -&gt; dict[int, str]:\n    \"\"\"Add sheets to a spreadsheet.\n\n    Args:\n        names: The name(s) of the new sheet(s) to add.\n        spreadsheet_id: The ID of the spreadsheet to add the sheet(s) to.\n\n    Returns:\n        The keys are the IDs of the new sheets and the values are the names.\n    \"\"\"\n    if isinstance(names, str):\n        names = [names]\n\n    response = self._spreadsheets.batchUpdate(\n        spreadsheetId=spreadsheet_id,\n        body={\"requests\": [{\"addSheet\": {\"properties\": {\"title\": name}}} for name in names]},\n    ).execute()\n\n    return {\n        r[\"addSheet\"][\"properties\"][\"sheetId\"]: r[\"addSheet\"][\"properties\"][\"title\"] for r in response[\"replies\"]\n    }\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.append","title":"append","text":"<pre><code>append(\n    values,\n    spreadsheet_id,\n    cell=None,\n    sheet=None,\n    *,\n    row_major=True,\n    raw=False,\n)\n</code></pre> <p>Append values to a sheet.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Any | list[Any] | tuple[Any, ...] | list[list[Any]] | tuple[tuple[Any, ...], ...]</code> <p>The value(s) to append</p> required <code>spreadsheet_id</code> <code>str</code> <p>The ID of a Google Sheets file.</p> required <code>cell</code> <code>str | None</code> <p>The cell (top-left corner) to start appending the values to. If the cell already contains data then new rows are inserted and the values are written to the new rows. For example, <code>'D100'</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of a sheet in the spreadsheet to append the values to. If not specified and only one sheet exists in the spreadsheet then automatically determines the sheet name; however, it is more efficient to specify the name of the sheet.</p> <code>None</code> <code>row_major</code> <code>bool</code> <p>Whether to append the values in row-major or column-major order.</p> <code>True</code> <code>raw</code> <code>bool</code> <p>Determines how the values should be interpreted. If <code>True</code>, the values will not be parsed and will be stored as-is. If <code>False</code>, the values will be parsed as if the user typed them into the UI. Numbers will stay as numbers, but strings may be converted to numbers, dates, etc. following the same rules that are applied when entering text into a cell via the Google Sheets UI.</p> <code>False</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def append(  # noqa: PLR0913\n    self,\n    values: Any | list[Any] | tuple[Any, ...] | list[list[Any]] | tuple[tuple[Any, ...], ...],\n    spreadsheet_id: str,\n    cell: str | None = None,\n    sheet: str | None = None,\n    *,\n    row_major: bool = True,\n    raw: bool = False,\n) -&gt; None:\n    \"\"\"Append values to a sheet.\n\n    Args:\n        values: The value(s) to append\n        spreadsheet_id: The ID of a Google Sheets file.\n        cell: The cell (top-left corner) to start appending the values to. If the\n            cell already contains data then new rows are inserted and the values\n            are written to the new rows. For example, `'D100'`.\n        sheet: The name of a sheet in the spreadsheet to append the values to.\n            If not specified and only one sheet exists in the spreadsheet\n            then automatically determines the sheet name; however, it is\n            more efficient to specify the name of the sheet.\n        row_major: Whether to append the values in row-major or column-major order.\n        raw: Determines how the values should be interpreted. If `True`,\n            the values will not be parsed and will be stored as-is. If\n            `False`, the values will be parsed as if the user typed\n            them into the UI. Numbers will stay as numbers, but strings may\n            be converted to numbers, dates, etc. following the same rules\n            that are applied when entering text into a cell via the Google\n            Sheets UI.\n    \"\"\"\n    self._spreadsheets.values().append(\n        spreadsheetId=spreadsheet_id,\n        range=self._get_range(sheet, cell, spreadsheet_id),\n        valueInputOption=\"RAW\" if raw else \"USER_ENTERED\",\n        insertDataOption=\"INSERT_ROWS\",\n        body={\n            \"values\": self._values(values),\n            \"majorDimension\": \"ROWS\" if row_major else \"COLUMNS\",\n        },\n    ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.cells","title":"cells","text":"<pre><code>cells(spreadsheet_id, ranges=None)\n</code></pre> <p>Return cells from a spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>spreadsheet_id</code> <code>str</code> <p>The ID of a Google Sheets file.</p> required <code>ranges</code> <code>str | list[str] | None</code> <p>The ranges to retrieve from the spreadsheet. If not specified then return all cells from all sheets. For example,</p> <ul> <li><code>'Sheet1'</code> \u2192 Return all cells from the sheet named <code>Sheet1</code></li> <li><code>'Sheet1!A1:H5'</code> \u2192 Return cells <code>A1:H5</code> from the sheet named <code>Sheet1</code></li> <li><code>['Sheet1!A1:H5', 'Data', 'Devices!B4:B9']</code> \u2192 Return cells <code>A1:H5</code>     from the sheet named <code>Sheet1</code>, all cells from the sheet named <code>Data</code>     and cells <code>B4:B9</code> from the sheet named Devices</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[list[GCell]]]</code> <p>The cells from the spreadsheet. The keys are the names of the sheets.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def cells(self, spreadsheet_id: str, ranges: str | list[str] | None = None) -&gt; dict[str, list[list[GCell]]]:  # noqa: C901\n    \"\"\"Return cells from a spreadsheet.\n\n    Args:\n        spreadsheet_id: The ID of a Google Sheets file.\n        ranges: The ranges to retrieve from the spreadsheet. If not specified then return all cells\n            from all sheets. For example,\n\n            * `'Sheet1'` &amp;#8594; Return all cells from the sheet named `Sheet1`\n            * `'Sheet1!A1:H5'` &amp;#8594; Return cells `A1:H5` from the sheet named `Sheet1`\n            * `['Sheet1!A1:H5', 'Data', 'Devices!B4:B9']` &amp;#8594; Return cells `A1:H5`\n                from the sheet named `Sheet1`, all cells from the sheet named `Data`\n                and cells `B4:B9` from the sheet named Devices\n\n    Returns:\n        The cells from the spreadsheet. The keys are the names of the sheets.\n    \"\"\"\n    response = self._spreadsheets.get(\n        spreadsheetId=spreadsheet_id,\n        includeGridData=True,\n        ranges=ranges,\n    ).execute()\n    cells: dict[str, list[list[GCell]]] = {}\n    for sheet in response[\"sheets\"]:\n        data: list[list[GCell]] = []\n        for item in sheet[\"data\"]:\n            for row in item.get(\"rowData\", []):\n                row_data: list[GCell] = []\n                for col in row.get(\"values\", []):\n                    effective_value = col.get(\"effectiveValue\", None)\n                    formatted = col.get(\"formattedValue\", \"\")\n                    if effective_value is None:\n                        value = None\n                        typ = GCellType.EMPTY\n                    elif \"numberValue\" in effective_value:\n                        value = effective_value[\"numberValue\"]\n                        t = col.get(\"effectiveFormat\", {}).get(\"numberFormat\", {}).get(\"type\", \"NUMBER\")\n                        try:\n                            typ = GCellType(t)\n                        except ValueError:\n                            typ = GCellType.UNKNOWN\n                    elif \"stringValue\" in effective_value:\n                        value = effective_value[\"stringValue\"]\n                        typ = GCellType.STRING\n                    elif \"boolValue\" in effective_value:\n                        value = effective_value[\"boolValue\"]\n                        typ = GCellType.BOOLEAN\n                    elif \"errorValue\" in effective_value:\n                        msg = effective_value[\"errorValue\"][\"message\"]\n                        value = f\"{col['formattedValue']} ({msg})\"\n                        typ = GCellType.ERROR\n                    else:\n                        value = formatted\n                        typ = GCellType.UNKNOWN\n                    row_data.append(GCell(value=value, type=typ, formatted=formatted))\n                data.append(row_data)\n            cells[sheet[\"properties\"][\"title\"]] = data\n    return cells\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.copy","title":"copy","text":"<pre><code>copy(\n    name_or_id, spreadsheet_id, destination_spreadsheet_id\n)\n</code></pre> <p>Copy a sheet from one spreadsheet to another spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>str | int</code> <p>The name or ID of the sheet to copy.</p> required <code>spreadsheet_id</code> <code>str</code> <p>The ID of the spreadsheet that contains the sheet.</p> required <code>destination_spreadsheet_id</code> <code>str</code> <p>The ID of a spreadsheet to copy the sheet to.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The ID of the sheet in the destination spreadsheet.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def copy(self, name_or_id: str | int, spreadsheet_id: str, destination_spreadsheet_id: str) -&gt; int:\n    \"\"\"Copy a sheet from one spreadsheet to another spreadsheet.\n\n    Args:\n        name_or_id: The name or ID of the sheet to copy.\n        spreadsheet_id: The ID of the spreadsheet that contains the sheet.\n        destination_spreadsheet_id: The ID of a spreadsheet to copy the sheet to.\n\n    Returns:\n        The ID of the sheet in the destination spreadsheet.\n    \"\"\"\n    sheet_id = name_or_id if isinstance(name_or_id, int) else self.sheet_id(name_or_id, spreadsheet_id)\n\n    response = (\n        self._spreadsheets.sheets()\n        .copyTo(\n            spreadsheetId=spreadsheet_id,\n            sheetId=sheet_id,\n            body={\n                \"destination_spreadsheet_id\": destination_spreadsheet_id,\n            },\n        )\n        .execute()\n    )\n    return int(response[\"sheetId\"])\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.create","title":"create","text":"<pre><code>create(name, sheet_names=None)\n</code></pre> <p>Create a new spreadsheet.</p> <p>The spreadsheet will be created in the My Drive root folder. To move it to a different folder use GDrive.create_folder and/or GDrive.move.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the spreadsheet.</p> required <code>sheet_names</code> <code>Iterable[str] | None</code> <p>The names of the sheets that will be in the new spreadsheet.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the spreadsheet that was created.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def create(self, name: str, sheet_names: Iterable[str] | None = None) -&gt; str:\n    \"\"\"Create a new spreadsheet.\n\n    The spreadsheet will be created in the *My Drive* root folder.\n    To move it to a different folder use [GDrive.create_folder][msl.io.google_api.GDrive.create_folder]\n    and/or [GDrive.move][msl.io.google_api.GDrive.move].\n\n    Args:\n        name: The name of the spreadsheet.\n        sheet_names: The names of the sheets that will be in the new spreadsheet.\n\n    Returns:\n        The ID of the spreadsheet that was created.\n    \"\"\"\n    body: dict[str, dict[str, str] | list[dict[str, dict[str, str]]]] = {\"properties\": {\"title\": name}}\n    if sheet_names:\n        body[\"sheets\"] = [{\"properties\": {\"title\": sn}} for sn in sheet_names]\n    response = self._spreadsheets.create(body=body).execute()\n    return str(response[\"spreadsheetId\"])\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.delete_sheets","title":"delete_sheets","text":"<pre><code>delete_sheets(names_or_ids, spreadsheet_id)\n</code></pre> <p>Delete sheets from a spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>names_or_ids</code> <code>str | int | Iterable[str | int]</code> <p>The name(s) or ID(s) of the sheet(s) to delete.</p> required <code>spreadsheet_id</code> <code>str</code> <p>The ID of the spreadsheet to delete the sheet(s) from.</p> required Source code in <code>src/msl/io/google_api.py</code> <pre><code>def delete_sheets(self, names_or_ids: str | int | Iterable[str | int], spreadsheet_id: str) -&gt; None:\n    \"\"\"Delete sheets from a spreadsheet.\n\n    Args:\n        names_or_ids: The name(s) or ID(s) of the sheet(s) to delete.\n        spreadsheet_id: The ID of the spreadsheet to delete the sheet(s) from.\n    \"\"\"\n    if isinstance(names_or_ids, (str, int)):\n        names_or_ids = [names_or_ids]\n\n    self._spreadsheets.batchUpdate(\n        spreadsheetId=spreadsheet_id,\n        body={\n            \"requests\": [\n                {\"deleteSheet\": {\"sheetId\": n if isinstance(n, int) else self.sheet_id(n, spreadsheet_id)}}\n                for n in names_or_ids\n            ]\n        },\n    ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.rename_sheet","title":"rename_sheet","text":"<pre><code>rename_sheet(name_or_id, new_name, spreadsheet_id)\n</code></pre> <p>Rename a sheet.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>str | int</code> <p>The name or ID of the sheet to rename.</p> required <code>new_name</code> <code>str</code> <p>The new name of the sheet.</p> required <code>spreadsheet_id</code> <code>str</code> <p>The ID of the spreadsheet that contains the sheet.</p> required Source code in <code>src/msl/io/google_api.py</code> <pre><code>def rename_sheet(self, name_or_id: str | int, new_name: str, spreadsheet_id: str) -&gt; None:\n    \"\"\"Rename a sheet.\n\n    Args:\n        name_or_id: The name or ID of the sheet to rename.\n        new_name: The new name of the sheet.\n        spreadsheet_id: The ID of the spreadsheet that contains the sheet.\n    \"\"\"\n    sheet_id = name_or_id if isinstance(name_or_id, int) else self.sheet_id(name_or_id, spreadsheet_id)\n\n    self._spreadsheets.batchUpdate(\n        spreadsheetId=spreadsheet_id,\n        body={\n            \"requests\": [\n                {\n                    \"updateSheetProperties\": {\n                        \"properties\": {\n                            \"sheetId\": sheet_id,\n                            \"title\": new_name,\n                        },\n                        \"fields\": \"title\",\n                    }\n                }\n            ]\n        },\n    ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.sheet_id","title":"sheet_id","text":"<pre><code>sheet_id(name, spreadsheet_id)\n</code></pre> <p>Returns the ID of a sheet.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the sheet.</p> required <code>spreadsheet_id</code> <code>str</code> <p>The ID of the spreadsheet.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The ID of the sheet.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def sheet_id(self, name: str, spreadsheet_id: str) -&gt; int:\n    \"\"\"Returns the ID of a sheet.\n\n    Args:\n        name: The name of the sheet.\n        spreadsheet_id: The ID of the spreadsheet.\n\n    Returns:\n        The ID of the sheet.\n    \"\"\"\n    request = self._spreadsheets.get(spreadsheetId=spreadsheet_id)\n    response = request.execute()\n    for sheet in response[\"sheets\"]:\n        if sheet[\"properties\"][\"title\"] == name:\n            return int(sheet[\"properties\"][\"sheetId\"])\n\n    msg = f\"A sheet named {name!r} does not exist\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.sheet_names","title":"sheet_names","text":"<pre><code>sheet_names(spreadsheet_id)\n</code></pre> <p>Get the names of all sheets in a spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>spreadsheet_id</code> <code>str</code> <p>The ID of a Google Sheets file.</p> required <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>The names of all sheets.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def sheet_names(self, spreadsheet_id: str) -&gt; tuple[str, ...]:\n    \"\"\"Get the names of all sheets in a spreadsheet.\n\n    Args:\n        spreadsheet_id: The ID of a Google Sheets file.\n\n    Returns:\n        The names of all sheets.\n    \"\"\"\n    request = self._spreadsheets.get(spreadsheetId=spreadsheet_id)\n    response = request.execute()\n    return tuple(r[\"properties\"][\"title\"] for r in response[\"sheets\"])\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.to_datetime","title":"to_datetime  <code>staticmethod</code>","text":"<pre><code>to_datetime(value)\n</code></pre> <p>Convert a serial number date into a datetime.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>A date in the serial number format.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>The date converted.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>@staticmethod\ndef to_datetime(value: float) -&gt; datetime:\n    \"\"\"Convert a *serial number* date into a [datetime][datetime.datetime].\n\n    Args:\n        value: A date in the *serial number* format.\n\n    Returns:\n        The date converted.\n    \"\"\"\n    days = int(value)\n    seconds = (value - days) * 86400  # 60 * 60 * 24\n    return GSheets.SERIAL_NUMBER_ORIGIN + timedelta(days=days, seconds=seconds)\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.values","title":"values","text":"<pre><code>values(\n    spreadsheet_id,\n    sheet=None,\n    cells=None,\n    *,\n    row_major=True,\n    value_option=GValueOption.FORMATTED,\n    datetime_option=GDateTimeOption.SERIAL_NUMBER,\n)\n</code></pre> <p>Return a range of values from a spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>spreadsheet_id</code> <code>str</code> <p>The ID of a Google Sheets file.</p> required <code>sheet</code> <code>str | None</code> <p>The name of a sheet in the spreadsheet to read the values from. If not specified and only one sheet exists in the spreadsheet then automatically determines the sheet name; however, it is more efficient to specify the name of the sheet.</p> <code>None</code> <code>cells</code> <code>str | None</code> <p>The <code>A1</code> notation or <code>R1C1</code> notation of the range to retrieve values from. If not specified then returns all values that are in <code>sheet</code>.</p> <code>None</code> <code>row_major</code> <code>bool</code> <p>Whether to return the values in row-major or column-major order.</p> <code>True</code> <code>value_option</code> <code>str | GValueOption</code> <p>How values should be represented in the output. If a str, it must be equal to one of the values in GValueOption.</p> <code>FORMATTED</code> <code>datetime_option</code> <code>str | GDateTimeOption</code> <p>How dates, times, and durations should be represented in the output. If a str, it must be equal to one of the values in GDateTimeOption. This argument is ignored if <code>value_option</code> is GValueOption.FORMATTED.</p> <code>SERIAL_NUMBER</code> <p>Returns:</p> Type Description <code>list[list[Any]]</code> <p>The values from the sheet.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def values(  # noqa: PLR0913\n    self,\n    spreadsheet_id: str,\n    sheet: str | None = None,\n    cells: str | None = None,\n    *,\n    row_major: bool = True,\n    value_option: str | GValueOption = GValueOption.FORMATTED,\n    datetime_option: str | GDateTimeOption = GDateTimeOption.SERIAL_NUMBER,\n) -&gt; list[list[Any]]:\n    \"\"\"Return a range of values from a spreadsheet.\n\n    Args:\n        spreadsheet_id: The ID of a Google Sheets file.\n        sheet: The name of a sheet in the spreadsheet to read the values from.\n            If not specified and only one sheet exists in the spreadsheet\n            then automatically determines the sheet name; however, it is\n            more efficient to specify the name of the sheet.\n        cells: The `A1` notation or `R1C1` notation of the range to retrieve values\n            from. If not specified then returns all values that are in `sheet`.\n        row_major: Whether to return the values in row-major or column-major order.\n        value_option: How values should be represented in the output. If a [str][],\n            it must be equal to one of the values in [GValueOption][msl.io.google_api.GValueOption].\n        datetime_option: How dates, times, and durations should be represented in the\n            output. If a [str][], it must be equal to one of the values in\n            [GDateTimeOption][msl.io.google_api.GDateTimeOption]. This argument is ignored if `value_option` is\n            [GValueOption.FORMATTED][msl.io.google_api.GValueOption.FORMATTED].\n\n    Returns:\n        The values from the sheet.\n    \"\"\"\n    if isinstance(value_option, GValueOption):\n        value_option = value_option.value\n\n    if isinstance(datetime_option, GDateTimeOption):\n        datetime_option = datetime_option.value\n\n    response = (\n        self._spreadsheets.values()\n        .get(\n            spreadsheetId=spreadsheet_id,\n            range=self._get_range(sheet, cells, spreadsheet_id),\n            majorDimension=\"ROWS\" if row_major else \"COLUMNS\",\n            valueRenderOption=value_option,\n            dateTimeRenderOption=datetime_option,\n        )\n        .execute()\n    )\n    return response.get(\"values\", [[]])  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GSheets.write","title":"write","text":"<pre><code>write(\n    values,\n    spreadsheet_id,\n    cell=None,\n    sheet=None,\n    *,\n    row_major=True,\n    raw=False,\n)\n</code></pre> <p>Write values to a sheet.</p> <p>If a cell that is being written to already contains a value, the value in that cell is overwritten with the new value.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Any | list[Any] | tuple[Any, ...] | list[list[Any]] | tuple[tuple[Any, ...], ...]</code> <p>The value(s) to write.</p> required <code>spreadsheet_id</code> <code>str</code> <p>The ID of a Google Sheets file.</p> required <code>cell</code> <code>str | None</code> <p>The cell (top-left corner) to start writing the values to. For example, <code>'C9'</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of a sheet in the spreadsheet to write the values to. If not specified and only one sheet exists in the spreadsheet then automatically determines the sheet name; however, it is more efficient to specify the name of the sheet.</p> <code>None</code> <code>row_major</code> <code>bool</code> <p>Whether to write the values in row-major or column-major order.</p> <code>True</code> <code>raw</code> <code>bool</code> <p>Determines how the values should be interpreted. If <code>True</code>, the values will not be parsed and will be stored as-is. If <code>False</code>, the values will be parsed as if the user typed them into the UI. Numbers will stay as numbers, but strings may be converted to numbers, dates, etc. following the same rules that are applied when entering text into a cell via the Google Sheets UI.</p> <code>False</code> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def write(  # noqa: PLR0913\n    self,\n    values: Any | list[Any] | tuple[Any, ...] | list[list[Any]] | tuple[tuple[Any, ...], ...],\n    spreadsheet_id: str,\n    cell: str | None = None,\n    sheet: str | None = None,\n    *,\n    row_major: bool = True,\n    raw: bool = False,\n) -&gt; None:\n    \"\"\"Write values to a sheet.\n\n    If a cell that is being written to already contains a value,\n    the value in that cell is overwritten with the new value.\n\n    Args:\n        values: The value(s) to write.\n        spreadsheet_id: The ID of a Google Sheets file.\n        cell: The cell (top-left corner) to start writing the values to. For example, `'C9'`.\n        sheet: The name of a sheet in the spreadsheet to write the values to.\n            If not specified and only one sheet exists in the spreadsheet\n            then automatically determines the sheet name; however, it is\n            more efficient to specify the name of the sheet.\n        row_major: Whether to write the values in row-major or column-major order.\n        raw: Determines how the values should be interpreted. If `True`,\n            the values will not be parsed and will be stored as-is. If\n            `False`, the values will be parsed as if the user typed\n            them into the UI. Numbers will stay as numbers, but strings may\n            be converted to numbers, dates, etc. following the same rules\n            that are applied when entering text into a cell via the Google\n            Sheets UI.\n    \"\"\"\n    self._spreadsheets.values().update(\n        spreadsheetId=spreadsheet_id,\n        range=self._get_range(sheet, cell, spreadsheet_id),\n        valueInputOption=\"RAW\" if raw else \"USER_ENTERED\",\n        body={\n            \"values\": self._values(values),\n            \"majorDimension\": \"ROWS\" if row_major else \"COLUMNS\",\n        },\n    ).execute()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GValueOption","title":"GValueOption","text":"<p>               Bases: <code>Enum</code></p> <p>Determines how values should be returned.</p>"},{"location":"api/google/#msl.io.google_api.GValueOption.FORMATTED","title":"FORMATTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FORMATTED = 'FORMATTED_VALUE'\n</code></pre> <p>Values will be calculated and formatted in the reply according to the cell's formatting. Formatting is based on the spreadsheet's locale, not the requesting user's locale. For example, if A1 is 1.23 and A2 is =A1 and formatted as currency, then A2 would return \"$1.23\".</p>"},{"location":"api/google/#msl.io.google_api.GValueOption.FORMULA","title":"FORMULA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FORMULA = 'FORMULA'\n</code></pre> <p>Values will not be calculated. The reply will include the formulas. For example, if A1 is 1.23 and A2 is =A1 and formatted as currency, then A2 would return \"=A1\".</p>"},{"location":"api/google/#msl.io.google_api.GValueOption.UNFORMATTED","title":"UNFORMATTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNFORMATTED = 'UNFORMATTED_VALUE'\n</code></pre> <p>Values will be calculated, but not formatted in the reply. For example, if A1 is 1.23 and A2 is =A1 and formatted as currency, then A2 would return the number 1.23.</p>"},{"location":"api/google/#msl.io.google_api.GoogleAPI","title":"GoogleAPI","text":"<pre><code>GoogleAPI(\n    *,\n    service,\n    version,\n    account,\n    credentials,\n    scopes,\n    read_only,\n)\n</code></pre> <p>Base class for all Google APIs.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    *,\n    service: str,\n    version: str,\n    account: str | None,\n    credentials: Path | None,\n    scopes: list[str],\n    read_only: bool,\n) -&gt; None:\n    \"\"\"Base class for all Google APIs.\"\"\"\n    name = f\"{account}-\" if account else \"\"\n    readonly = \"-readonly\" if read_only else \"\"\n    token = MSL_IO_DIR / f\"{name}token-{service}{readonly}.json\"\n    oauth = _authenticate(token, credentials, scopes)\n    self._service: Any = build(service, version, credentials=oauth)  # pyright: ignore[reportPossiblyUnboundVariable]\n</code></pre>"},{"location":"api/google/#msl.io.google_api.GoogleAPI.service","title":"service  <code>property</code>","text":"<pre><code>service\n</code></pre> <p>The Resource object with methods for interacting with the API service.</p>"},{"location":"api/google/#msl.io.google_api.GoogleAPI.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the connection to the API service.</p> Source code in <code>src/msl/io/google_api.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the connection to the API service.\"\"\"\n    self._service.close()\n</code></pre>"},{"location":"api/google/#msl.io.google_api.Profile","title":"Profile  <code>dataclass</code>","text":"<pre><code>Profile(\n    email_address, messages_total, threads_total, history_id\n)\n</code></pre> <p>An authenticated user's Gmail profile.</p> <p>Attributes:</p> Name Type Description <code>email_address</code> <code>str</code> <p>The authenticated user's email address</p> <code>messages_total</code> <code>int</code> <p>The total number of messages in the mailbox</p> <code>threads_total</code> <code>int</code> <p>The total number of threads in the mailbox</p> <code>history_id</code> <code>str</code> <p>The ID of the mailbox's current history record</p>"},{"location":"api/metadata/","title":"metadata","text":"<p>Provides information about data.</p>"},{"location":"api/metadata/#msl.io.metadata.Metadata","title":"Metadata","text":"<pre><code>Metadata(*, read_only, node_name, **kwargs)\n</code></pre> <p>               Bases: <code>FreezableMap[Any]</code></p> <p>Provides information about data.</p> <p>Warning</p> <p>Do not instantiate directly. A Metadata object is created automatically when create_dataset or create_group is called.</p> <p>Parameters:</p> Name Type Description Default <code>read_only</code> <code>bool</code> <p>Whether Metadata is initialised in read-only mode.</p> required <code>node_name</code> <code>str</code> <p>The name of the node that the Metadata is associated with.</p> required <code>kwargs</code> <code>Any</code> <p>All other keyword arguments are used to create the mapping.</p> <code>{}</code> Source code in <code>src/msl/io/metadata.py</code> <pre><code>def __init__(self, *, read_only: bool, node_name: str, **kwargs: Any) -&gt; None:\n    \"\"\"Provides information about data.\n\n    !!! warning\n        Do not instantiate directly. A [Metadata][msl.io.metadata.Metadata] object is created automatically\n        when [create_dataset][msl.io.node.Group.create_dataset] or\n        [create_group][msl.io.node.Group.create_group] is called.\n\n    Args:\n        read_only: Whether [Metadata][msl.io.metadata.Metadata] is initialised in read-only mode.\n        node_name: The name of the node that the [Metadata][msl.io.metadata.Metadata] is associated with.\n        kwargs: All other keyword arguments are used to create the mapping.\n    \"\"\"\n    meta = {k: _value(value=v, read_only=read_only, name=node_name) for k, v in kwargs.items()}\n    super().__init__(read_only=read_only, **meta)\n    self._node_name: str = node_name\n</code></pre>"},{"location":"api/metadata/#msl.io.metadata.Metadata.read_only","title":"read_only  <code>property</code> <code>writable</code>","text":"<pre><code>read_only\n</code></pre> <p>bool \u2014 Whether the map is in read-only mode.</p>"},{"location":"api/metadata/#msl.io.metadata.Metadata.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Maybe remove all items from the map, only if the map is not in read-only mode.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Maybe remove all items from the map, only if the map is not in read-only mode.\"\"\"\n    self._raise_if_read_only()\n    self._mapping.clear()\n</code></pre>"},{"location":"api/metadata/#msl.io.metadata.Metadata.copy","title":"copy","text":"<pre><code>copy(*, read_only=None)\n</code></pre> <p>Create a copy of the Metadata.</p> <p>Parameters:</p> Name Type Description Default <code>read_only</code> <code>bool | None</code> <p>Whether the copy should be created in read-only mode. If <code>None</code>, creates a copy using the mode for the Metadata object that is being copied.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metadata</code> <p>A copy of the Metadata.</p> Source code in <code>src/msl/io/metadata.py</code> <pre><code>def copy(self, *, read_only: bool | None = None) -&gt; Metadata:\n    \"\"\"Create a copy of the [Metadata][msl.io.metadata.Metadata].\n\n    Args:\n        read_only: Whether the copy should be created in read-only mode.\n            If `None`, creates a copy using the mode for the [Metadata][msl.io.metadata.Metadata]\n            object that is being copied.\n\n    Returns:\n        A copy of the [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    ro = self._read_only if read_only is None else read_only\n    return Metadata(read_only=ro, node_name=self._node_name, **self._mapping)\n</code></pre>"},{"location":"api/metadata/#msl.io.metadata.Metadata.fromkeys","title":"fromkeys","text":"<pre><code>fromkeys(seq, value=None, *, read_only=None)\n</code></pre> <p>Create a new Metadata object with keys from <code>seq</code> and values set to <code>value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Iterable[str]</code> <p>Any iterable object that contains the names of the keys.</p> required <code>value</code> <code>Any</code> <p>The default value to use for each key.</p> <code>None</code> <code>read_only</code> <code>bool | None</code> <p>Whether the returned Metadata object should be initialised in read-only mode. If <code>None</code>, uses the mode for the Metadata that is used to call this method.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metadata</code> <p>A new Metadata object.</p> Source code in <code>src/msl/io/metadata.py</code> <pre><code>def fromkeys(self, seq: Iterable[str], value: Any = None, *, read_only: bool | None = None) -&gt; Metadata:\n    \"\"\"Create a new [Metadata][msl.io.metadata.Metadata] object with keys from `seq` and values set to `value`.\n\n    Args:\n        seq: Any iterable object that contains the names of the keys.\n        value: The default value to use for each key.\n        read_only: Whether the returned [Metadata][msl.io.metadata.Metadata] object should be initialised in\n            read-only mode. If `None`, uses the mode for the [Metadata][msl.io.metadata.Metadata] that is used\n            to call this method.\n\n    Returns:\n        A new [Metadata][msl.io.metadata.Metadata] object.\n    \"\"\"\n    ro = self._read_only if read_only is None else read_only\n    return Metadata(read_only=ro, node_name=self._node_name, **dict.fromkeys(seq, value))\n</code></pre>"},{"location":"api/metadata/#msl.io.metadata.Metadata.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>ItemsView[str, VT] \u2014 Return a view of the map's items.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def items(self) -&gt; ItemsView[str, VT]:\n    \"\"\"[ItemsView][collections.abc.ItemsView][[str][], VT] &amp;mdash; Return a view of the map's items.\"\"\"\n    return ItemsView(self)\n</code></pre>"},{"location":"api/metadata/#msl.io.metadata.Metadata.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>KeysView[str] \u2014 Returns a view of the map's keys.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def keys(self) -&gt; KeysView[str]:\n    \"\"\"[KeysView][collections.abc.KeysView][[str][]] &amp;mdash; Returns a view of the map's keys.\"\"\"\n    return KeysView(self)\n</code></pre>"},{"location":"api/metadata/#msl.io.metadata.Metadata.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>ValuesView[VT] \u2014 Returns a view of the map's values.</p> Source code in <code>src/msl/io/freezable.py</code> <pre><code>def values(self) -&gt; ValuesView[VT]:\n    \"\"\"[ValuesView][collections.abc.ValuesView][VT] &amp;mdash; Returns a view of the map's values.\"\"\"\n    return ValuesView(self)\n</code></pre>"},{"location":"api/node/","title":"node","text":"<p>Node objects in the hierarchical tree.</p>"},{"location":"api/node/#msl.io.node.Dataset","title":"Dataset","text":"<pre><code>Dataset(\n    *,\n    name,\n    parent,\n    read_only,\n    shape=(0,),\n    dtype=float,\n    buffer=None,\n    offset=0,\n    strides=None,\n    order=None,\n    data=None,\n    **metadata,\n)\n</code></pre> <p>               Bases: <code>NDArrayOperatorsMixin</code>, <code>Sequence[Any]</code></p> <p>A Dataset functions as a numpy ndarray with Metadata.</p> <p>Warning</p> <p>Do not instantiate directly. Create a new Dataset using the create_dataset method.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A name to associate with this Dataset.</p> required <code>parent</code> <code>Group | None</code> <p>The parent Group to the Dataset.</p> required <code>read_only</code> <code>bool</code> <p>Whether the Dataset is initialised in read-only mode.</p> required <code>shape</code> <code>ShapeLike</code> <p>See numpy ndarray. Only used if <code>data</code> is <code>None</code>.</p> <code>(0,)</code> <code>dtype</code> <code>DTypeLike</code> <p>See numpy ndarray. Only used if <code>data</code> is <code>None</code> or if <code>data</code> is not a numpy ndarray instance.</p> <code>float</code> <code>buffer</code> <code>Buffer | None</code> <p>See numpy ndarray. Only used if <code>data</code> is <code>None</code>.</p> <code>None</code> <code>offset</code> <code>SupportsIndex</code> <p>See numpy ndarray. Only used if <code>data</code> is <code>None</code>.</p> <code>0</code> <code>strides</code> <code>ShapeLike | None</code> <p>See numpy ndarray. Only used if <code>data</code> is <code>None</code>.</p> <code>None</code> <code>order</code> <code>Literal['K', 'A', 'C', 'F'] | None</code> <p>See numpy ndarray. Only used if <code>data</code> is <code>None</code> or if <code>data</code> is not a numpy ndarray instance.</p> <code>None</code> <code>data</code> <code>ArrayLike | None</code> <p>If not <code>None</code>, it must be either a numpy ndarray or an array-like object which will be passed to asarray, as well as <code>dtype</code> and <code>order</code>, to be used as the underlying data.</p> <code>None</code> <code>metadata</code> <code>Any</code> <p>All other keyword arguments are used as Metadata for this Dataset.</p> <code>{}</code> Source code in <code>src/msl/io/node.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    *,\n    name: str,\n    parent: Group | None,\n    read_only: bool,\n    shape: ShapeLike = (0,),\n    dtype: DTypeLike = float,\n    buffer: Buffer | None = None,\n    offset: SupportsIndex = 0,\n    strides: ShapeLike | None = None,\n    order: Literal[\"K\", \"A\", \"C\", \"F\"] | None = None,\n    data: ArrayLike | None = None,\n    **metadata: Any,\n) -&gt; None:\n    \"\"\"A *Dataset* functions as a numpy [ndarray][numpy.ndarray] with [Metadata][msl.io.metadata.Metadata].\n\n    !!! warning\n        Do not instantiate directly. Create a new [Dataset][msl.io.node.Dataset] using the\n        [create_dataset][msl.io.node.Group.create_dataset] method.\n\n    Args:\n        name: A name to associate with this [Dataset][msl.io.node.Dataset].\n        parent: The parent [Group][msl.io.node.Group] to the [Dataset][msl.io.node.Dataset].\n        read_only: Whether the [Dataset][msl.io.node.Dataset] is initialised in read-only mode.\n        shape: See numpy [ndarray][numpy.ndarray]. Only used if `data` is `None`.\n        dtype: See numpy [ndarray][numpy.ndarray]. Only used if `data` is `None` or if\n            `data` is not a numpy [ndarray][numpy.ndarray] instance.\n        buffer: See numpy [ndarray][numpy.ndarray]. Only used if `data` is `None`.\n        offset: See numpy [ndarray][numpy.ndarray]. Only used if `data` is `None`.\n        strides: See numpy [ndarray][numpy.ndarray]. Only used if `data` is `None`.\n        order: See numpy [ndarray][numpy.ndarray]. Only used if `data` is `None` or if\n            `data` is not a numpy [ndarray][numpy.ndarray] instance.\n        data: If not `None`, it must be either a numpy [ndarray][numpy.ndarray] or\n            an array-like object which will be passed to [asarray][numpy.asarray],\n            as well as `dtype` and `order`, to be used as the underlying data.\n        metadata: All other keyword arguments are used as\n            [Metadata][msl.io.metadata.Metadata] for this [Dataset][msl.io.node.Dataset].\n    \"\"\"\n    name = _unix_name(name, parent)\n    self._name: str = name\n    self._parent: Group | None = parent\n    self._metadata: Metadata = Metadata(read_only=read_only, node_name=name, **metadata)\n\n    self._data: NDArray[Any]\n    if data is None:\n        self._data = np.ndarray(shape, dtype=dtype, buffer=buffer, offset=offset, strides=strides, order=order)\n    elif isinstance(data, np.ndarray):\n        self._data = data\n    elif isinstance(data, Dataset):\n        self._data = data.data\n    else:\n        self._data = np.asarray(data, dtype=dtype, order=order)\n\n    self.read_only = read_only\n    _notify_created(self, parent)\n</code></pre>"},{"location":"api/node/#msl.io.node.Dataset.data","title":"data  <code>property</code>","text":"<pre><code>data\n</code></pre> <p>ndarray \u2014 The data of the Dataset.</p> <p>Tip</p> <p>You do not have to call this attribute to access the underlying numpy ndarray. You can directly call any ndarray attribute from the Dataset instance.</p> <p>For example,</p> <pre><code>&gt;&gt;&gt; dataset\n&lt;Dataset '/my_data' shape=(4, 3) dtype='&lt;f8' (0 metadata)&gt;\n&gt;&gt;&gt; dataset.data\narray([[ 0.,  1.,  2.],\n       [ 3.,  4.,  5.],\n       [ 6.,  7.,  8.],\n       [ 9., 10., 11.]])\n&gt;&gt;&gt; dataset.size\n12\n&gt;&gt;&gt; dataset.tolist()\n[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0], [9.0, 10.0, 11.0]]\n&gt;&gt;&gt; dataset.mean(axis=0)\narray([4.5, 5.5, 6.5])\n&gt;&gt;&gt; dataset[::2]\narray([[0., 1., 2.],\n       [6., 7., 8.]])\n</code></pre>"},{"location":"api/node/#msl.io.node.Dataset.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata\n</code></pre> <p>Metadata \u2014 The metadata for this Dataset.</p>"},{"location":"api/node/#msl.io.node.Dataset.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>str \u2014 The name of this Dataset.</p>"},{"location":"api/node/#msl.io.node.Dataset.parent","title":"parent  <code>property</code>","text":"<pre><code>parent\n</code></pre> <p>Group | None \u2014 The parent of this Dataset.</p>"},{"location":"api/node/#msl.io.node.Dataset.read_only","title":"read_only  <code>property</code> <code>writable</code>","text":"<pre><code>read_only\n</code></pre> <p>bool \u2014 Whether the Dataset is in read-only mode.</p> <p>This is equivalent to setting the <code>WRITEABLE</code> property in numpy.ndarray.setflags.</p>"},{"location":"api/node/#msl.io.node.Dataset.add_metadata","title":"add_metadata","text":"<pre><code>add_metadata(**metadata)\n</code></pre> <p>Add metadata to the Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Any</code> <p>Key-value pairs to add to the Metadata for this Dataset.</p> <code>{}</code> Source code in <code>src/msl/io/node.py</code> <pre><code>def add_metadata(self, **metadata: Any) -&gt; None:\n    \"\"\"Add metadata to the [Dataset][msl.io.node.Dataset].\n\n    Args:\n        metadata: Key-value pairs to add to the [Metadata][msl.io.metadata.Metadata] for this\n            [Dataset][msl.io.node.Dataset].\n    \"\"\"\n    self._metadata.update(**metadata)\n</code></pre>"},{"location":"api/node/#msl.io.node.Dataset.copy","title":"copy","text":"<pre><code>copy(*, read_only=None)\n</code></pre> <p>Create a copy of this Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>read_only</code> <code>bool | None</code> <p>Whether the copy should be created in read-only mode. If <code>None</code>, creates a copy using the mode for the Dataset that is being copied.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>A copy of this Dataset.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def copy(self, *, read_only: bool | None = None) -&gt; Dataset:\n    \"\"\"Create a copy of this [Dataset][msl.io.node.Dataset].\n\n    Args:\n        read_only: Whether the copy should be created in read-only mode. If `None`,\n            creates a copy using the mode for the [Dataset][msl.io.node.Dataset] that is being copied.\n\n    Returns:\n        A copy of this [Dataset][msl.io.node.Dataset].\n    \"\"\"\n    return Dataset(\n        name=self._name,\n        parent=self._parent,\n        read_only=self.read_only if read_only is None else read_only,\n        data=self._data.copy(),\n        **self._metadata.copy(),\n    )\n</code></pre>"},{"location":"api/node/#msl.io.node.DatasetLogging","title":"DatasetLogging","text":"<pre><code>DatasetLogging(\n    *,\n    name,\n    parent,\n    attributes,\n    level=logging.NOTSET,\n    logger=None,\n    date_fmt=None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>A Dataset that handles logging records.</p> <p>Warning</p> <p>Do not instantiate directly. Create a new DatasetLogging using create_dataset_logging.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A name to associate with the Dataset.</p> required <code>parent</code> <code>Group | None</code> <p>The parent to the <code>DatasetLogging</code>.</p> required <code>attributes</code> <code>Sequence[str]</code> <p>The attribute names to include in the Dataset for each logging record.</p> required <code>level</code> <code>str | int</code> <p>The logging level to use.</p> <code>NOTSET</code> <code>logger</code> <code>Logger | None</code> <p>The Logger that this <code>DatasetLogging</code> instance will be associated with. If <code>None</code>, it is associated with the root Logger.</p> <code>None</code> <code>date_fmt</code> <code>str | None</code> <p>The datetime format code to use to represent the asctime attribute (only if asctime is included as one of the <code>attributes</code>).</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to Dataset. The default behaviour is to append every logging record to the Dataset. This guarantees that the size of the Dataset is equal to the number of logging records that were added to it. However, this behaviour can decrease performance if many logging records are added often because a copy of the data in the Dataset is created for each logging record that is added. You can improve performance by specifying an initial size of the Dataset by including a <code>shape</code> or a <code>size</code> keyword argument. This will also automatically allocate more memory that is proportional to the size of the Dataset, if the size of the Dataset needs to be increased. If you do this then you will want to call remove_empty_rows before writing <code>DatasetLogging</code> to a file or interacting with the data in <code>DatasetLogging</code> to remove the extra empty rows that were created.</p> <code>{}</code> Source code in <code>src/msl/io/node.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    *,\n    name: str,\n    parent: Group | None,\n    attributes: Sequence[str],\n    level: str | int = logging.NOTSET,\n    logger: logging.Logger | None = None,\n    date_fmt: str | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"A [Dataset][msl.io.node.Dataset] that handles [logging][]{:target=\"_blank\"} records.\n\n    !!! warning\n        Do not instantiate directly. Create a new [DatasetLogging][msl.io.node.DatasetLogging] using\n        [create_dataset_logging][msl.io.node.Group.create_dataset_logging].\n\n    Args:\n        name: A name to associate with the [Dataset][msl.io.node.Dataset].\n        parent: The parent to the `DatasetLogging`.\n        attributes: The [attribute names][logrecord-attributes] to include in the\n            [Dataset][msl.io.node.Dataset] for each [logging record][log-record].\n        level: The [logging level][levels] to use.\n        logger: The [Logger][logging.Logger] that this `DatasetLogging` instance\n            will be associated with. If `None`, it is associated with the _root_ [Logger][logging.Logger].\n        date_fmt: The [datetime][datetime.datetime] [format code][strftime-strptime-behavior]\n            to use to represent the _asctime_ [attribute][logrecord-attributes] (only if\n            _asctime_ is included as one of the `attributes`).\n        kwargs: All additional keyword arguments are passed to [Dataset][msl.io.node.Dataset].\n            The default behaviour is to append every [logging record][log-record]\n            to the [Dataset][msl.io.node.Dataset]. This guarantees that the size of the\n            [Dataset][msl.io.node.Dataset] is equal to the number of [logging records][log-record]\n            that were added to it. However, this behaviour can decrease performance if many\n            [logging records][log-record] are added often because a copy of the data in the\n            [Dataset][msl.io.node.Dataset] is created for each [logging record][log-record]\n            that is added. You can improve performance by specifying an initial size of the\n            [Dataset][msl.io.node.Dataset] by including a `shape` or a `size` keyword argument.\n            This will also automatically allocate more memory that is proportional to the size of the\n            [Dataset][msl.io.node.Dataset], if the size of the\n            [Dataset][msl.io.node.Dataset] needs to be increased. If you do this then you will\n            want to call [remove_empty_rows][msl.io.node.DatasetLogging.remove_empty_rows] before\n            writing `DatasetLogging` to a file or interacting with the data in `DatasetLogging` to\n            remove the extra _empty_ rows that were created.\n    \"\"\"\n    if not attributes:\n        msg = \"Must specify logging attributes\"\n        raise ValueError(msg)\n\n    if not all(isinstance(a, str) for a in attributes):  # pyright: ignore[reportUnnecessaryIsInstance]\n        msg = f\"Must specify attribute names as strings, got: {attributes}\"\n        raise ValueError(msg)\n\n    self._logger: logging.Logger | None = None\n    self._attributes: tuple[str, ...] = tuple(attributes)\n    self._uses_asctime: bool = \"asctime\" in attributes\n    self._date_fmt: str = date_fmt or \"%Y-%m-%dT%H:%M:%S.%f\"\n\n    _level: int = getattr(logging, level) if isinstance(level, str) else int(level)\n\n    # these 3 keys in the metadata are used to distinguish a DatasetLogging\n    # object from a regular Dataset object\n    kwargs[\"logging_level\"] = _level\n    kwargs[\"logging_level_name\"] = logging.getLevelName(_level)\n    kwargs[\"logging_date_format\"] = date_fmt\n\n    self._auto_resize: bool = \"size\" in kwargs or \"shape\" in kwargs\n    if self._auto_resize:\n        if \"size\" in kwargs:\n            kwargs[\"shape\"] = (kwargs.pop(\"size\"),)\n        elif isinstance(kwargs[\"shape\"], int):\n            kwargs[\"shape\"] = (kwargs[\"shape\"],)\n\n        shape = kwargs[\"shape\"]\n        if len(shape) != 1:\n            msg = f\"Invalid shape {shape}, the number of dimensions must be 1\"\n            raise ValueError(msg)\n        if shape[0] &lt; 0:\n            msg = f\"Invalid shape {shape}\"\n            raise ValueError(msg)\n\n    self._dtype: np.dtype[np.object_ | np.void] = np.dtype([(a, object) for a in attributes])\n    super().__init__(name=name, parent=parent, read_only=False, dtype=self._dtype, **kwargs)\n\n    self._index: int | np.intp = np.count_nonzero(self._data)\n    if self._auto_resize and self._data.shape &lt; kwargs[\"shape\"]:\n        self._resize(new_allocated=kwargs[\"shape\"][0])\n\n    self._handler: logging.Handler = logging.Handler(level=_level)\n    self._handler.set_name(self.name)\n    self._handler.emit = self._emit  # type: ignore[method-assign]\n    self.set_logger(logger or logging.getLogger())\n</code></pre>"},{"location":"api/node/#msl.io.node.DatasetLogging.attributes","title":"attributes  <code>property</code>","text":"<pre><code>attributes\n</code></pre> <p>tuple[str, ...] \u2014 The attribute names that are logged.</p>"},{"location":"api/node/#msl.io.node.DatasetLogging.date_fmt","title":"date_fmt  <code>property</code>","text":"<pre><code>date_fmt\n</code></pre> <p>str \u2014 The datetime format code.</p>"},{"location":"api/node/#msl.io.node.DatasetLogging.level","title":"level  <code>property</code> <code>writable</code>","text":"<pre><code>level\n</code></pre> <p>int \u2014 The logging level that is used.</p>"},{"location":"api/node/#msl.io.node.DatasetLogging.logger","title":"logger  <code>property</code>","text":"<pre><code>logger\n</code></pre> <p>Logger | <code>None</code> \u2014 The Logger for this <code>DatasetLogging</code>.</p>"},{"location":"api/node/#msl.io.node.DatasetLogging.add_filter","title":"add_filter","text":"<pre><code>add_filter(log_filter)\n</code></pre> <p>Add a logging filter.</p> <p>Parameters:</p> Name Type Description Default <code>log_filter</code> <code>Filter</code> <p>The logging Filter to add to the Handler</p> required Source code in <code>src/msl/io/node.py</code> <pre><code>def add_filter(self, log_filter: logging.Filter) -&gt; None:\n    \"\"\"Add a logging filter.\n\n    Args:\n        log_filter: The logging [Filter][logging.Filter] to add to the [Handler][logging.Handler]\n    \"\"\"\n    self._handler.addFilter(log_filter)\n</code></pre>"},{"location":"api/node/#msl.io.node.DatasetLogging.remove_empty_rows","title":"remove_empty_rows","text":"<pre><code>remove_empty_rows()\n</code></pre> <p>Remove empty rows from the Dataset.</p> <p>If the DatasetLogging object was initialized with a <code>shape</code> or a <code>size</code> keyword argument then the size of the Dataset is always greater than or equal to the number of logging records that were added to it. Calling this method will remove the rows in the Dataset that were not from a logging record.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def remove_empty_rows(self) -&gt; None:\n    \"\"\"Remove empty rows from the [Dataset][msl.io.node.Dataset].\n\n    If the [DatasetLogging][msl.io.node.DatasetLogging] object was initialized with a `shape` or a `size`\n    keyword argument then the size of the [Dataset][msl.io.node.Dataset] is always greater than or equal to\n    the number of [logging records][log-record] that were added to it. Calling this method will remove the\n    rows in the [Dataset][msl.io.node.Dataset] that were not from a [logging record][log-record].\n    \"\"\"\n    assert self._dtype.names is not None  # noqa: S101\n\n    # don't use \"is not None\" since this does not work as expected\n    self._data: NDArray[Any] = self._data[self._data[self._dtype.names[0]] != None]  # noqa: E711\n</code></pre>"},{"location":"api/node/#msl.io.node.DatasetLogging.remove_filter","title":"remove_filter","text":"<pre><code>remove_filter(log_filter)\n</code></pre> <p>Remove a logging filter.</p> <p>Parameters:</p> Name Type Description Default <code>log_filter</code> <code>Filter</code> <p>The logging Filter to remove from the Handler.</p> required Source code in <code>src/msl/io/node.py</code> <pre><code>def remove_filter(self, log_filter: logging.Filter) -&gt; None:\n    \"\"\"Remove a logging filter.\n\n    Args:\n        log_filter: The logging [Filter][logging.Filter] to remove from the [Handler][logging.Handler].\n    \"\"\"\n    self._handler.removeFilter(log_filter)\n</code></pre>"},{"location":"api/node/#msl.io.node.DatasetLogging.remove_handler","title":"remove_handler","text":"<pre><code>remove_handler()\n</code></pre> <p>Remove this class's Handler from the associated Logger.</p> <p>After calling this method logging records are no longer added to the Dataset.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def remove_handler(self) -&gt; None:\n    \"\"\"Remove this class's [Handler][logging.Handler] from the associated [Logger][logging.Logger].\n\n    After calling this method [logging records][log-record] are no longer added\n    to the [Dataset][msl.io.node.Dataset].\n    \"\"\"\n    if self._logger is not None:\n        self._logger.removeHandler(self._handler)\n</code></pre>"},{"location":"api/node/#msl.io.node.DatasetLogging.set_logger","title":"set_logger","text":"<pre><code>set_logger(logger)\n</code></pre> <p>Add this class's Handler to a Logger.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The Logger to add this class's Handler to.</p> required Source code in <code>src/msl/io/node.py</code> <pre><code>def set_logger(self, logger: logging.Logger) -&gt; None:\n    \"\"\"Add this class's [Handler][logging.Handler] to a [Logger][logging.Logger].\n\n    Args:\n        logger: The [Logger][logging.Logger] to add this class's [Handler][logging.Handler] to.\n    \"\"\"\n    level = self._handler.level\n    if logger.level == 0 or logger.level &gt; level:\n        logger.setLevel(level)\n\n    self.remove_handler()\n    logger.addHandler(self._handler)\n    self._logger = logger\n</code></pre>"},{"location":"api/node/#msl.io.node.Group","title":"Group","text":"<pre><code>Group(*, name, parent, read_only, **metadata)\n</code></pre> <p>               Bases: <code>FreezableMap['Dataset | Group']</code></p> <p>A Group can contain sub-Groups and/or Datasets.</p> <p>Warning</p> <p>Do not instantiate directly. Create a new Group using create_group.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of this Group. Uses a naming convention analogous to UNIX file systems where each Group can be thought of as a directory and where every subdirectory is separated from its parent directory by the <code>/</code> character.</p> required <code>parent</code> <code>Group | None</code> <p>The parent to this Group.</p> required <code>read_only</code> <code>bool</code> <p>Whether the Group is initialised in read-only mode.</p> required <code>metadata</code> <code>Any</code> <p>All additional keyword arguments are used to create the Metadata for this Group.</p> <code>{}</code> Source code in <code>src/msl/io/node.py</code> <pre><code>def __init__(self, *, name: str, parent: Group | None, read_only: bool, **metadata: Any) -&gt; None:\n    \"\"\"A [Group][msl.io.node.Group] can contain sub-[Group][msl.io.node.Group]s and/or [Dataset][msl.io.node.Dataset]s.\n\n    !!! warning\n        Do not instantiate directly. Create a new [Group][msl.io.node.Group] using\n        [create_group][msl.io.node.Group.create_group].\n\n    Args:\n        name: The name of this [Group][msl.io.node.Group]. Uses a naming convention analogous to UNIX\n            file systems where each [Group][msl.io.node.Group] can be thought\n            of as a directory and where every subdirectory is separated from its\n            parent directory by the `/` character.\n        parent: The parent to this [Group][msl.io.node.Group].\n        read_only: Whether the [Group][msl.io.node.Group] is initialised in read-only mode.\n        metadata: All additional keyword arguments are used to create the\n            [Metadata][msl.io.metadata.Metadata] for this [Group][msl.io.node.Group].\n    \"\"\"  # noqa: E501\n    name = _unix_name(name, parent)\n    self._name: str = name\n    self._parent: Group | None = parent\n    self._metadata: Metadata = Metadata(read_only=read_only, node_name=name, **metadata)\n    _notify_created(self, parent)\n    super().__init__(read_only=read_only)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata\n</code></pre> <p>Metadata \u2014 The metadata for this Group.</p>"},{"location":"api/node/#msl.io.node.Group.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>str \u2014 The name of this Group.</p>"},{"location":"api/node/#msl.io.node.Group.parent","title":"parent  <code>property</code>","text":"<pre><code>parent\n</code></pre> <p>Group | None \u2014 The parent of this Group.</p>"},{"location":"api/node/#msl.io.node.Group.read_only","title":"read_only  <code>property</code> <code>writable</code>","text":"<pre><code>read_only\n</code></pre> <p>bool \u2014 Whether this Group is in read-only mode.</p> <p>Setting this value will also update all sub-Groups and sub-Datasets to be in the same mode.</p>"},{"location":"api/node/#msl.io.node.Group.add_dataset","title":"add_dataset","text":"<pre><code>add_dataset(name, dataset)\n</code></pre> <p>Add a Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new Dataset to add. Automatically creates the ancestor Groups if they do not exist.</p> required <code>dataset</code> <code>Dataset</code> <p>The Dataset to add. The data and the Metadata in the Dataset are copied.</p> required Source code in <code>src/msl/io/node.py</code> <pre><code>def add_dataset(self, name: str, dataset: Dataset) -&gt; None:\n    \"\"\"Add a [Dataset][msl.io.node.Dataset].\n\n    Args:\n        name: The name of the new [Dataset][msl.io.node.Dataset] to add. Automatically creates the ancestor\n            [Group][msl.io.node.Group]s if they do not exist.\n        dataset: The [Dataset][msl.io.node.Dataset] to add. The data and the\n            [Metadata][msl.io.metadata.Metadata] in the [Dataset][msl.io.node.Dataset] are copied.\n    \"\"\"\n    if not isinstance(dataset, Dataset):  # pyright: ignore[reportUnnecessaryIsInstance]\n        msg = f\"Must pass in a Dataset object, got {dataset!r}\"  # type: ignore[unreachable] # pyright: ignore[reportUnreachable]\n        raise TypeError(msg)\n\n    name = \"/\" + name.strip(\"/\")\n    _ = self.create_dataset(name, read_only=dataset.read_only, data=dataset.data.copy(), **dataset.metadata.copy())\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.add_dataset_logging","title":"add_dataset_logging","text":"<pre><code>add_dataset_logging(name, dataset_logging)\n</code></pre> <p>Add a DatasetLogging.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new DatasetLogging to add. Automatically creates the ancestor Groups if they do not exist.</p> required <code>dataset_logging</code> <code>DatasetLogging</code> <p>The DatasetLogging to add. The data and Metadata are copied.</p> required Source code in <code>src/msl/io/node.py</code> <pre><code>def add_dataset_logging(self, name: str, dataset_logging: DatasetLogging) -&gt; None:\n    \"\"\"Add a [DatasetLogging][msl.io.node.DatasetLogging].\n\n    Args:\n        name: The name of the new [DatasetLogging][msl.io.node.DatasetLogging] to add.\n            Automatically creates the ancestor [Group][msl.io.node.Group]s if they do not exist.\n        dataset_logging: The [DatasetLogging][msl.io.node.DatasetLogging] to add. The\n            data and [Metadata][msl.io.metadata.Metadata] are copied.\n    \"\"\"\n    if not isinstance(dataset_logging, DatasetLogging):  # pyright: ignore[reportUnnecessaryIsInstance]\n        msg = f\"Must pass in a DatasetLogging object, got {dataset_logging!r}\"  # type: ignore[unreachable] # pyright: ignore[reportUnreachable]\n        raise TypeError(msg)\n\n    name = \"/\" + name.strip(\"/\")\n    _ = self.create_dataset_logging(\n        name,\n        level=dataset_logging.level,\n        attributes=dataset_logging.attributes,\n        logger=dataset_logging.logger,\n        date_fmt=dataset_logging.date_fmt,\n        data=dataset_logging.data.copy(),\n        **dataset_logging.metadata.copy(),\n    )\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.add_group","title":"add_group","text":"<pre><code>add_group(name, group)\n</code></pre> <p>Add a Group.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new Group to add. Automatically creates the ancestor Groups if they do not exist.</p> required <code>group</code> <code>Group</code> <p>The Group to add. The Datasets and Metadata that are contained within the <code>group</code> will be copied.</p> required Source code in <code>src/msl/io/node.py</code> <pre><code>def add_group(self, name: str, group: Group) -&gt; None:\n    \"\"\"Add a [Group][msl.io.node.Group].\n\n    Args:\n        name: The name of the new [Group][msl.io.node.Group] to add. Automatically creates the ancestor\n            [Group][msl.io.node.Group]s if they do not exist.\n        group: The [Group][msl.io.node.Group] to add. The [Dataset][msl.io.node.Dataset]s and\n            [Metadata][msl.io.metadata.Metadata] that are contained within the\n            `group` will be copied.\n    \"\"\"\n    if not isinstance(group, Group):  # pyright: ignore[reportUnnecessaryIsInstance]\n        msg = f\"Must pass in a Group object, got {group!r}\"  # type: ignore[unreachable] # pyright: ignore[reportUnreachable]\n        raise TypeError(msg)\n\n    name = \"/\" + name.strip(\"/\")\n\n    if not group:  # no sub-Groups or Datasets, only add the Metadata\n        _ = self.create_group(name + group.name, **group.metadata.copy())\n        return\n\n    for key, node in group.items():\n        n = name + key\n        if isinstance(node, Group):\n            _ = self.create_group(n, read_only=node.read_only, **node.metadata.copy())\n        else:  # must be a Dataset\n            _ = self.create_dataset(n, read_only=node.read_only, data=node.data.copy(), **node.metadata.copy())\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.add_metadata","title":"add_metadata","text":"<pre><code>add_metadata(**metadata)\n</code></pre> <p>Add metadata to the Group.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Any</code> <p>Key-value pairs to add to the Metadata for this Group.</p> <code>{}</code> Source code in <code>src/msl/io/node.py</code> <pre><code>def add_metadata(self, **metadata: Any) -&gt; None:\n    \"\"\"Add metadata to the [Group][msl.io.node.Group].\n\n    Args:\n        metadata: Key-value pairs to add to the [Metadata][msl.io.metadata.Metadata] for this\n            [Group][msl.io.node.Group].\n    \"\"\"\n    self._metadata.update(**metadata)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.ancestors","title":"ancestors","text":"<pre><code>ancestors()\n</code></pre> <p>Yield all ancestor (parent) Groups of this Group.</p> <p>Yields:</p> Type Description <code>Group</code> <p>The ancestors of this Group.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def ancestors(self) -&gt; Iterator[Group]:\n    \"\"\"Yield all ancestor (parent) [Group][msl.io.node.Group]s of this [Group][msl.io.node.Group].\n\n    Yields:\n        The ancestors of this [Group][msl.io.node.Group].\n    \"\"\"\n    parent = self.parent\n    while parent is not None:\n        yield parent\n        parent = parent.parent\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(name, *, read_only=None, **kwargs)\n</code></pre> <p>Create a new Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new Dataset. Automatically creates the ancestor Groups if they do not exist. See here for an example.</p> required <code>read_only</code> <code>bool | None</code> <p>Whether to create the new Dataset in read-only mode. If <code>None</code>, uses the mode for this Group.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to Dataset.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The new Dataset that was created.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def create_dataset(self, name: str, *, read_only: bool | None = None, **kwargs: Any) -&gt; Dataset:\n    \"\"\"Create a new [Dataset][msl.io.node.Dataset].\n\n    Args:\n        name: The name of the new [Dataset][msl.io.node.Dataset]. Automatically creates the ancestor\n            [Group][msl.io.node.Group]s if they do not exist. See [here][automatic-group-creation]\n            for an example.\n        read_only: Whether to create the new [Dataset][msl.io.node.Dataset] in read-only mode.\n            If `None`, uses the mode for this [Group][msl.io.node.Group].\n        kwargs: All additional keyword arguments are passed to [Dataset][msl.io.node.Dataset].\n\n    Returns:\n        The new [Dataset][msl.io.node.Dataset] that was created.\n    \"\"\"\n    read_only, kwargs = self._check(read_only=read_only, **kwargs)\n    name, parent = self._create_ancestors(name, read_only=read_only)\n    return Dataset(name=name, parent=parent, read_only=read_only, **kwargs)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.create_dataset_logging","title":"create_dataset_logging","text":"<pre><code>create_dataset_logging(\n    name,\n    *,\n    level=\"INFO\",\n    attributes=None,\n    logger=None,\n    date_fmt=None,\n    **kwargs,\n)\n</code></pre> <p>Create a Dataset that handles logging records.</p> <p>See here for an example.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A name to associate with the Dataset. Automatically creates the ancestor Groups if they do not exist.</p> required <code>level</code> <code>str | int</code> <p>The logging level to use.</p> <code>'INFO'</code> <code>attributes</code> <code>Sequence[str] | None</code> <p>The attribute names to include in the Dataset for each logging record. If <code>None</code>, uses asctime, levelname, name, and message.</p> <code>None</code> <code>logger</code> <code>Logger | None</code> <p>The Logger that the DatasetLogging object will be associated with. If <code>None</code>, it is associated with the root Logger.</p> <code>None</code> <code>date_fmt</code> <code>str | None</code> <p>The datetime format code to use to represent the asctime attribute in. If <code>None</code>, uses the ISO 8601 format <code>\"%Y-%m-%dT%H:%M:%S.%f\"</code>.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to Dataset. The default behaviour is to append every logging record to the Dataset. This guarantees that the size of the Dataset is equal to the number of logging records that were added to it. However, this behaviour can decrease performance if many logging records are added often because a copy of the data in the Dataset is created for each logging record that is added. You can improve performance by specifying an initial size of the Dataset by including a <code>shape</code> or a <code>size</code> keyword argument. This will also automatically create additional empty rows in the Dataset, that is proportional to the size of the Dataset, if the size of the Dataset needs to be increased. If you do this then you will want to call remove_empty_rows before writing DatasetLogging to a file or interacting with the data in DatasetLogging to remove the empty rows that were created.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DatasetLogging</code> <p>The DatasetLogging that was created.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def create_dataset_logging(\n    self,\n    name: str,\n    *,\n    level: str | int = \"INFO\",\n    attributes: Sequence[str] | None = None,\n    logger: Logger | None = None,\n    date_fmt: str | None = None,\n    **kwargs: Any,\n) -&gt; DatasetLogging:\n    \"\"\"Create a [Dataset][msl.io.node.Dataset] that handles [logging][] records.\n\n    !!! example \"See [here][msl-io-dataset-logging] for an example.\"\n\n    Args:\n        name: A name to associate with the [Dataset][msl.io.node.Dataset].\n            Automatically creates the ancestor [Group][msl.io.node.Group]s if they do not exist.\n        level: The [logging level][levels] to use.\n        attributes: The [attribute names][logrecord-attributes] to include in the\n            [Dataset][msl.io.node.Dataset] for each [logging record][log-record].\n            If `None`, uses _asctime_, _levelname_, _name_, and _message_.\n        logger: The [Logger][logging.Logger] that the [DatasetLogging][msl.io.node.DatasetLogging] object\n            will be associated with. If `None`, it is associated with the _root_ [Logger][logging.Logger].\n        date_fmt: The [datetime][datetime.datetime] [format code][strftime-strptime-behavior]\n            to use to represent the _asctime_ [attribute][logrecord-attributes] in.\n            If `None`, uses the ISO 8601 format `\"%Y-%m-%dT%H:%M:%S.%f\"`.\n        kwargs: All additional keyword arguments are passed to [Dataset][msl.io.node.Dataset].\n            The default behaviour is to append every [logging record][log-record]\n            to the [Dataset][msl.io.node.Dataset]. This guarantees that the size of the\n            [Dataset][msl.io.node.Dataset] is equal to the number of\n            [logging records][log-record] that were added to it. However, this behaviour\n            can decrease performance if many [logging records][log-record] are\n            added often because a copy of the data in the [Dataset][msl.io.node.Dataset] is\n            created for each [logging record][log-record] that is added. You can improve\n            performance by specifying an initial size of the [Dataset][msl.io.node.Dataset]\n            by including a `shape` or a `size` keyword argument. This will also automatically\n            create additional empty rows in the [Dataset][msl.io.node.Dataset], that is\n            proportional to the size of the [Dataset][msl.io.node.Dataset], if the size of the\n            [Dataset][msl.io.node.Dataset] needs to be increased. If you do this then you will\n            want to call [remove_empty_rows][msl.io.node.DatasetLogging.remove_empty_rows] before\n            writing [DatasetLogging][msl.io.node.DatasetLogging] to a file or interacting\n            with the data in [DatasetLogging][msl.io.node.DatasetLogging] to remove the\n            _empty_ rows that were created.\n\n    Returns:\n        The [DatasetLogging][msl.io.node.DatasetLogging] that was created.\n    \"\"\"\n    read_only, metadata = self._check(read_only=False, **kwargs)\n    name, parent = self._create_ancestors(name, read_only=read_only)\n    if attributes is None:\n        # if the default attribute names are changed then update the `attributes`\n        # description in the docstring of create_dataset_logging() and require_dataset_logging()\n        attributes = [\"asctime\", \"levelname\", \"name\", \"message\"]\n    if date_fmt is None:\n        # if the default date_fmt is changed then update the `date_fmt`\n        # description in the docstring of create_dataset_logging() and require_dataset_logging()\n        date_fmt = \"%Y-%m-%dT%H:%M:%S.%f\"\n    return DatasetLogging(\n        name=name, parent=parent, level=level, attributes=attributes, logger=logger, date_fmt=date_fmt, **metadata\n    )\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.create_group","title":"create_group","text":"<pre><code>create_group(name, *, read_only=None, **metadata)\n</code></pre> <p>Create a new Group.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new Group. Automatically creates the ancestor Groups if they do not exist.</p> required <code>read_only</code> <code>bool | None</code> <p>Whether to create the new Group in read-only mode. If <code>None</code>, uses the mode for this Group.</p> <code>None</code> <code>metadata</code> <code>Any</code> <p>All additional keyword arguments are used to create the Metadata for the new Group.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The new Group that was created.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def create_group(self, name: str, *, read_only: bool | None = None, **metadata: Any) -&gt; Group:\n    \"\"\"Create a new [Group][msl.io.node.Group].\n\n    Args:\n        name: The name of the new [Group][msl.io.node.Group]. Automatically creates the ancestor\n            [Group][msl.io.node.Group]s if they do not exist.\n        read_only: Whether to create the new [Group][msl.io.node.Group] in read-only mode.\n            If `None`, uses the mode for this [Group][msl.io.node.Group].\n        metadata: All additional keyword arguments are used to create the [Metadata][msl.io.metadata.Metadata]\n            for the new [Group][msl.io.node.Group].\n\n    Returns:\n        The new [Group][msl.io.node.Group] that was created.\n    \"\"\"\n    read_only, metadata = self._check(read_only=read_only, **metadata)\n    name, parent = self._create_ancestors(name, read_only=read_only)\n    return Group(name=name, parent=parent, read_only=read_only, **metadata)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.datasets","title":"datasets","text":"<pre><code>datasets(*, exclude=None, include=None, flags=0)\n</code></pre> <p>Yield the Datasets in this Group.</p> <p>Parameters:</p> Name Type Description Default <code>exclude</code> <code>str | None</code> <p>A regular-expression pattern to use to exclude Datasets. The re.search function is used to compare the <code>exclude</code> pattern with the name of each Dataset. If there is a match, the Dataset is not yielded.</p> <code>None</code> <code>include</code> <code>str | None</code> <p>A regular-expression pattern to use to include Datasets. The re.search function is used to compare the <code>include</code> pattern with the name of each Dataset. If there is a match, the Dataset is yielded.</p> <code>None</code> <code>flags</code> <code>int</code> <p>Regular-expression flags that are passed to re.compile.</p> <code>0</code> <p>Yields:</p> Type Description <code>Dataset</code> <p>The filtered Datasets based on the <code>exclude</code> and <code>include</code> patterns. The <code>exclude</code> pattern has more precedence than the <code>include</code> pattern if there is a conflict.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def datasets(self, *, exclude: str | None = None, include: str | None = None, flags: int = 0) -&gt; Iterator[Dataset]:\n    \"\"\"Yield the [Dataset][msl.io.node.Dataset]s in this [Group][msl.io.node.Group].\n\n    Args:\n        exclude: A regular-expression pattern to use to exclude [Dataset][msl.io.node.Dataset]s.\n            The [re.search][] function is used to compare the `exclude` pattern\n            with the [name][msl.io.node.Dataset.name] of each [Dataset][msl.io.node.Dataset]. If\n            there is a match, the [Dataset][msl.io.node.Dataset] is not yielded.\n        include: A regular-expression pattern to use to include [Dataset][msl.io.node.Dataset]s.\n            The [re.search][] function is used to compare the `include` pattern\n            with the [name][msl.io.node.Dataset.name] of each [Dataset][msl.io.node.Dataset]. If\n            there is a match, the [Dataset][msl.io.node.Dataset] is yielded.\n        flags: Regular-expression flags that are passed to [re.compile][].\n\n    Yields:\n        The filtered [Dataset][msl.io.node.Dataset]s based on the `exclude` and `include` patterns.\n            The `exclude` pattern has more precedence than the `include` pattern if there is a conflict.\n    \"\"\"\n    e = None if exclude is None else re.compile(exclude, flags=flags)\n    i = None if include is None else re.compile(include, flags=flags)\n    for obj in self._mapping.values():\n        if isinstance(obj, Dataset):\n            if e and e.search(obj.name):\n                continue\n            if i and not i.search(obj.name):\n                continue\n            yield obj\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.descendants","title":"descendants","text":"<pre><code>descendants()\n</code></pre> <p>Yield all descendant (children) Groups of this Group.</p> <p>Yields:</p> Type Description <code>Group</code> <p>The descendants of this Group.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def descendants(self) -&gt; Iterator[Group]:\n    \"\"\"Yield all descendant (children) [Group][msl.io.node.Group]s of this [Group][msl.io.node.Group].\n\n    Yields:\n        The descendants of this [Group][msl.io.node.Group].\n    \"\"\"\n    for obj in self._mapping.values():\n        if isinstance(obj, Group):\n            yield obj\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.groups","title":"groups","text":"<pre><code>groups(*, exclude=None, include=None, flags=0)\n</code></pre> <p>Yield the sub-Groups of this Group.</p> <p>Parameters:</p> Name Type Description Default <code>exclude</code> <code>str | None</code> <p>A regular-expression pattern to use to exclude sub-Groups. The re.search function is used to compare the <code>exclude</code> pattern with the name of each sub-Group. If there is a match, the sub-Group is not yielded.</p> <code>None</code> <code>include</code> <code>str | None</code> <p>A regular-expression pattern to use to include sub-Groups. The re.search function is used to compare the <code>include</code> pattern with the name of each sub-Group. If there is a match, the sub-Group is yielded.</p> <code>None</code> <code>flags</code> <code>int</code> <p>Regular-expression flags that are passed to re.compile.</p> <code>0</code> <p>Yields:</p> Type Description <code>Group</code> <p>The filtered sub-Groups based on the <code>exclude</code> and <code>include</code> patterns. The <code>exclude</code> pattern has more precedence than the <code>include</code> pattern if there is a conflict.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def groups(self, *, exclude: str | None = None, include: str | None = None, flags: int = 0) -&gt; Iterator[Group]:\n    \"\"\"Yield the sub-[Group][msl.io.node.Group]s of this [Group][msl.io.node.Group].\n\n    Args:\n        exclude: A regular-expression pattern to use to exclude sub-[Group][msl.io.node.Group]s.\n            The [re.search][] function is used to compare the `exclude` pattern with the\n            [name][msl.io.node.Group.name] of each sub-[Group][msl.io.node.Group]. If there is a match,\n            the sub-[Group][msl.io.node.Group] is not yielded.\n        include: A regular-expression pattern to use to include sub-[Group][msl.io.node.Group]s.\n            The [re.search][] function is used to compare the `include` pattern with the\n            [name][msl.io.node.Group.name] of each sub-[Group][msl.io.node.Group]. If there is a match,\n            the sub-[Group][msl.io.node.Group] is yielded.\n        flags: Regular-expression flags that are passed to [re.compile][].\n\n    Yields:\n        The filtered sub-[Group][msl.io.node.Group]s based on the `exclude` and `include` patterns.\n            The `exclude` pattern has more precedence than the `include` pattern if there is a conflict.\n    \"\"\"\n    e = None if exclude is None else re.compile(exclude, flags=flags)\n    i = None if include is None else re.compile(include, flags=flags)\n    for obj in self._mapping.values():\n        if isinstance(obj, Group):\n            if e and e.search(obj.name):\n                continue\n            if i and not i.search(obj.name):\n                continue\n            yield obj\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.is_dataset","title":"is_dataset  <code>staticmethod</code>","text":"<pre><code>is_dataset(obj)\n</code></pre> <p>Check if an object is an instance of Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether <code>obj</code> is an instance of Dataset.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>@staticmethod\ndef is_dataset(obj: object) -&gt; bool:\n    \"\"\"Check if an object is an instance of [Dataset][msl.io.node.Dataset].\n\n    Args:\n        obj: The object to check.\n\n    Returns:\n        Whether `obj` is an instance of [Dataset][msl.io.node.Dataset].\n    \"\"\"\n    return isinstance(obj, Dataset)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.is_dataset_logging","title":"is_dataset_logging  <code>staticmethod</code>","text":"<pre><code>is_dataset_logging(obj)\n</code></pre> <p>Check if an object is an instance of DatasetLogging.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether <code>obj</code> is an instance of DatasetLogging.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>@staticmethod\ndef is_dataset_logging(obj: object) -&gt; bool:\n    \"\"\"Check if an object is an instance of [DatasetLogging][msl.io.node.DatasetLogging].\n\n    Args:\n        obj: The object to check.\n\n    Returns:\n        Whether `obj` is an instance of [DatasetLogging][msl.io.node.DatasetLogging].\n    \"\"\"\n    return isinstance(obj, DatasetLogging)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.is_group","title":"is_group  <code>staticmethod</code>","text":"<pre><code>is_group(obj)\n</code></pre> <p>Check if an object is an instance of Group.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether <code>obj</code> is an instance of Group.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>@staticmethod\ndef is_group(obj: object) -&gt; bool:\n    \"\"\"Check if an object is an instance of [Group][msl.io.node.Group].\n\n    Args:\n        obj: The object to check.\n\n    Returns:\n        Whether `obj` is an instance of [Group][msl.io.node.Group].\n    \"\"\"\n    return isinstance(obj, Group)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.remove","title":"remove","text":"<pre><code>remove(name)\n</code></pre> <p>Remove a Group or a Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the Group or Dataset to remove.</p> required <p>Returns:</p> Type Description <code>Dataset | Group | None</code> <p>The Group or Dataset that was removed or <code>None</code> if there was no Group or Dataset with the specified <code>name</code>.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def remove(self, name: str) -&gt; Dataset | Group | None:\n    \"\"\"Remove a [Group][msl.io.node.Group] or a [Dataset][msl.io.node.Dataset].\n\n    Args:\n        name: The name of the [Group][msl.io.node.Group] or [Dataset][msl.io.node.Dataset] to remove.\n\n    Returns:\n        The [Group][msl.io.node.Group] or [Dataset][msl.io.node.Dataset] that was removed or `None` if\n            there was no [Group][msl.io.node.Group] or [Dataset][msl.io.node.Dataset] with the specified `name`.\n    \"\"\"\n    name = \"/\" + name.strip(\"/\")\n    return self.pop(name, None)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.require_dataset","title":"require_dataset","text":"<pre><code>require_dataset(name, *, read_only=None, **kwargs)\n</code></pre> <p>Require that a Dataset exists.</p> <p>If the Dataset exists it will be returned, otherwise it is created then returned.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the required Dataset. Automatically creates the ancestor Groups if they do not exist.</p> required <code>read_only</code> <code>bool | None</code> <p>Whether to create the required Dataset in read-only mode. If <code>None</code>, uses the mode for this Group.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to Dataset.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The Dataset that was created or that already existed.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def require_dataset(self, name: str, *, read_only: bool | None = None, **kwargs: Any) -&gt; Dataset:\n    \"\"\"Require that a [Dataset][msl.io.node.Dataset] exists.\n\n    If the [Dataset][msl.io.node.Dataset] exists it will be returned, otherwise it is created then returned.\n\n    Args:\n        name: The name of the required [Dataset][msl.io.node.Dataset]. Automatically creates the ancestor\n            [Group][msl.io.node.Group]s if they do not exist.\n        read_only: Whether to create the required [Dataset][msl.io.node.Dataset] in read-only mode.\n            If `None`, uses the mode for this [Group][msl.io.node.Group].\n        kwargs: All additional keyword arguments are passed to [Dataset][msl.io.node.Dataset].\n\n    Returns:\n        The [Dataset][msl.io.node.Dataset] that was created or that already existed.\n    \"\"\"\n    name = \"/\" + name.strip(\"/\")\n    dataset_name = name if self.parent is None else self.name + name\n    for dataset in self.datasets():\n        if dataset.name == dataset_name:\n            if read_only is not None:\n                dataset.read_only = read_only\n            if kwargs:  # only add the kwargs that should be Metadata\n                for kw in [\"shape\", \"dtype\", \"buffer\", \"offset\", \"strides\", \"order\", \"data\"]:\n                    kwargs.pop(kw, None)\n            dataset.add_metadata(**kwargs)\n            return dataset\n    return self.create_dataset(name, read_only=read_only, **kwargs)\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.require_dataset_logging","title":"require_dataset_logging","text":"<pre><code>require_dataset_logging(\n    name,\n    *,\n    level=\"INFO\",\n    attributes=None,\n    logger=None,\n    date_fmt=None,\n    **kwargs,\n)\n</code></pre> <p>Require that a Dataset exists for handling logging records.</p> <p>If the DatasetLogging exists it will be returned otherwise it is created and then returned.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A name to associate with the Dataset. Automatically creates the ancestor Groups if they do not exist.</p> required <code>level</code> <code>str | int</code> <p>The logging level to use.</p> <code>'INFO'</code> <code>attributes</code> <code>Sequence[str] | None</code> <p>The attribute names to include in the Dataset for each logging record. If <code>None</code>, uses asctime, levelname, name, and message.</p> <code>None</code> <code>logger</code> <code>Logger | None</code> <p>The Logger that the DatasetLogging object will be associated with. If <code>None</code>, it is associated with the root Logger.</p> <code>None</code> <code>date_fmt</code> <code>str | None</code> <p>The datetime format code to use to represent the asctime attribute in. If <code>None</code>, uses the ISO 8601 format <code>\"%Y-%m-%dT%H:%M:%S.%f\"</code>.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to Dataset. The default behaviour is to append every logging record to the Dataset. This guarantees that the size of the Dataset is equal to the number of logging records that were added to it. However, this behaviour can decrease performance if many logging records are added often because a copy of the data in the Dataset is created for each logging record that is added. You can improve performance by specifying an initial size of the Dataset by including a <code>shape</code> or a <code>size</code> keyword argument. This will also automatically create additional empty rows in the Dataset, that is proportional to the size of the Dataset, if the size of the Dataset needs to be increased. If you do this then you will want to call remove_empty_rows before writing DatasetLogging to a file or interacting with the data in DatasetLogging to remove the empty rows that were created.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DatasetLogging</code> <p>The DatasetLogging that was created or that already existed.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def require_dataset_logging(\n    self,\n    name: str,\n    *,\n    level: str | int = \"INFO\",\n    attributes: Sequence[str] | None = None,\n    logger: Logger | None = None,\n    date_fmt: str | None = None,\n    **kwargs: Any,\n) -&gt; DatasetLogging:\n    \"\"\"Require that a [Dataset][msl.io.node.Dataset] exists for handling [logging][] records.\n\n    If the [DatasetLogging][msl.io.node.DatasetLogging] exists it will be returned\n    otherwise it is created and then returned.\n\n    Args:\n        name: A name to associate with the [Dataset][msl.io.node.Dataset].\n            Automatically creates the ancestor [Group][msl.io.node.Group]s if they do not exist.\n        level: The [logging level][levels] to use.\n        attributes: The [attribute names][logrecord-attributes] to include in the\n            [Dataset][msl.io.node.Dataset] for each [logging record][log-record].\n            If `None`, uses _asctime_, _levelname_, _name_, and _message_.\n        logger: The [Logger][logging.Logger] that the [DatasetLogging][msl.io.node.DatasetLogging] object\n            will be associated with. If `None`, it is associated with the _root_ [Logger][logging.Logger].\n        date_fmt: The [datetime][datetime.datetime] [format code][strftime-strptime-behavior]\n            to use to represent the _asctime_ [attribute][logrecord-attributes] in.\n            If `None`, uses the ISO 8601 format `\"%Y-%m-%dT%H:%M:%S.%f\"`.\n        kwargs: All additional keyword arguments are passed to [Dataset][msl.io.node.Dataset].\n            The default behaviour is to append every [logging record][log-record]\n            to the [Dataset][msl.io.node.Dataset]. This guarantees that the size of the\n            [Dataset][msl.io.node.Dataset] is equal to the number of\n            [logging records][log-record] that were added to it. However, this behaviour\n            can decrease performance if many [logging records][log-record] are\n            added often because a copy of the data in the [Dataset][msl.io.node.Dataset] is\n            created for each [logging record][log-record] that is added. You can improve\n            performance by specifying an initial size of the [Dataset][msl.io.node.Dataset]\n            by including a `shape` or a `size` keyword argument. This will also automatically\n            create additional empty rows in the [Dataset][msl.io.node.Dataset], that is\n            proportional to the size of the [Dataset][msl.io.node.Dataset], if the size of the\n            [Dataset][msl.io.node.Dataset] needs to be increased. If you do this then you will\n            want to call [remove_empty_rows][msl.io.node.DatasetLogging.remove_empty_rows] before\n            writing [DatasetLogging][msl.io.node.DatasetLogging] to a file or interacting\n            with the data in [DatasetLogging][msl.io.node.DatasetLogging] to remove the\n            _empty_ rows that were created.\n\n    Returns:\n        The [DatasetLogging][msl.io.node.DatasetLogging] that was created or that already existed.\n    \"\"\"\n    name = \"/\" + name.strip(\"/\")\n    dataset_name = name if self.parent is None else self.name + name\n    for dataset in self.datasets():\n        if dataset.name == dataset_name:\n            if (\n                (\"logging_level\" not in dataset.metadata)\n                or (\"logging_level_name\" not in dataset.metadata)\n                or (\"logging_date_format\" not in dataset.metadata)\n            ):\n                msg = \"The required Dataset was found but it is not used for logging\"\n                raise ValueError(msg)\n\n            if attributes and (dataset.dtype.names != tuple(attributes)):\n                msg = (\n                    f\"The attribute names of the existing logging Dataset are \"\n                    f\"{dataset.dtype.names} which does not equal {tuple(attributes)}\"\n                )\n                raise ValueError(msg)\n\n            if isinstance(dataset, DatasetLogging):\n                return dataset\n\n            # replace the existing Dataset with a new DatasetLogging object\n            meta = dataset.metadata.copy()\n            data = dataset.data.copy()\n\n            # remove the existing Dataset from its descendants, itself and its ancestors\n            groups = (*tuple(self.descendants()), self, *tuple(self.ancestors()))\n            for group in groups:\n                for dset in group.datasets():\n                    if dset is dataset:\n                        key = \"/\" + dset.name.lstrip(group.name)\n                        del group._mapping[key]  # noqa: SLF001\n                        break\n\n            # temporarily make this Group not in read-only mode\n            original_read_only_mode = bool(self._read_only)\n            self._read_only: bool = False\n            kwargs.update(meta)\n            dset = self.create_dataset_logging(\n                name,\n                level=level,\n                attributes=data.dtype.names,\n                logger=logger,\n                date_fmt=meta.logging_date_format,\n                data=data,\n                **kwargs,\n            )\n            self._read_only = original_read_only_mode\n            return dset\n\n    return self.create_dataset_logging(\n        name, level=level, attributes=attributes, logger=logger, date_fmt=date_fmt, **kwargs\n    )\n</code></pre>"},{"location":"api/node/#msl.io.node.Group.require_group","title":"require_group","text":"<pre><code>require_group(name, *, read_only=None, **metadata)\n</code></pre> <p>Require that a Group exists.</p> <p>If the Group exists it will be returned otherwise it is created then returned.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the Group to require. Automatically creates the ancestor Groups if they do not exist.</p> required <code>read_only</code> <code>bool | None</code> <p>Whether to return the required Group in read-only mode. If <code>None</code>, uses the mode for this Group.</p> <code>None</code> <code>metadata</code> <code>Any</code> <p>All additional keyword arguments are used as Metadata for the required Group.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Group</code> <p>The required Group that was created or that already existed.</p> Source code in <code>src/msl/io/node.py</code> <pre><code>def require_group(self, name: str, *, read_only: bool | None = None, **metadata: Any) -&gt; Group:\n    \"\"\"Require that a [Group][msl.io.node.Group] exists.\n\n    If the [Group][msl.io.node.Group] exists it will be returned otherwise it is created then returned.\n\n    Args:\n        name: The name of the [Group][msl.io.node.Group] to require. Automatically creates the ancestor\n            [Group][msl.io.node.Group]s if they do not exist.\n        read_only: Whether to return the required [Group][msl.io.node.Group] in read-only mode.\n            If `None`, uses the mode for this [Group][msl.io.node.Group].\n        metadata: All additional keyword arguments are used as [Metadata][msl.io.metadata.Metadata]\n            for the required [Group][msl.io.node.Group].\n\n    Returns:\n        The required [Group][msl.io.node.Group] that was created or that already existed.\n    \"\"\"\n    name = \"/\" + name.strip(\"/\")\n    group_name = name if self.parent is None else self.name + name\n    for group in self.groups():\n        if group.name == group_name:\n            if read_only is not None:\n                group.read_only = read_only\n            group.add_metadata(**metadata)\n            return group\n    return self.create_group(name, read_only=read_only, **metadata)\n</code></pre>"},{"location":"api/spreadsheets/","title":"spreadsheets","text":"<p>Classes for reading cells in spreadsheets.</p> <p>Read an Excel spreadsheet (.xls and .xlsx files).</p> <p>Read a Google Sheets spreadsheet.</p> <p>Read an OpenDocument Spreadsheet (.ods and .fods files, Version 1.2).</p>"},{"location":"api/spreadsheets/#msl.io.readers.spreadsheet.Spreadsheet","title":"Spreadsheet","text":"<pre><code>Spreadsheet(file)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for spreadsheets.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The location of the spreadsheet on a local hard drive or on a network drive.</p> required Source code in <code>src/msl/io/readers/spreadsheet.py</code> <pre><code>def __init__(self, file: str) -&gt; None:\n    \"\"\"Abstract base class for spreadsheets.\n\n    Args:\n        file: The location of the spreadsheet on a local hard drive or on a network drive.\n    \"\"\"\n    self._file: str = file\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.spreadsheet.Spreadsheet.file","title":"file  <code>property</code>","text":"<pre><code>file\n</code></pre> <p>str \u2014 The location of the spreadsheet on a local hard drive or on a network drive.</p>"},{"location":"api/spreadsheets/#msl.io.readers.spreadsheet.Spreadsheet.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read(\n    cells=None,\n    sheet=None,\n    *,\n    as_datetime=True,\n    merged=False,\n)\n</code></pre> <p>Read values from the spreadsheet.</p> <p>You must override this method.</p> <p>Parameters:</p> Name Type Description Default <code>cells</code> <code>str | None</code> <p>The cell(s) to read. For example, <code>C9</code> will return a single value and <code>C9:G20</code> will return all values in the specified range. If not specified then returns all values in the specified <code>sheet</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of the sheet to read the value(s) from. If there is only one sheet in the spreadsheet then you do not need to specify the name of the sheet.</p> <code>None</code> <code>as_datetime</code> <code>bool</code> <p>Whether dates should be returned as datetime or date objects. If <code>False</code>, dates are returned as a string in the format of the spreadsheet cell.</p> <code>True</code> <code>merged</code> <code>bool</code> <p>Applies to cells that are merged with other cells. The details depend on the type of spreadsheet document that is being read. Some documents allow the hidden cells that are part of a merger to retain its unmerged value. Other documents associate the merged value only with the top-left cell and all other cells in the merger are empty.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any | list[tuple[Any, ...]]</code> <p>The value(s) of the requested cell(s).</p> Source code in <code>src/msl/io/readers/spreadsheet.py</code> <pre><code>@abstractmethod\ndef read(\n    self, cells: str | None = None, sheet: str | None = None, *, as_datetime: bool = True, merged: bool = False\n) -&gt; Any | list[tuple[Any, ...]]:\n    \"\"\"Read values from the spreadsheet.\n\n    !!! warning \"You must override this method.\"\n\n    Args:\n        cells: The cell(s) to read. For example, `C9` will return a single value\n            and `C9:G20` will return all values in the specified range. If not\n            specified then returns all values in the specified `sheet`.\n        sheet: The name of the sheet to read the value(s) from. If there is only one\n            sheet in the spreadsheet then you do not need to specify the name of the sheet.\n        as_datetime: Whether dates should be returned as [datetime][datetime.datetime] or\n            [date][datetime.date] objects. If `False`, dates are returned as a string in\n            the format of the spreadsheet cell.\n        merged: Applies to cells that are merged with other cells. The details depend\n            on the type of spreadsheet document that is being read. Some documents allow\n            the hidden cells that are part of a merger to retain its unmerged value. Other\n            documents associate the merged value only with the top-left cell and all\n            other cells in the merger are empty.\n\n    Returns:\n        The value(s) of the requested cell(s).\n    \"\"\"\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.spreadsheet.Spreadsheet.sheet_names","title":"sheet_names  <code>abstractmethod</code>","text":"<pre><code>sheet_names()\n</code></pre> <p>Get the names of all sheets in the spreadsheet.</p> <p>You must override this method.</p> <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>The names of all sheets.</p> Source code in <code>src/msl/io/readers/spreadsheet.py</code> <pre><code>@abstractmethod\ndef sheet_names(self) -&gt; tuple[str, ...]:\n    \"\"\"Get the names of all sheets in the spreadsheet.\n\n    !!! warning \"You must override this method.\"\n\n    Returns:\n        The names of all sheets.\n    \"\"\"\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.spreadsheet.Spreadsheet.to_indices","title":"to_indices  <code>staticmethod</code>","text":"<pre><code>to_indices(cell)\n</code></pre> <p>Convert a string representation of a cell to row and column indices.</p> <p>Parameters:</p> Name Type Description Default <code>cell</code> <code>str</code> <p>The cell. Can be letters only (a column) or letters and a number (a column and a row).</p> required <p>Returns:</p> Type Description <code>tuple[int | None, int]</code> <p>The (row_index, column_index). If <code>cell</code> does not contain a row number then the row index is <code>None</code>. The row and column index are zero based.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; to_indices(\"A\")\n(None, 0)\n&gt;&gt;&gt; to_indices(\"A1\")\n(0, 0)\n&gt;&gt;&gt; to_indices(\"AA10\")\n(9, 26)\n&gt;&gt;&gt; to_indices(\"AAA111\")\n(110, 702)\n&gt;&gt;&gt; to_indices(\"MSL123456\")\n(123455, 9293)\n&gt;&gt;&gt; to_indices(\"BIPM\")\n(None, 41664)\n</code></pre> Source code in <code>src/msl/io/readers/spreadsheet.py</code> <pre><code>@staticmethod\ndef to_indices(cell: str) -&gt; tuple[int | None, int]:\n    \"\"\"Convert a string representation of a cell to row and column indices.\n\n    Args:\n        cell: The cell. Can be letters only (a column) or letters and a number\n            (a column and a row).\n\n    Returns:\n        The *(row_index, column_index)*. If `cell` does not contain a row number\n            then the row index is `None`. The row and column index are zero based.\n\n    **Examples:**\n    &lt;!-- invisible-code-block: pycon\n    &gt;&gt;&gt; from msl.io.readers.spreadsheet import Spreadsheet\n    &gt;&gt;&gt; to_indices = Spreadsheet.to_indices\n\n    --&gt;\n\n    ```pycon\n    &gt;&gt;&gt; to_indices(\"A\")\n    (None, 0)\n    &gt;&gt;&gt; to_indices(\"A1\")\n    (0, 0)\n    &gt;&gt;&gt; to_indices(\"AA10\")\n    (9, 26)\n    &gt;&gt;&gt; to_indices(\"AAA111\")\n    (110, 702)\n    &gt;&gt;&gt; to_indices(\"MSL123456\")\n    (123455, 9293)\n    &gt;&gt;&gt; to_indices(\"BIPM\")\n    (None, 41664)\n\n    ```\n    \"\"\"\n    match = _cell_regex.match(cell)\n    if not match:\n        msg = f\"Invalid cell {cell!r}\"\n        raise ValueError(msg)\n\n    letters, numbers = match.groups()\n    row = max(0, int(numbers) - 1) if numbers else None\n    uppercase = string.ascii_uppercase\n    col = sum((26**i) * (1 + uppercase.index(c)) for i, c in enumerate(letters[::-1]))\n    return row, col - 1\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.spreadsheet.Spreadsheet.to_letters","title":"to_letters  <code>staticmethod</code>","text":"<pre><code>to_letters(index)\n</code></pre> <p>Convert a column index to column letters.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The column index (zero based).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The corresponding spreadsheet column letter(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; to_letters(0)\n'A'\n&gt;&gt;&gt; to_letters(1)\n'B'\n&gt;&gt;&gt; to_letters(26)\n'AA'\n&gt;&gt;&gt; to_letters(702)\n'AAA'\n&gt;&gt;&gt; to_letters(494264)\n'ABCDE'\n</code></pre> Source code in <code>src/msl/io/readers/spreadsheet.py</code> <pre><code>@staticmethod\ndef to_letters(index: int) -&gt; str:\n    \"\"\"Convert a column index to column letters.\n\n    Args:\n        index: The column index (zero based).\n\n    Returns:\n        The corresponding spreadsheet column letter(s).\n\n    **Examples:**\n    &lt;!-- invisible-code-block: pycon\n    &gt;&gt;&gt; from msl.io.readers.spreadsheet import Spreadsheet\n    &gt;&gt;&gt; to_letters = Spreadsheet.to_letters\n\n    --&gt;\n\n    ```pycon\n    &gt;&gt;&gt; to_letters(0)\n    'A'\n    &gt;&gt;&gt; to_letters(1)\n    'B'\n    &gt;&gt;&gt; to_letters(26)\n    'AA'\n    &gt;&gt;&gt; to_letters(702)\n    'AAA'\n    &gt;&gt;&gt; to_letters(494264)\n    'ABCDE'\n\n    ```\n    \"\"\"\n    letters: list[str] = []\n    uppercase = string.ascii_uppercase\n    while index &gt;= 0:\n        div, mod = divmod(index, 26)\n        letters.append(uppercase[mod])\n        index = div - 1\n    return \"\".join(letters[::-1])\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.spreadsheet.Spreadsheet.to_slices","title":"to_slices  <code>staticmethod</code>","text":"<pre><code>to_slices(cells, row_step=None, column_step=None)\n</code></pre> <p>Convert a range of cells to slices of row and column indices.</p> <p>Parameters:</p> Name Type Description Default <code>cells</code> <code>str</code> <p>The cells. Can be letters only (a column) or letters and a number (a column and a row).</p> required <code>row_step</code> <code>int | None</code> <p>The step-by value for the row slice.</p> <code>None</code> <code>column_step</code> <code>int | None</code> <p>The step-by value for the column slice.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[slice[int, int | None, int | None], slice[int, int, int | None]]</code> <p>The row slice, the column slice.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; to_slices(\"A:A\")\n(slice(0, None, None), slice(0, 1, None))\n&gt;&gt;&gt; to_slices(\"A:H\")\n(slice(0, None, None), slice(0, 8, None))\n&gt;&gt;&gt; to_slices(\"B2:M10\")\n(slice(1, 10, None), slice(1, 13, None))\n&gt;&gt;&gt; to_slices(\"A5:M100\", row_step=2, column_step=4)\n(slice(4, 100, 2), slice(0, 13, 4))\n</code></pre> Source code in <code>src/msl/io/readers/spreadsheet.py</code> <pre><code>@staticmethod\ndef to_slices(\n    cells: str, row_step: int | None = None, column_step: int | None = None\n) -&gt; tuple[slice[int, int | None, int | None], slice[int, int, int | None]]:\n    \"\"\"Convert a range of cells to slices of row and column indices.\n\n    Args:\n        cells: The cells. Can be letters only (a column) or letters and a number\n            (a column and a row).\n        row_step: The step-by value for the row slice.\n        column_step: The step-by value for the column slice.\n\n    Returns:\n        The row [slice][], the column [slice][].\n\n    **Examples:**\n    &lt;!-- invisible-code-block: pycon\n    &gt;&gt;&gt; from msl.io.readers.spreadsheet import Spreadsheet\n    &gt;&gt;&gt; to_slices = Spreadsheet.to_slices\n\n    --&gt;\n\n    ```pycon\n    &gt;&gt;&gt; to_slices(\"A:A\")\n    (slice(0, None, None), slice(0, 1, None))\n    &gt;&gt;&gt; to_slices(\"A:H\")\n    (slice(0, None, None), slice(0, 8, None))\n    &gt;&gt;&gt; to_slices(\"B2:M10\")\n    (slice(1, 10, None), slice(1, 13, None))\n    &gt;&gt;&gt; to_slices(\"A5:M100\", row_step=2, column_step=4)\n    (slice(4, 100, 2), slice(0, 13, 4))\n\n    ```\n    \"\"\"\n    split = cells.split(\":\")\n    if len(split) != 2:  # noqa: PLR2004\n        msg = f\"Invalid cell range {cells!r}\"\n        raise ValueError(msg)\n\n    r1, c1 = Spreadsheet.to_indices(split[0])\n    r2, c2 = Spreadsheet.to_indices(split[1])\n    if r1 is None:\n        r1 = 0\n    if r2 is not None:\n        r2 += 1\n    c2 += 1\n    return slice(r1, r2, row_step), slice(c1, c2, column_step)\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.excel.ExcelReader","title":"ExcelReader","text":"<pre><code>ExcelReader(file, **kwargs)\n</code></pre> <p>               Bases: <code>Spreadsheet</code></p> <p>Read an Excel spreadsheet (.xls and .xlsx files).</p> <p>This class simply provides a convenience for reading cell values (not drawings or charts) from Excel spreadsheets. It is not registered as a Reader because the information in a spreadsheet is unstructured and therefore one cannot generalize how to parse a spreadsheet to create a Root.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>The path to an Excel spreadsheet file.</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are passed to xlrd.open_workbook. You can use an <code>encoding</code> keyword argument as an alias for <code>encoding_override</code>. The default <code>on_demand</code> value is <code>True</code>.</p> <code>{}</code> <p>Examples: <pre><code>from msl.io import ExcelReader\nexcel = ExcelReader(\"lab_environment.xlsx\")\n</code></pre></p> Source code in <code>src/msl/io/readers/excel.py</code> <pre><code>def __init__(self, file: PathLike, **kwargs: Any) -&gt; None:\n    \"\"\"Read an Excel spreadsheet (*.xls* and *.xlsx* files).\n\n    This class simply provides a convenience for reading cell values (not\n    drawings or charts) from Excel spreadsheets. It is not registered as\n    a [Reader][msl.io.base.Reader] because the information in a spreadsheet\n    is unstructured and therefore one cannot generalize how to parse a\n    spreadsheet to create a [Root][msl.io.base.Root].\n\n    Args:\n        file: The path to an Excel spreadsheet file.\n        kwargs: All keyword arguments are passed to [xlrd.open_workbook][]{:target=\"_blank\"}.\n            You can use an `encoding` keyword argument as an alias for `encoding_override`.\n            The default `on_demand` value is `True`.\n\n    **Examples:**\n    ```python\n    from msl.io import ExcelReader\n    excel = ExcelReader(\"lab_environment.xlsx\")\n    ```\n    \"\"\"\n    f = os.fsdecode(file)\n    super().__init__(f)\n\n    # change the default on_demand value\n    if \"on_demand\" not in kwargs:\n        kwargs[\"on_demand\"] = True\n\n    # 'encoding' is an alias for 'encoding_override'\n    encoding = kwargs.pop(\"encoding\", None)\n    if encoding is not None:\n        kwargs[\"encoding_override\"] = encoding\n\n    self._workbook: Book = open_workbook(f, **kwargs)\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.excel.ExcelReader.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the workbook.</p> Source code in <code>src/msl/io/readers/excel.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the workbook.\"\"\"\n    self._workbook.release_resources()\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.excel.ExcelReader.dimensions","title":"dimensions","text":"<pre><code>dimensions(sheet)\n</code></pre> <p>Get the number of rows and columns in a sheet.</p> <p>Parameters:</p> Name Type Description Default <code>sheet</code> <code>str</code> <p>The name of a sheet to get the dimensions of.</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The (number of rows, number of columns) in <code>sheet</code>.</p> Source code in <code>src/msl/io/readers/excel.py</code> <pre><code>def dimensions(self, sheet: str) -&gt; tuple[int, int]:\n    \"\"\"Get the number of rows and columns in a sheet.\n\n    Args:\n        sheet: The name of a sheet to get the dimensions of.\n\n    Returns:\n        The *(number of rows, number of columns)* in `sheet`.\n    \"\"\"\n    try:\n        s = self._workbook.sheet_by_name(sheet)\n    except XLRDError:\n        msg = f\"A sheet named {sheet!r} is not in {self._file!r}\"\n        raise ValueError(msg) from None\n    else:\n        return (s.nrows, s.ncols)\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.excel.ExcelReader.read","title":"read","text":"<pre><code>read(\n    cells=None,\n    sheet=None,\n    *,\n    as_datetime=True,\n    merged=False,\n)\n</code></pre> <p>Read cell values from the Excel spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>cells</code> <code>str | None</code> <p>The cell(s) to read. For example, <code>C9</code> will return a single value and <code>C9:G20</code> will return all values in the specified range. If not specified then returns all values in the specified <code>sheet</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of the sheet to read the value(s) from. If there is only one sheet in the spreadsheet then you do not need to specify the name of the sheet.</p> <code>None</code> <code>as_datetime</code> <code>bool</code> <p>Whether dates should be returned as datetime or date objects. If <code>False</code>, dates are returned as an ISO 8601 string.</p> <code>True</code> <code>merged</code> <code>bool</code> <p>Applies to cells that are merged with other cells. If cells are merged, then only the top-left cell has the value and all other cells in the merger are empty. Enabling this argument is currently not supported and the value must be <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any | list[tuple[Any, ...]]</code> <p>The value(s) of the requested cell(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; excel.read()\n[('temperature', 'humidity'), (20.33, 49.82), (20.23, 46.06), (20.41, 47.06), (20.29, 48.32)]\n&gt;&gt;&gt; excel.read(\"B2\")\n49.82\n&gt;&gt;&gt; excel.read(\"A:A\")\n[('temperature',), (20.33,), (20.23,), (20.41,), (20.29,)]\n&gt;&gt;&gt; excel.read(\"A1:B1\")\n[('temperature', 'humidity')]\n&gt;&gt;&gt; excel.read(\"A2:B4\")\n[(20.33, 49.82), (20.23, 46.06), (20.41, 47.06)]\n</code></pre> Source code in <code>src/msl/io/readers/excel.py</code> <pre><code>def read(  # noqa: C901\n    self, cells: str | None = None, sheet: str | None = None, *, as_datetime: bool = True, merged: bool = False\n) -&gt; Any | list[tuple[Any, ...]]:\n    \"\"\"Read cell values from the Excel spreadsheet.\n\n    Args:\n        cells: The cell(s) to read. For example, `C9` will return a single value\n            and `C9:G20` will return all values in the specified range. If not\n            specified then returns all values in the specified `sheet`.\n        sheet: The name of the sheet to read the value(s) from. If there is only\n            one sheet in the spreadsheet then you do not need to specify the name\n            of the sheet.\n        as_datetime: Whether dates should be returned as [datetime][datetime.datetime] or\n            [date][datetime.date] objects. If `False`, dates are returned as an\n            ISO 8601 string.\n        merged: Applies to cells that are merged with other cells. If cells are merged, then\n            only the top-left cell has the value and all other cells in the merger are empty.\n            Enabling this argument is currently not supported and the value must be `False`.\n\n    Returns:\n        The value(s) of the requested cell(s).\n\n    **Examples:**\n    &lt;!-- invisible-code-block: pycon\n    &gt;&gt;&gt; from msl.io import ExcelReader\n    &gt;&gt;&gt; excel = ExcelReader('./tests/samples/lab_environment.xlsx')\n\n    --&gt;\n\n    ```pycon\n    &gt;&gt;&gt; excel.read()\n    [('temperature', 'humidity'), (20.33, 49.82), (20.23, 46.06), (20.41, 47.06), (20.29, 48.32)]\n    &gt;&gt;&gt; excel.read(\"B2\")\n    49.82\n    &gt;&gt;&gt; excel.read(\"A:A\")\n    [('temperature',), (20.33,), (20.23,), (20.41,), (20.29,)]\n    &gt;&gt;&gt; excel.read(\"A1:B1\")\n    [('temperature', 'humidity')]\n    &gt;&gt;&gt; excel.read(\"A2:B4\")\n    [(20.33, 49.82), (20.23, 46.06), (20.41, 47.06)]\n\n    ```\n    \"\"\"\n    if merged:\n        msg = \"The `merged` argument must be False to read an Excel spreadsheet\"\n        raise ValueError(msg)\n\n    if not sheet:\n        names = self.sheet_names()\n        if len(names) == 1:\n            sheet_name = names[0]\n        elif not names:\n            msg = \"Cannot determine the names of the sheets in the Excel file\"\n            raise ValueError(msg)\n        else:\n            sheets = \", \".join(repr(n) for n in names)\n            msg = (\n                f\"{self.file!r} contains the following sheets:\\n  {sheets}\\n\"\n                f\"You must specify the name of the sheet to read\"\n            )\n            raise ValueError(msg)\n    else:\n        sheet_name = sheet\n\n    try:\n        _sheet = self._workbook.sheet_by_name(sheet_name)\n    except XLRDError:\n        msg = f\"A sheet named {sheet_name!r} is not in {self._file!r}\"\n        raise ValueError(msg) from None\n\n    if not cells:\n        return [\n            tuple(self._value(_sheet, r, c, as_datetime) for c in range(_sheet.ncols)) for r in range(_sheet.nrows)\n        ]\n\n    split = cells.split(\":\")\n    r1, c1 = self.to_indices(split[0])\n    if r1 is None:\n        r1 = 0\n\n    if len(split) == 1:\n        try:\n            return self._value(_sheet, r1, c1, as_datetime=as_datetime)\n        except IndexError:\n            return None\n\n    if r1 &gt;= _sheet.nrows or c1 &gt;= _sheet.ncols:\n        return []\n\n    r2, c2 = self.to_indices(split[1])\n    r2 = _sheet.nrows if r2 is None else min(r2 + 1, _sheet.nrows)\n    c2 = min(c2 + 1, _sheet.ncols)\n    return [tuple(self._value(_sheet, r, c, as_datetime) for c in range(c1, c2)) for r in range(r1, r2)]\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.excel.ExcelReader.sheet_names","title":"sheet_names","text":"<pre><code>sheet_names()\n</code></pre> <p>Get the names of all sheets in the Excel spreadsheet.</p> <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>The names of all sheets.</p> Source code in <code>src/msl/io/readers/excel.py</code> <pre><code>def sheet_names(self) -&gt; tuple[str, ...]:\n    \"\"\"Get the names of all sheets in the Excel spreadsheet.\n\n    Returns:\n        The names of all sheets.\n    \"\"\"\n    return tuple(self._workbook.sheet_names())\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.gsheets.GSheetsReader","title":"GSheetsReader","text":"<pre><code>GSheetsReader(file, *, account=None, credentials=None)\n</code></pre> <p>               Bases: <code>Spreadsheet</code></p> <p>Read a Google Sheets spreadsheet.</p> <p>This class simply provides a convenience for reading cell values (not drawings or charts) from Google spreadsheets. It is not registered as a Reader because the information in a spreadsheet is unstructured and therefore one cannot generalize how to parse a spreadsheet to create a Root.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>The ID or path of a Google Sheets spreadsheet.</p> required <code>account</code> <code>str | None</code> <p>Since a person may have multiple Google accounts, and multiple people may run the same code, this parameter decides which token to load to authenticate with the Google API. The value can be any text (or <code>None</code>) that you want to associate with a particular Google account, provided that it contains valid characters for a filename. The value that you chose when you authenticated with your <code>credentials</code> should be used for all future instances of this class to access that particular Google account. You can associate a different value with a Google account at any time (by passing in a different <code>account</code> value), but you may be asked to authenticate with your <code>credentials</code> again, or, alternatively, you can rename the token files located in MSL_IO_DIR to match the new <code>account</code> value.</p> <code>None</code> <code>credentials</code> <code>PathLike | None</code> <p>The path to the client secrets OAuth credential file. This parameter only needs to be specified the first time that you authenticate with a particular Google account or if you delete the token file that was created when you previously authenticated.</p> <code>None</code> <p>Examples: <pre><code>from msl.io import GSheetsReader\n\n# Specify the path\nsheets = GSheetsReader(\"Google Drive/registers/equipment.gsheet\")\n\n# Specify the ID\nsheets = GSheetsReader(\"1TI3pM-534SZ5DQTEZ-7HCI04648f8ZpLGbfHWJu9FSo\")\n</code></pre></p> Source code in <code>src/msl/io/readers/gsheets.py</code> <pre><code>def __init__(\n    self,\n    file: PathLike,\n    *,\n    account: str | None = None,\n    credentials: PathLike | None = None,\n) -&gt; None:\n    \"\"\"Read a Google Sheets spreadsheet.\n\n    This class simply provides a convenience for reading cell values (not\n    drawings or charts) from Google spreadsheets. It is not registered as\n    a [Reader][msl.io.base.Reader] because the information in a spreadsheet\n    is unstructured and therefore one cannot generalize how to parse a\n    spreadsheet to create a [Root][msl.io.base.Root].\n\n    Args:\n        file: The ID or path of a Google Sheets spreadsheet.\n        account: Since a person may have multiple Google accounts, and multiple people\n            may run the same code, this parameter decides which token to load\n            to authenticate with the Google API. The value can be any text (or\n            `None`) that you want to associate with a particular Google\n            account, provided that it contains valid characters for a filename.\n            The value that you chose when you authenticated with your `credentials`\n            should be used for all future instances of this class to access that\n            particular Google account. You can associate a different value with\n            a Google account at any time (by passing in a different `account`\n            value), but you may be asked to authenticate with your `credentials`\n            again, or, alternatively, you can rename the token files located in\n            [MSL_IO_DIR][msl.io.constants.MSL_IO_DIR] to match the new `account` value.\n        credentials: The path to the *client secrets* OAuth credential file. This\n            parameter only needs to be specified the first time that you\n            authenticate with a particular Google account or if you delete\n            the token file that was created when you previously authenticated.\n\n    **Examples:**\n    ```python\n    from msl.io import GSheetsReader\n\n    # Specify the path\n    sheets = GSheetsReader(\"Google Drive/registers/equipment.gsheet\")\n\n    # Specify the ID\n    sheets = GSheetsReader(\"1TI3pM-534SZ5DQTEZ-7HCI04648f8ZpLGbfHWJu9FSo\")\n    ```\n    \"\"\"\n    file = os.fsdecode(file)\n    super().__init__(file)\n\n    path, ext = os.path.splitext(file)  # noqa: PTH122\n    folders, _ = os.path.split(path)\n\n    self._spreadsheet_id: str\n    if ext or folders or not _google_file_id_regex.match(path):\n        self._spreadsheet_id = GDrive(account=account, credentials=credentials).file_id(\n            path, mime_type=GSheets.MIME_TYPE\n        )\n    else:\n        self._spreadsheet_id = path\n\n    self._gsheets: GSheets = GSheets(account=account, credentials=credentials, read_only=True)\n    self._cached_sheet_name: str | None = None\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.gsheets.GSheetsReader.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the connection to the GSheet API service.</p> Source code in <code>src/msl/io/readers/gsheets.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the connection to the GSheet API service.\"\"\"\n    self._gsheets.close()\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.gsheets.GSheetsReader.read","title":"read","text":"<pre><code>read(\n    cells=None,\n    sheet=None,\n    *,\n    as_datetime=True,\n    merged=False,\n)\n</code></pre> <p>Read cell values from the Google Sheets spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>cells</code> <code>str | None</code> <p>The cell(s) to read. For example, <code>C9</code> will return a single value and <code>C9:G20</code> will return all values in the specified range. If not specified then returns all values in the specified <code>sheet</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of the sheet to read the value(s) from. If there is only one sheet in the spreadsheet then you do not need to specify the name of the sheet.</p> <code>None</code> <code>as_datetime</code> <code>bool</code> <p>Whether dates should be returned as datetime or date objects. If <code>False</code>, dates are returned as a string in the display format of the spreadsheet cell.</p> <code>True</code> <code>merged</code> <code>bool</code> <p>Applies to cells that are merged with other cells. If cells are merged, then only the top-left cell has the value and all other cells in the merger are empty. Enabling this argument is currently not supported and the value must be <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any | list[tuple[Any, ...]]</code> <p>The value(s) of the requested cell(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sheets.read()\n[('temperature', 'humidity'), (20.33, 49.82), (20.23, 46.06), (20.41, 47.06), (20.29, 48.32)]\n&gt;&gt;&gt; sheets.read(\"B2\")\n49.82\n&gt;&gt;&gt; sheets.read(\"A:A\")\n[('temperature',), (20.33,), (20.23,), (20.41,), (20.29,)]\n&gt;&gt;&gt; sheets.read(\"A1:B1\")\n[('temperature', 'humidity')]\n&gt;&gt;&gt; sheets.read(\"A2:B4\")\n[(20.33, 49.82), (20.23, 46.06), (20.41, 47.06)]\n</code></pre> Source code in <code>src/msl/io/readers/gsheets.py</code> <pre><code>def read(  # noqa: C901, PLR0912\n    self, cells: str | None = None, sheet: str | None = None, *, as_datetime: bool = True, merged: bool = False\n) -&gt; Any | list[tuple[Any, ...]]:\n    \"\"\"Read cell values from the Google Sheets spreadsheet.\n\n    Args:\n        cells: The cell(s) to read. For example, `C9` will return a single value\n            and `C9:G20` will return all values in the specified range. If not\n            specified then returns all values in the specified `sheet`.\n        sheet: The name of the sheet to read the value(s) from. If there is only one\n            sheet in the spreadsheet then you do not need to specify the name of the sheet.\n        as_datetime: Whether dates should be returned as [datetime][datetime.datetime] or\n            [date][datetime.date] objects. If `False`, dates are returned as a string in\n            the display format of the spreadsheet cell.\n        merged: Applies to cells that are merged with other cells. If cells are merged, then\n            only the top-left cell has the value and all other cells in the merger are empty.\n            Enabling this argument is currently not supported and the value must be `False`.\n\n    Returns:\n        The value(s) of the requested cell(s).\n\n    **Examples:**\n    &lt;!-- invisible-code-block: pycon\n    &gt;&gt;&gt; SKIP_IF_NO_GOOGLE_SHEETS_READ_TOKEN()\n    &gt;&gt;&gt; from msl.io import GSheetsReader\n    &gt;&gt;&gt; sheets = GSheetsReader('1TI3pM-534SZ5DQTEZ-7vCI04l48f8ZpLGbfEWJuCFSo', account='testing')\n\n    --&gt;\n\n    ```pycon\n    &gt;&gt;&gt; sheets.read()\n    [('temperature', 'humidity'), (20.33, 49.82), (20.23, 46.06), (20.41, 47.06), (20.29, 48.32)]\n    &gt;&gt;&gt; sheets.read(\"B2\")\n    49.82\n    &gt;&gt;&gt; sheets.read(\"A:A\")\n    [('temperature',), (20.33,), (20.23,), (20.41,), (20.29,)]\n    &gt;&gt;&gt; sheets.read(\"A1:B1\")\n    [('temperature', 'humidity')]\n    &gt;&gt;&gt; sheets.read(\"A2:B4\")\n    [(20.33, 49.82), (20.23, 46.06), (20.41, 47.06)]\n\n    ```\n    \"\"\"\n    if merged:\n        msg = \"The `merged` argument must be False to read a Google spreadsheet\"\n        raise ValueError(msg)\n\n    if not sheet:\n        if self._cached_sheet_name:\n            sheet = self._cached_sheet_name\n        else:\n            names = self.sheet_names()\n            if len(names) != 1:\n                sheet_names = \", \".join(repr(n) for n in names)\n                msg = (\n                    f\"{self._file} contains the following sheets:\\n  {sheet_names}\\n\"\n                    f\"You must specify the name of the sheet to read\"\n                )\n                raise ValueError(msg)\n            sheet = names[0]\n            self._cached_sheet_name = sheet\n\n    ranges = f\"{sheet}!{cells}\" if cells else sheet\n\n    cells_dict = self._gsheets.cells(self._spreadsheet_id, ranges=ranges)\n\n    if sheet not in cells_dict:\n        msg = f\"A sheet named {sheet!r} is not in {self._file!r}\"\n        raise ValueError(msg)\n\n    values: list[tuple[Any, ...]] = []\n    for row in cells_dict[sheet]:\n        row_values: list[Any] = []\n        for item in row:\n            if item.type == GCellType.DATE:\n                value = GSheets.to_datetime(item.value).date() if as_datetime else item.formatted\n            elif item.type == GCellType.DATE_TIME:\n                value = GSheets.to_datetime(item.value) if as_datetime else item.formatted\n            else:\n                value = item.value\n            row_values.append(value)\n        values.append(tuple(row_values))\n\n    if not cells:\n        return values\n\n    if \":\" not in cells:\n        if values:\n            return values[0][0]\n        return None\n\n    return values\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.gsheets.GSheetsReader.sheet_names","title":"sheet_names","text":"<pre><code>sheet_names()\n</code></pre> <p>Get the names of all sheets in the Google Sheets spreadsheet.</p> <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>The names of all sheets.</p> Source code in <code>src/msl/io/readers/gsheets.py</code> <pre><code>def sheet_names(self) -&gt; tuple[str, ...]:\n    \"\"\"Get the names of all sheets in the Google Sheets spreadsheet.\n\n    Returns:\n        The names of all sheets.\n    \"\"\"\n    return self._gsheets.sheet_names(self._spreadsheet_id)\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.ods.ODSReader","title":"ODSReader","text":"<pre><code>ODSReader(file, **kwargs)\n</code></pre> <p>               Bases: <code>Spreadsheet</code></p> <p>Read an OpenDocument Spreadsheet (.ods and .fods files, Version 1.2).</p> <p>This class simply provides a convenience for reading cell values (not drawings or charts) from OpenDocument Spreadsheets. It is not registered as a Reader because the information in a spreadsheet is unstructured and therefore one cannot generalize how to parse a spreadsheet to create a Root.</p> <p>Tip</p> <p>If defusedxml is installed, that package is used to parse the contents of the file instead of the xml module.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>The path to an OpenDocument Spreadsheet file.</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are ignored.</p> <code>{}</code> <p>Examples: <pre><code>from msl.io import ODSReader\nods = ODSReader(\"lab_environment.ods\")\n</code></pre></p> Source code in <code>src/msl/io/readers/ods.py</code> <pre><code>def __init__(self, file: PathLike, **kwargs: Any) -&gt; None:  # noqa: ARG002\n    \"\"\"Read an OpenDocument Spreadsheet (*.ods* and *.fods* files, Version 1.2).\n\n    This class simply provides a convenience for reading cell values (not\n    drawings or charts) from OpenDocument Spreadsheets. It is not registered\n    as a [Reader][msl.io.base.Reader] because the information in a spreadsheet\n    is unstructured and therefore one cannot generalize how to parse a\n    spreadsheet to create a [Root][msl.io.base.Root].\n\n    !!! tip\n        If [defusedxml](https://pypi.org/project/defusedxml/){:target=\"_blank\"} is installed,\n        that package is used to parse the contents of the file instead of the\n        [xml][]{:target=\"_blank\"} module.\n\n    Args:\n        file: The path to an OpenDocument Spreadsheet file.\n        kwargs: All keyword arguments are ignored.\n\n    **Examples:**\n    ```python\n    from msl.io import ODSReader\n    ods = ODSReader(\"lab_environment.ods\")\n    ```\n    \"\"\"\n    f = os.fsdecode(file)\n    super().__init__(f)\n\n    self._spans: dict[int, tuple[int, Any]] = {}  # column-index: (spans-remaining, cell-value)\n\n    ext = get_extension(f).lower()\n    content: Element[str]\n    if ext == \".ods\":\n        with ZipFile(f) as z:\n            try:\n                content = ET.XML(z.read(\"content.xml\"))\n            except SyntaxError as e:\n                e.msg += \"\\nThe ODS file might be password protected (which is not supported)\"\n                raise\n    elif ext == \".fods\":\n        with open(f, mode=\"rb\") as fp:  # noqa: PTH123\n            content = ET.XML(fp.read())\n    else:\n        msg = f\"Unsupported OpenDocument Spreadsheet file extension {ext!r}\"\n        raise ValueError(msg)\n\n    self._tables: dict[str, Element[str]] = {\n        self._attribute(t, \"table\", \"name\"): t for t in content.findall(\".//table:table\", namespaces=self._ns)\n    }\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.ods.ODSReader.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Free memory resources that are used to read the OpenDocument Spreadsheet.</p> Source code in <code>src/msl/io/readers/ods.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Free memory resources that are used to read the OpenDocument Spreadsheet.\"\"\"\n    self._tables.clear()\n    self._spans.clear()\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.ods.ODSReader.dimensions","title":"dimensions","text":"<pre><code>dimensions(sheet)\n</code></pre> <p>Get the number of rows and columns in a sheet.</p> <p>Parameters:</p> Name Type Description Default <code>sheet</code> <code>str</code> <p>The name of a sheet to get the dimensions of.</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>The (number of rows, number of columns) in <code>sheet</code>.</p> Source code in <code>src/msl/io/readers/ods.py</code> <pre><code>def dimensions(self, sheet: str) -&gt; tuple[int, int]:\n    \"\"\"Get the number of rows and columns in a sheet.\n\n    Args:\n        sheet: The name of a sheet to get the dimensions of.\n\n    Returns:\n        The *(number of rows, number of columns)* in `sheet`.\n    \"\"\"\n    table = self._tables.get(sheet)\n    if table is None:\n        msg = f\"A sheet named {sheet!r} is not in {self._file!r}\"\n        raise ValueError(msg) from None\n\n    num_cols = 0\n    for col in table.findall(\".//table:table-column\", namespaces=self._ns):\n        num_cols += int(self._attribute(col, \"table\", \"number-columns-repeated\", \"1\"))\n\n    num_rows = sum(1 for _ in self._rows(table, 0, sys.maxsize - 1))\n    return (num_rows, num_cols)\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.ods.ODSReader.read","title":"read","text":"<pre><code>read(\n    cells=None,\n    sheet=None,\n    *,\n    as_datetime=True,\n    merged=False,\n)\n</code></pre> <p>Read cell values from the OpenDocument Spreadsheet.</p> <p>Parameters:</p> Name Type Description Default <code>cells</code> <code>str | None</code> <p>The cell(s) to read. For example, <code>C9</code> will return a single value and <code>C9:G20</code> will return all values in the specified range. If not specified then returns all values in the specified <code>sheet</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of the sheet to read the value(s) from. If there is only one sheet in the spreadsheet then you do not need to specify the name of the sheet.</p> <code>None</code> <code>as_datetime</code> <code>bool</code> <p>Whether dates should be returned as datetime or date objects. If <code>False</code>, dates are returned as a string in the display format of the spreadsheet cell.</p> <code>True</code> <code>merged</code> <code>bool</code> <p>Applies to cells that are merged with other cells. If <code>False</code>, the value of each unmerged cell is returned, otherwise the same value is returned for all merged cells. In an OpenDocument Spreadsheet, the value of a hidden cell that is merged with a visible cell can still be retained (depends on how the merger was performed).</p> <code>False</code> <p>Returns:</p> Type Description <code>Any | list[tuple[Any, ...]]</code> <p>The value(s) of the requested cell(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ods.read()\n[('temperature', 'humidity'), (20.33, 49.82), (20.23, 46.06), (20.41, 47.06), (20.29, 48.32)]\n&gt;&gt;&gt; ods.read(\"B2\")\n49.82\n&gt;&gt;&gt; ods.read(\"A:A\")\n[('temperature',), (20.33,), (20.23,), (20.41,), (20.29,)]\n&gt;&gt;&gt; ods.read(\"A1:B1\")\n[('temperature', 'humidity')]\n&gt;&gt;&gt; ods.read(\"A2:B4\")\n[(20.33, 49.82), (20.23, 46.06), (20.41, 47.06)]\n</code></pre> Source code in <code>src/msl/io/readers/ods.py</code> <pre><code>def read(  # noqa: C901, PLR0912\n    self, cells: str | None = None, sheet: str | None = None, *, as_datetime: bool = True, merged: bool = False\n) -&gt; Any | list[tuple[Any, ...]]:\n    \"\"\"Read cell values from the OpenDocument Spreadsheet.\n\n    Args:\n        cells: The cell(s) to read. For example, `C9` will return a single value\n            and `C9:G20` will return all values in the specified range. If not\n            specified then returns all values in the specified `sheet`.\n        sheet: The name of the sheet to read the value(s) from. If there is only\n            one sheet in the spreadsheet then you do not need to specify the name\n            of the sheet.\n        as_datetime: Whether dates should be returned as [datetime][datetime.datetime]\n            or [date][datetime.date] objects. If `False`, dates are returned as a\n            string in the display format of the spreadsheet cell.\n        merged: Applies to cells that are merged with other cells. If `False`, the\n            value of each unmerged cell is returned, otherwise the same value is\n            returned for all merged cells. In an OpenDocument Spreadsheet, the value\n            of a hidden cell that is merged with a visible cell can still be retained\n            (depends on how the merger was performed).\n\n    Returns:\n        The value(s) of the requested cell(s).\n\n    **Examples:**\n    &lt;!-- invisible-code-block: pycon\n    &gt;&gt;&gt; from msl.io import ODSReader\n    &gt;&gt;&gt; ods = ODSReader('./tests/samples/lab_environment.ods')\n\n    --&gt;\n\n    ```pycon\n    &gt;&gt;&gt; ods.read()\n    [('temperature', 'humidity'), (20.33, 49.82), (20.23, 46.06), (20.41, 47.06), (20.29, 48.32)]\n    &gt;&gt;&gt; ods.read(\"B2\")\n    49.82\n    &gt;&gt;&gt; ods.read(\"A:A\")\n    [('temperature',), (20.33,), (20.23,), (20.41,), (20.29,)]\n    &gt;&gt;&gt; ods.read(\"A1:B1\")\n    [('temperature', 'humidity')]\n    &gt;&gt;&gt; ods.read(\"A2:B4\")\n    [(20.33, 49.82), (20.23, 46.06), (20.41, 47.06)]\n\n    ```\n    \"\"\"\n    if not sheet:\n        names = self.sheet_names()\n        if len(names) == 1:\n            name = names[0]\n        elif not names:\n            msg = \"Cannot determine the names of the sheets in the OpenDocument file\"\n            raise ValueError(msg)\n        else:\n            sheets = \", \".join(repr(n) for n in names)\n            msg = (\n                f\"{self.file!r} contains the following sheets:\\n  {sheets}\\n\"\n                f\"You must specify the name of the sheet to read\"\n            )\n            raise ValueError(msg)\n    else:\n        name = sheet\n\n    table = self._tables.get(name)\n    if table is None:\n        msg = f\"A sheet named {sheet!r} is not in {self._file!r}\"\n        raise ValueError(msg)\n\n    maxsize = sys.maxsize - 1\n    r1, c1, r2, c2, contains_colon = 0, 0, maxsize, maxsize, False\n    if cells:\n        split = cells.split(\":\")\n        r, c1 = self.to_indices(split[0])\n        r1 = 0 if r is None else r\n        if len(split) &gt; 1:\n            contains_colon = True\n            r, c2 = self.to_indices(split[1])\n            if r is not None:\n                r2 = r\n        else:\n            r2, c2 = r1, c1\n\n    self._spans.clear()\n    data: list[tuple[Any, ...]] = []\n    for row in self._rows(table, r1, r2):\n        values = tuple(self._cell(row, c1, c2, as_datetime, merged))\n        if values:\n            data.append(values)\n\n    if not contains_colon and r1 == r2 and c1 == c2:\n        try:\n            return data[0][0]\n        except IndexError:\n            return None\n\n    return data\n</code></pre>"},{"location":"api/spreadsheets/#msl.io.readers.ods.ODSReader.sheet_names","title":"sheet_names","text":"<pre><code>sheet_names()\n</code></pre> <p>Get the names of all sheets in the OpenDocument Spreadsheet.</p> <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>The names of all sheets.</p> Source code in <code>src/msl/io/readers/ods.py</code> <pre><code>def sheet_names(self) -&gt; tuple[str, ...]:\n    \"\"\"Get the names of all sheets in the OpenDocument Spreadsheet.\n\n    Returns:\n        The names of all sheets.\n    \"\"\"\n    return tuple(self._tables.keys())\n</code></pre>"},{"location":"api/tables/","title":"tables","text":"<p>Read tabular data from a file.</p>"},{"location":"api/tables/#msl.io.tables.extension_delimiter_map","title":"extension_delimiter_map  <code>module-attribute</code>","text":"<pre><code>extension_delimiter_map = {'.csv': ','}\n</code></pre> <p>The delimiter to use to separate columns in a table based on the file extension.</p> <p>If the <code>delimiter</code> keyword is not specified when calling the read_table function then this extension-delimiter map is used to determine the value of the delimiter to use to separate the columns in a text-based file format. If the file extension is not in the map, then columns are split by any whitespace.</p> <p>See the Overview for an example.</p>"},{"location":"api/tables/#msl.io.tables.read_table","title":"read_table","text":"<pre><code>read_table(file, **kwargs)\n</code></pre> <p>Read data in a table format from a file.</p> <p>A table has the following properties:</p> <ol> <li>The first row is a header</li> <li>All rows have the same number of columns</li> <li>All data values in a column have the same data type</li> </ol> <p>See the Overview for examples.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike</code> <p>The file to read. If <code>file</code> is a Google Sheets spreadsheet then <code>file</code> must end with <code>.gsheet</code> even if the ID of the spreadsheet is specified.</p> required <code>kwargs</code> <code>Any</code> <p>If the file is an Excel spreadsheet then the keyword arguments are passed to read_table_excel. If a Google Sheets spreadsheet then the keyword arguments are passed to read_table_gsheets. If an OpenDocument Spreadsheet then the keyword arguments are passed to read_table_ods. Otherwise, all keyword arguments are passed to read_table_text.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The table as a Dataset. The header is included in the Metadata.</p> Source code in <code>src/msl/io/tables.py</code> <pre><code>def read_table(file: PathLike | ReadLike, **kwargs: Any) -&gt; Dataset:\n    \"\"\"Read data in a table format from a file.\n\n    A *table* has the following properties:\n\n    1. The first row is a header\n    2. All rows have the same number of columns\n    3. All data values in a column have the same data type\n\n    !!! example \"See the [Overview][read-a-table] for examples.\"\n\n    Args:\n        file: The file to read. If `file` is a Google Sheets spreadsheet then `file` must end\n            with `.gsheet` even if the ID of the spreadsheet is specified.\n        kwargs: If the file is an Excel spreadsheet then the keyword arguments are passed to\n            [read_table_excel][msl.io.tables.read_table_excel]. If a Google Sheets spreadsheet then\n            the keyword arguments are passed to [read_table_gsheets][msl.io.tables.read_table_gsheets].\n            If an OpenDocument Spreadsheet then the keyword arguments are passed to\n            [read_table_ods][msl.io.tables.read_table_ods]. Otherwise, all keyword arguments are\n            passed to [read_table_text][msl.io.tables.read_table_text].\n\n    Returns:\n        The table as a [Dataset][msl.io.node.Dataset]. The header is included in the\n            [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    ext = get_extension(file).lower()\n    if ext in {\".xls\", \".xlsx\"}:\n        return read_table_excel(file, **kwargs)\n\n    if ext in {\".ods\", \".fods\"}:\n        return read_table_ods(file, **kwargs)\n\n    if ext == \".gsheet\":\n        file = os.fsdecode(file) if isinstance(file, (bytes, str, os.PathLike)) else str(file.name)\n        return read_table_gsheets(file.removesuffix(\".gsheet\"), **kwargs)\n\n    return read_table_text(file, **kwargs)\n</code></pre>"},{"location":"api/tables/#msl.io.tables.read_table_excel","title":"read_table_excel","text":"<pre><code>read_table_excel(\n    file,\n    *,\n    cells=None,\n    sheet=None,\n    as_datetime=True,\n    dtype=None,\n    **kwargs,\n)\n</code></pre> <p>Read a data table from an Excel spreadsheet.</p> <p>The generic way to read any table is with the read_table function.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike</code> <p>The file to read.</p> required <code>cells</code> <code>str | None</code> <p>The cells to read. For example, <code>C9</code> (i.e, specifying only the top-left cell of the table) will start at cell C9 and include all columns to the right and all rows below C9, <code>A:C</code> includes all rows in columns A, B and C, and, <code>C9:G20</code> includes only the specified cells. If not specified, assumes that the table starts at cell <code>A1</code> and returns all cells from the specified <code>sheet</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of the sheet to read the data from. If there is only one sheet in the workbook then you do not need to specify the name of the sheet.</p> <code>None</code> <code>as_datetime</code> <code>bool</code> <p>Whether dates should be returned as datetime or date objects. If <code>False</code>, dates are returned as an ISO 8601 string.</p> <code>True</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type(s) to use for the table.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to xlrd.open_workbook. Can use an <code>encoding</code> keyword argument as an alias for <code>encoding_override</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The table as a Dataset. The header is included in the Metadata.</p> Source code in <code>src/msl/io/tables.py</code> <pre><code>def read_table_excel(\n    file: PathLike | ReadLike,\n    *,\n    cells: str | None = None,\n    sheet: str | None = None,\n    as_datetime: bool = True,\n    dtype: DTypeLike = None,\n    **kwargs: Any,\n) -&gt; Dataset:\n    \"\"\"Read a data table from an Excel spreadsheet.\n\n    The generic way to read any table is with the [read_table][msl.io.tables.read_table] function.\n\n    Args:\n        file: The file to read.\n        cells: The cells to read. For example, `C9` (i.e, specifying only the top-left cell\n            of the table) will start at cell C9 and include all columns to the right and\n            all rows below C9, `A:C` includes all rows in columns A, B and C, and, `C9:G20`\n            includes only the specified cells. If not specified, assumes that the table\n            starts at cell `A1` and returns all cells from the specified `sheet`.\n        sheet: The name of the sheet to read the data from. If there is only one sheet\n            in the workbook then you do not need to specify the name of the sheet.\n        as_datetime: Whether dates should be returned as [datetime][datetime.datetime] or\n            [date][datetime.date] objects. If `False`, dates are returned as an\n            ISO 8601 string.\n        dtype: The data type(s) to use for the table.\n        kwargs: All additional keyword arguments are passed to [xlrd.open_workbook][].\n            Can use an `encoding` keyword argument as an alias for `encoding_override`.\n\n    Returns:\n        The table as a [Dataset][msl.io.node.Dataset]. The header is included in the\n            [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    file = os.fsdecode(file) if isinstance(file, (bytes, str, os.PathLike)) else str(file.name)\n\n    with ExcelReader(file, **kwargs) as excel:\n        if cells is not None and not _spreadsheet_range_regex.match(cells):\n            match = _spreadsheet_top_left_regex.match(cells)\n            if not match:\n                msg = f\"Invalid cell {cells!r}\"\n                raise ValueError(msg)\n            name = sheet or excel.sheet_names()[0]\n            num_rows, num_cols = excel.dimensions(name)\n            letters = excel.to_letters(num_cols - 1)\n            cells += f\":{letters}{num_rows}\"\n        table = excel.read(cells, sheet=sheet, as_datetime=as_datetime)\n\n    return _spreadsheet_to_dataset(table, file, dtype)\n</code></pre>"},{"location":"api/tables/#msl.io.tables.read_table_gsheets","title":"read_table_gsheets","text":"<pre><code>read_table_gsheets(\n    file,\n    cells=None,\n    sheet=None,\n    *,\n    as_datetime=True,\n    dtype=None,\n    **kwargs,\n)\n</code></pre> <p>Read a data table from a Google Sheets spreadsheet.</p> <p>Note</p> <p>You must have already performed the instructions specified in GDrive and in GSheets to be able to use this function.</p> <p>The generic way to read any table is with the read_table function.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike</code> <p>The file to read. Can be the ID of a Google Sheets spreadsheet.</p> required <code>cells</code> <code>str | None</code> <p>The cells to read. For example, <code>C9</code> (i.e, specifying only the top-left cell of the table) will start at cell C9 and include all columns to the right and all rows below C9, <code>A:C</code> includes all rows in columns A, B and C, and, <code>C9:G20</code> includes only the specified cells. If not specified, assumes that the table starts at cell <code>A1</code> and returns all cells from the specified <code>sheet</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of the sheet to read the data from. If there is only one sheet in the spreadsheet then you do not need to specify the name of the sheet.</p> <code>None</code> <code>as_datetime</code> <code>bool</code> <p>Whether dates should be returned as datetime or date objects. If <code>False</code>, dates are returned as a string in the display format of the spreadsheet cell.</p> <code>True</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type(s) to use for the table.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to GSheetsReader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The table as a Dataset. The header is included in the Metadata.</p> Source code in <code>src/msl/io/tables.py</code> <pre><code>def read_table_gsheets(\n    file: PathLike | ReadLike,\n    cells: str | None = None,\n    sheet: str | None = None,\n    *,\n    as_datetime: bool = True,\n    dtype: DTypeLike = None,\n    **kwargs: Any,\n) -&gt; Dataset:\n    \"\"\"Read a data table from a Google Sheets spreadsheet.\n\n    !!! note\n        You must have already performed the instructions specified in\n        [GDrive][msl.io.google_api.GDrive] and in [GSheets][msl.io.google_api.GSheets]\n        to be able to use this function.\n\n    The generic way to read any table is with the [read_table][msl.io.tables.read_table] function.\n\n    Args:\n        file: The file to read. Can be the ID of a Google Sheets spreadsheet.\n        cells: The cells to read. For example, `C9` (i.e, specifying only the top-left cell\n            of the table) will start at cell C9 and include all columns to the right and\n            all rows below C9, `A:C` includes all rows in columns A, B and C, and, `C9:G20`\n            includes only the specified cells. If not specified, assumes that the table\n            starts at cell `A1` and returns all cells from the specified `sheet`.\n        sheet: The name of the sheet to read the data from. If there is only one sheet\n            in the spreadsheet then you do not need to specify the name of the sheet.\n        as_datetime: Whether dates should be returned as [datetime][datetime.datetime] or\n            [date][datetime.date] objects. If `False`, dates are returned as a string in\n            the display format of the spreadsheet cell.\n        dtype: The data type(s) to use for the table.\n        kwargs: All additional keyword arguments are passed to [GSheetsReader][msl.io.readers.gsheets.GSheetsReader].\n\n    Returns:\n        The table as a [Dataset][msl.io.node.Dataset]. The header is included in the\n            [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    file = os.fsdecode(file) if isinstance(file, (bytes, str, os.PathLike)) else str(file.name)\n\n    with GSheetsReader(file, **kwargs) as sheets:\n        if cells is not None and not _spreadsheet_range_regex.match(cells):\n            if not _spreadsheet_top_left_regex.match(cells):\n                msg = f\"Invalid cell {cells!r}\"\n                raise ValueError(msg)\n\n            r, c = sheets.to_indices(cells)\n            data = sheets.read(sheet=sheet, as_datetime=as_datetime)\n            table = [row[c:] for row in data[r:]]\n        else:\n            table = sheets.read(cells, sheet=sheet, as_datetime=as_datetime)\n\n    return _spreadsheet_to_dataset(table, file, dtype)\n</code></pre>"},{"location":"api/tables/#msl.io.tables.read_table_ods","title":"read_table_ods","text":"<pre><code>read_table_ods(\n    file,\n    *,\n    cells=None,\n    sheet=None,\n    as_datetime=True,\n    merged=False,\n    dtype=None,\n    **kwargs,\n)\n</code></pre> <p>Read a data table from an OpenDocument Spreadsheet.</p> <p>The generic way to read any table is with the read_table function.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike</code> <p>The file to read.</p> required <code>cells</code> <code>str | None</code> <p>The cells to read. For example, <code>C9</code> (i.e, specifying only the top-left cell of the table) will start at cell C9 and include all columns to the right and all rows below C9, <code>A:C</code> includes all rows in columns A, B and C, and, <code>C9:G20</code> includes only the specified cells. If not specified, assumes that the table starts at cell <code>A1</code> and returns all cells from the specified <code>sheet</code>.</p> <code>None</code> <code>sheet</code> <code>str | None</code> <p>The name of the sheet to read the data from. If there is only one sheet in the OpenDocument then you do not need to specify the name of the sheet.</p> <code>None</code> <code>as_datetime</code> <code>bool</code> <p>Whether dates should be returned as datetime or date objects. If <code>False</code>, dates are returned as a string in the display format of the spreadsheet cell.</p> <code>True</code> <code>merged</code> <code>bool</code> <p>Applies to cells that are merged with other cells. If <code>False</code>, the value of each unmerged cell is returned, otherwise the same value is returned for all merged cells. In an OpenDocument Spreadsheet, the value of a hidden cell that is merged with a visible cell can still be retained (depends on how the merger was performed).</p> <code>False</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type(s) to use for the table.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All keyword arguments are passed to ODSReader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The table as a Dataset. The header is included in the Metadata.</p> Source code in <code>src/msl/io/tables.py</code> <pre><code>def read_table_ods(  # noqa: PLR0913\n    file: PathLike | ReadLike,\n    *,\n    cells: str | None = None,\n    sheet: str | None = None,\n    as_datetime: bool = True,\n    merged: bool = False,\n    dtype: DTypeLike = None,\n    **kwargs: Any,\n) -&gt; Dataset:\n    \"\"\"Read a data table from an OpenDocument Spreadsheet.\n\n    The generic way to read any table is with the [read_table][msl.io.tables.read_table] function.\n\n    Args:\n        file: The file to read.\n        cells: The cells to read. For example, `C9` (i.e, specifying only the top-left cell\n            of the table) will start at cell C9 and include all columns to the right and\n            all rows below C9, `A:C` includes all rows in columns A, B and C, and, `C9:G20`\n            includes only the specified cells. If not specified, assumes that the table\n            starts at cell `A1` and returns all cells from the specified `sheet`.\n        sheet: The name of the sheet to read the data from. If there is only one sheet\n            in the OpenDocument then you do not need to specify the name of the sheet.\n        as_datetime: Whether dates should be returned as [datetime][datetime.datetime] or\n            [date][datetime.date] objects. If `False`, dates are returned as a string in\n            the display format of the spreadsheet cell.\n        merged: Applies to cells that are merged with other cells. If `False`, the\n            value of each unmerged cell is returned, otherwise the same value is\n            returned for all merged cells. In an OpenDocument Spreadsheet, the value\n            of a hidden cell that is merged with a visible cell can still be retained\n            (depends on how the merger was performed).\n        dtype: The data type(s) to use for the table.\n        kwargs: All keyword arguments are passed to [ODSReader][msl.io.readers.ods.ODSReader].\n\n    Returns:\n        The table as a [Dataset][msl.io.node.Dataset]. The header is included in the\n            [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    file = os.fsdecode(file) if isinstance(file, (bytes, str, os.PathLike)) else str(file.name)\n    with ODSReader(file, **kwargs) as ods:\n        if cells is not None and not _spreadsheet_range_regex.match(cells):\n            match = _spreadsheet_top_left_regex.match(cells)\n            if not match:\n                msg = f\"Invalid cell {cells!r}\"\n                raise ValueError(msg)\n            name = sheet or ods.sheet_names()[0]\n            num_rows, num_columns = ods.dimensions(name)\n            letters = ods.to_letters(num_columns - 1)\n            cells += f\":{letters}{num_rows}\"\n        table = ods.read(cells, sheet=sheet, as_datetime=as_datetime, merged=merged)\n    return _spreadsheet_to_dataset(table, file, dtype)\n</code></pre>"},{"location":"api/tables/#msl.io.tables.read_table_text","title":"read_table_text","text":"<pre><code>read_table_text(file, **kwargs)\n</code></pre> <p>Read a data table from a text-based file.</p> <p>The generic way to read any table is with the read_table function.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike</code> <p>The file to read.</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are passed to numpy loadtxt. If the <code>delimiter</code> is not specified and the <code>file</code> has <code>.csv</code> as the file extension then the <code>delimiter</code> is automatically set to be <code>,</code> (see extension_delimiter_map for more details).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The table as a Dataset. The header is included in the Metadata.</p> Source code in <code>src/msl/io/tables.py</code> <pre><code>def read_table_text(file: PathLike | ReadLike, **kwargs: Any) -&gt; Dataset:\n    \"\"\"Read a data table from a text-based file.\n\n    The generic way to read any table is with the [read_table][msl.io.tables.read_table] function.\n\n    Args:\n        file: The file to read.\n        kwargs: All keyword arguments are passed to numpy [loadtxt][numpy.loadtxt]. If the\n            `delimiter` is not specified and the `file` has `.csv` as the file\n            extension then the `delimiter` is automatically set to be `,` (see\n            [extension_delimiter_map][msl.io.tables.extension_delimiter_map]\n            for more details).\n\n    Returns:\n        The table as a [Dataset][msl.io.node.Dataset]. The header is included in the\n            [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    if kwargs.get(\"unpack\", False):\n        msg = \"Cannot use the 'unpack' option\"\n        raise ValueError(msg)\n\n    if \"delimiter\" not in kwargs:\n        ext = get_extension(file).lower()\n        kwargs[\"delimiter\"] = extension_delimiter_map.get(ext)\n\n    if \"skiprows\" not in kwargs:\n        kwargs[\"skiprows\"] = 0\n    kwargs[\"skiprows\"] += 1  # Reader.get_lines is 1-based, np.loadtxt is 0-based\n\n    first_line = [\n        h.decode() if isinstance(h, bytes) else h for h in get_lines(file, kwargs[\"skiprows\"], kwargs[\"skiprows\"])\n    ]\n\n    if not first_line:\n        header, data = [], np.array([])\n    else:\n        header = first_line[0].split(kwargs[\"delimiter\"])\n\n        use_cols = kwargs.get(\"usecols\")\n        if use_cols:\n            if isinstance(use_cols, int):\n                use_cols = [use_cols]\n            header = [header[i] for i in use_cols]\n\n        dtype = kwargs.get(\"dtype\")\n        if isinstance(dtype, str) and dtype.startswith(\"header\"):\n            kwargs[\"dtype\"] = _header_dtype(dtype, header)\n\n        # Calling np.loadtxt (on Python 3.5, 3.6 and 3.7) on a file\n        # on a mapped drive could raise an OSError. This occurred\n        # when a local folder was shared and then mapped on the same\n        # computer. Opening the file using open() and then passing\n        # in the file handle to np.loadtxt is more universal\n        if isinstance(file, (bytes, str, os.PathLike)):\n            with Path(os.fsdecode(file)).open() as f:\n                data = np.loadtxt(f, **kwargs)\n        else:\n            data = np.loadtxt(file, **kwargs)\n\n    return Dataset(\n        name=get_basename(file), parent=None, read_only=True, data=data, header=np.asarray(header, dtype=str)\n    )\n</code></pre>"},{"location":"api/types/","title":"types","text":"<p>Custom type annotations.</p>"},{"location":"api/types/#msl.io.types.Buffer","title":"Buffer  <code>module-attribute</code>","text":"<pre><code>Buffer = Buffer\n</code></pre> <p>Object exposing the buffer protocol.</p>"},{"location":"api/types/#msl.io.types.PathLike","title":"PathLike  <code>module-attribute</code>","text":"<pre><code>PathLike = Union[str, bytes, PathLike[str], PathLike[bytes]]\n</code></pre> <p>A path-like object.</p>"},{"location":"api/types/#msl.io.types.ReadLike","title":"ReadLike  <code>module-attribute</code>","text":"<pre><code>ReadLike = Union[FileLikeRead[str], FileLikeRead[bytes]]\n</code></pre> <p>A file-like object for reading str or bytes.</p>"},{"location":"api/types/#msl.io.types.ShapeLike","title":"ShapeLike  <code>module-attribute</code>","text":"<pre><code>ShapeLike = Union[SupportsIndex, Sequence[SupportsIndex]]\n</code></pre> <p>Anything that can be coerced to a shape tuple for an ndarray.</p>"},{"location":"api/types/#msl.io.types.ToIndex","title":"ToIndex  <code>module-attribute</code>","text":"<pre><code>ToIndex = Union[SupportsIndex, slice, EllipsisType, None]\n</code></pre> <p>Anything that can be used as the <code>key</code> for numpy.ndarray.__setitem__.</p>"},{"location":"api/types/#msl.io.types.ToIndices","title":"ToIndices  <code>module-attribute</code>","text":"<pre><code>ToIndices = Union[ToIndex, tuple[ToIndex, ...]]\n</code></pre> <p>Anything that can be used as the <code>key</code> for numpy.ndarray.__setitem__.</p>"},{"location":"api/types/#msl.io.types.WriteLike","title":"WriteLike  <code>module-attribute</code>","text":"<pre><code>WriteLike = Union[FileLikeWrite[str], FileLikeWrite[bytes]]\n</code></pre> <p>A file-like object for writing str or bytes.</p>"},{"location":"api/types/#msl.io.types.FileLikeRead","title":"FileLikeRead","text":"<p>               Bases: <code>Iterator[T_co]</code>, <code>Protocol[T_co]</code></p> <p>A file-like object for reading.</p>"},{"location":"api/types/#msl.io.types.FileLikeRead.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>File name.</p>"},{"location":"api/types/#msl.io.types.FileLikeRead.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the stream.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the stream.\"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.FileLikeRead.read","title":"read","text":"<pre><code>read(size=-1)\n</code></pre> <p>Read from the stream.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def read(self, size: int | None = -1, /) -&gt; T_co:\n    \"\"\"Read from the stream.\"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.FileLikeRead.readline","title":"readline","text":"<pre><code>readline()\n</code></pre> <p>Read a line from the stream.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def readline(self) -&gt; T_co:\n    \"\"\"Read a line from the stream.\"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.FileLikeRead.seek","title":"seek","text":"<pre><code>seek(offset, whence=0)\n</code></pre> <p>Change the stream position to the given byte offset.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def seek(self, offset: int, whence: int = 0, /) -&gt; int:\n    \"\"\"Change the stream position to the given byte offset.\"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.FileLikeRead.tell","title":"tell","text":"<pre><code>tell()\n</code></pre> <p>Returns the current stream position.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def tell(self) -&gt; int:\n    \"\"\"Returns the current stream position.\"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.FileLikeWrite","title":"FileLikeWrite","text":"<p>               Bases: <code>Protocol[T_contra]</code></p> <p>A file-like object for writing.</p>"},{"location":"api/types/#msl.io.types.FileLikeWrite.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>File name.</p>"},{"location":"api/types/#msl.io.types.FileLikeWrite.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the stream.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the stream.\"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.FileLikeWrite.write","title":"write","text":"<pre><code>write(b)\n</code></pre> <p>Write to the stream.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def write(self, b: T_contra, /) -&gt; int:\n    \"\"\"Write to the stream.\"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.MediaDownloadProgress","title":"MediaDownloadProgress","text":"<p>               Bases: <code>Protocol</code></p> <p>Status of a resumable GDrive download.</p>"},{"location":"api/types/#msl.io.types.MediaDownloadProgress.resumable_progress","title":"resumable_progress  <code>instance-attribute</code>","text":"<pre><code>resumable_progress\n</code></pre> <p>int \u2014 Number of bytes received so far.</p>"},{"location":"api/types/#msl.io.types.MediaDownloadProgress.total_size","title":"total_size  <code>instance-attribute</code>","text":"<pre><code>total_size\n</code></pre> <p>int \u2014 Total number of bytes in complete download.</p>"},{"location":"api/types/#msl.io.types.MediaDownloadProgress.progress","title":"progress","text":"<pre><code>progress()\n</code></pre> <p>Percent of download completed.</p> <p>Returns:</p> Type Description <code>float</code> <p>Download percentage.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def progress(self) -&gt; float:\n    \"\"\"Percent of download completed.\n\n    Returns:\n        Download percentage.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/types/#msl.io.types.SupportsRead","title":"SupportsRead","text":"<p>               Bases: <code>Protocol[T_co]</code></p> <p>A file-like object that has a <code>read</code> method.</p>"},{"location":"api/types/#msl.io.types.SupportsRead.read","title":"read","text":"<pre><code>read(size=-1)\n</code></pre> <p>Read from the stream.</p> Source code in <code>src/msl/io/types.py</code> <pre><code>def read(self, size: int | None = -1, /) -&gt; T_co:\n    \"\"\"Read from the stream.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/","title":"utils","text":"<p>General functions.</p>"},{"location":"api/utils/#msl.io.utils.GitHead","title":"GitHead  <code>dataclass</code>","text":"<pre><code>GitHead(hash, timestamp)\n</code></pre> <p>Information about the HEAD of a git repository.</p> <p>This class is returned from the git_head function.</p>"},{"location":"api/utils/#msl.io.utils.checksum","title":"checksum","text":"<pre><code>checksum(\n    file,\n    *,\n    algorithm=\"sha256\",\n    chunk_size=65536,\n    shake_length=256,\n)\n</code></pre> <p>Get the checksum of a file.</p> <p>A checksum is a sequence of numbers and letters that act as a fingerprint for a file against which later comparisons can be made to detect errors or changes in the file. It can be used to verify the integrity of the file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | FileLikeRead[bytes]</code> <p>A file to get the checksum of.</p> required <code>algorithm</code> <code>str</code> <p>The hash algorithm to use to compute the checksum. See hashlib for more details.</p> <code>'sha256'</code> <code>chunk_size</code> <code>int</code> <p>The number of bytes to read at a time from the file. It is useful to tweak this parameter when reading a large file to improve performance.</p> <code>65536</code> <code>shake_length</code> <code>int</code> <p>The digest length to use for the <code>shake_128</code> or <code>shake_256</code> algorithm. See hexdigest for more details.</p> <code>256</code> <p>Returns:</p> Type Description <code>str</code> <p>The checksum value (which only contains hexadecimal digits).</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def checksum(\n    file: PathLike | FileLikeRead[bytes], *, algorithm: str = \"sha256\", chunk_size: int = 65536, shake_length: int = 256\n) -&gt; str:\n    \"\"\"Get the checksum of a file.\n\n    A checksum is a sequence of numbers and letters that act as a fingerprint\n    for a file against which later comparisons can be made to detect errors or\n    changes in the file. It can be used to verify the integrity of the file.\n\n    Args:\n        file: A file to get the checksum of.\n        algorithm: The hash algorithm to use to compute the checksum. See [hashlib][] for more details.\n        chunk_size: The number of bytes to read at a time from the file. It is useful\n            to tweak this parameter when reading a large file to improve performance.\n        shake_length: The digest length to use for the `shake_128` or `shake_256` algorithm.\n            See [hexdigest][hashlib.shake.hexdigest] for more details.\n\n    Returns:\n        The checksum value (which only contains hexadecimal digits).\n    \"\"\"\n    import hashlib  # noqa: PLC0415\n\n    def _read(fp: FileLikeRead[bytes]) -&gt; None:\n        # read in chucks to avoid loading the entire file at once\n        while True:\n            data = fp.read(chunk_size)\n            if not data:\n                break\n            _hash.update(data)\n\n    _hash = hashlib.new(algorithm)\n\n    if isinstance(file, (str, bytes, os.PathLike)):\n        with Path(os.fsdecode(file)).open(\"rb\") as f:\n            _read(f)\n    else:\n        position = file.tell()\n        _read(file)\n        _ = file.seek(position)\n\n    try:\n        return _hash.hexdigest()\n    except TypeError:\n        return _hash.hexdigest(shake_length)  # type: ignore[call-arg]  # pyright: ignore[reportCallIssue,reportUnknownVariableType]\n</code></pre>"},{"location":"api/utils/#msl.io.utils.copy","title":"copy","text":"<pre><code>copy(\n    source,\n    destination,\n    *,\n    overwrite=False,\n    include_metadata=True,\n    follow_symlinks=True,\n)\n</code></pre> <p>Copy a file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>PathLike</code> <p>The path to a file to copy.</p> required <code>destination</code> <code>PathLike</code> <p>A directory to copy the file to or a full path (i.e., includes the basename). If the directory does not exist then it, and all intermediate directories, will be created.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the <code>destination</code> file if it already exists. If <code>destination</code> already exists and <code>overwrite</code> is <code>False</code> then a FileExistsError is raised.</p> <code>False</code> <code>include_metadata</code> <code>bool</code> <p>Whether to also copy information such as the file permissions, the latest access time and latest modification time with the file.</p> <code>True</code> <code>follow_symlinks</code> <code>bool</code> <p>Whether to follow symbolic links.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to where the file was copied.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def copy(\n    source: PathLike,\n    destination: PathLike,\n    *,\n    overwrite: bool = False,\n    include_metadata: bool = True,\n    follow_symlinks: bool = True,\n) -&gt; Path:\n    \"\"\"Copy a file.\n\n    Args:\n        source: The path to a file to copy.\n        destination: A directory to copy the file to or a full path (i.e., includes the basename).\n            If the directory does not exist then it, and all intermediate directories, will be created.\n        overwrite: Whether to overwrite the `destination` file if it already exists. If `destination`\n            already exists and `overwrite` is `False` then a [FileExistsError][] is raised.\n        include_metadata: Whether to also copy information such as the file permissions,\n            the latest access time and latest modification time with the file.\n        follow_symlinks: Whether to follow symbolic links.\n\n    Returns:\n        The path to where the file was copied.\n    \"\"\"\n    import shutil  # noqa: PLC0415\n\n    src = Path(os.fsdecode(source))\n    dst = Path(os.fsdecode(destination))\n    if dst.is_dir():\n        dst = dst / src.name\n    else:\n        dst.parent.mkdir(parents=True, exist_ok=True)\n\n    if not overwrite and dst.is_file():\n        msg = f\"Will not overwrite {destination!r}\"\n        raise FileExistsError(msg)\n\n    _ = shutil.copyfile(src, dst, follow_symlinks=follow_symlinks)\n    if include_metadata:\n        shutil.copystat(src, dst, follow_symlinks=follow_symlinks)\n\n    return dst\n</code></pre>"},{"location":"api/utils/#msl.io.utils.get_basename","title":"get_basename","text":"<pre><code>get_basename(obj)\n</code></pre> <p>Get the basename (the final path component) of a file.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>PathLike | ReadLike | WriteLike</code> <p>The object to get the basename of. If <code>obj</code> is an in-memory file-like object then the class __name__ of <code>obj</code> is returned.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The basename of <code>obj</code>.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def get_basename(obj: PathLike | ReadLike | WriteLike) -&gt; str:\n    r\"\"\"Get the basename (the final path component) of a file.\n\n    Args:\n        obj: The object to get the basename of. If `obj` is an in-memory file-like\n            object then the class [\\_\\_name\\_\\_][definition.__name__] of `obj` is returned.\n\n    Returns:\n        The basename of `obj`.\n    \"\"\"\n    if isinstance(obj, (str, bytes, os.PathLike)):\n        return Path(os.fsdecode(obj)).name\n\n    try:\n        return Path(obj.name).name\n    except AttributeError:\n        return obj.__class__.__name__\n</code></pre>"},{"location":"api/utils/#msl.io.utils.get_bytes","title":"get_bytes","text":"<pre><code>get_bytes(file, *positions)\n</code></pre> <p>Return bytes from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>FileLikeRead[bytes] | PathLike</code> <p>The file to read bytes from.</p> required <code>positions</code> <code>int | None</code> <p>The position(s) in the file to retrieve bytes from.</p> <code>()</code> <p>Examples: <pre><code>get_bytes(file)  # returns all bytes\nget_bytes(file, 5)  # returns the first 5 bytes\nget_bytes(file, -5)  # returns the last 5 bytes\nget_bytes(file, 5, 10)  # returns bytes 5 through 10 (inclusive)\nget_bytes(file, 3, -1)  # skips the first 2 bytes and returns the rest\nget_bytes(file, -8, -4)  # returns the eighth- through fourth-last bytes (inclusive)\nget_bytes(file, 1, -1, 2)  # returns every other byte\n</code></pre></p> <p>Returns:</p> Type Description <code>bytes</code> <p>The bytes from the file.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def get_bytes(file: FileLikeRead[bytes] | PathLike, *positions: int | None) -&gt; bytes:  # noqa: C901, PLR0912, PLR0915\n    \"\"\"Return bytes from a file.\n\n    Args:\n        file: The file to read bytes from.\n        positions: The position(s) in the file to retrieve bytes from.\n\n    **Examples:**\n    ```python\n    get_bytes(file)  # returns all bytes\n    get_bytes(file, 5)  # returns the first 5 bytes\n    get_bytes(file, -5)  # returns the last 5 bytes\n    get_bytes(file, 5, 10)  # returns bytes 5 through 10 (inclusive)\n    get_bytes(file, 3, -1)  # skips the first 2 bytes and returns the rest\n    get_bytes(file, -8, -4)  # returns the eighth- through fourth-last bytes (inclusive)\n    get_bytes(file, 1, -1, 2)  # returns every other byte\n    ```\n\n    Returns:\n        The bytes from the file.\n    \"\"\"\n    size: int\n    path: Path | None\n    if isinstance(file, (bytes, str, os.PathLike)):\n        path = Path(os.fsdecode(file))\n        try:\n            size = path.stat().st_size\n        except OSError:\n            # A file on a mapped network drive can raise the following:\n            #   [WinError 87] The parameter is incorrect\n            # for Python 3.5, 3.6 and 3.7. Also, calling os.path.getsize\n            # on a file on a mapped network drive could return 0\n            # (without raising OSError) on Python 3.8 and 3.9, which is\n            # why we set size=0 on an OSError\n            size = 0\n\n        if size == 0:\n            with path.open(\"rb\") as f:\n                _ = f.seek(0, os.SEEK_END)\n                size = f.tell()\n    else:\n        path = None\n        position = file.tell()\n        _ = file.seek(0, os.SEEK_END)\n        size = file.tell()\n        _ = file.seek(position)\n\n    if not positions:\n        start, stop, step = 0, size, 1\n    elif len(positions) == 1:\n        start, step = 0, 1\n        stop = size if positions[0] is None else positions[0]\n        if stop &lt; 0:\n            start, stop = size + stop + 1, size\n    elif len(positions) == 2:  # noqa: PLR2004\n        start, step = positions[0] or 0, 1\n        stop = size if positions[1] is None or positions[1] == -1 else positions[1]\n    else:\n        start, stop, step = positions[0] or 0, positions[1] or size, positions[2] or 1\n\n    if start &lt; 0:\n        start = max(size + start, 0)\n    elif start &gt; 0:\n        start -= 1\n    start = min(size, start)\n\n    if stop &lt; 0:\n        stop += size + 1\n    stop = min(size, stop)\n\n    n_bytes = max(0, stop - start)\n    if isinstance(file, (bytes, str, os.PathLike)):\n        assert path is not None  # noqa: S101\n        with path.open(\"rb\") as f:\n            _ = f.seek(start)\n            data = f.read(n_bytes)\n    else:\n        position = file.tell()\n        _ = file.seek(start)\n        data = file.read(n_bytes)\n        _ = file.seek(position)\n\n    if step != 1:\n        return data[::step]\n    return data\n</code></pre>"},{"location":"api/utils/#msl.io.utils.get_extension","title":"get_extension","text":"<pre><code>get_extension(file)\n</code></pre> <p>Return the extension of the file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike | WriteLike</code> <p>The file to get the extension of.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extension (including the <code>.</code>).</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def get_extension(file: PathLike | ReadLike | WriteLike) -&gt; str:\n    \"\"\"Return the extension of the file.\n\n    Args:\n        file: The file to get the extension of.\n\n    Returns:\n        The extension (including the `.`).\n    \"\"\"\n    if isinstance(file, (bytes, str, os.PathLike)):\n        return Path(os.fsdecode(file)).suffix\n\n    try:\n        return get_extension(file.name)\n    except AttributeError:\n        return \"\"\n</code></pre>"},{"location":"api/utils/#msl.io.utils.get_lines","title":"get_lines","text":"<pre><code>get_lines(\n    file: FileLikeRead[str] | PathLike,\n    *lines: int | None,\n    remove_empty_lines: bool = False,\n    encoding: str | None = \"utf-8\",\n    errors: Literal[\"strict\", \"ignore\"] | None = \"strict\",\n) -&gt; list[str]\n</code></pre><pre><code>get_lines(\n    file: FileLikeRead[bytes],\n    *lines: int | None,\n    remove_empty_lines: bool = False,\n    encoding: str | None = None,\n    errors: Literal[\"strict\", \"ignore\"] | None = None,\n) -&gt; list[bytes]\n</code></pre> <pre><code>get_lines(\n    file,\n    *lines,\n    remove_empty_lines=False,\n    encoding=\"utf-8\",\n    errors=\"strict\",\n)\n</code></pre> <p>Return lines from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | ReadLike</code> <p>The file to read lines from.</p> required <code>lines</code> <code>int | None</code> <p>The line(s) in the file to get.</p> <code>()</code> <code>remove_empty_lines</code> <code>bool</code> <p>Whether to remove all empty lines.</p> <code>False</code> <code>encoding</code> <code>str | None</code> <p>The name of the encoding to use to decode the file.</p> <code>'utf-8'</code> <code>errors</code> <code>Literal['strict', 'ignore'] | None</code> <p>How encoding errors are to be handled.</p> <code>'strict'</code> <p>Examples: <pre><code>get_lines(file)  # returns all lines\nget_lines(file, 5)  # returns the first 5 lines\nget_lines(file, -5)  # returns the last 5 lines\nget_lines(file, 2, 4)  # returns lines 2, 3 and 4\nget_lines(file, 2, -2)  # skips the first and last lines and returns the rest\nget_lines(file, -4, -2)  # returns the fourth-, third- and second-last lines\nget_lines(file, 1, -1, 6)  # returns every sixth line in the file\n</code></pre></p> <p>Returns:</p> Type Description <code>list[bytes] | list[str]</code> <p>The lines from the <code>file</code>. Trailing whitespace is stripped from each line. A list[bytes] is returned if <code>file</code> is a file-like object opened in binary mode, otherwise a list[str] is returned.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def get_lines(  # noqa: PLR0912\n    file: PathLike | ReadLike,\n    *lines: int | None,\n    remove_empty_lines: bool = False,\n    encoding: str | None = \"utf-8\",\n    errors: Literal[\"strict\", \"ignore\"] | None = \"strict\",\n) -&gt; list[bytes] | list[str]:\n    \"\"\"Return lines from a file.\n\n    Args:\n        file: The file to read lines from.\n        lines: The line(s) in the file to get.\n        remove_empty_lines: Whether to remove all empty lines.\n        encoding: The name of the encoding to use to decode the file.\n        errors: How encoding errors are to be handled.\n\n    **Examples:**\n    ```python\n    get_lines(file)  # returns all lines\n    get_lines(file, 5)  # returns the first 5 lines\n    get_lines(file, -5)  # returns the last 5 lines\n    get_lines(file, 2, 4)  # returns lines 2, 3 and 4\n    get_lines(file, 2, -2)  # skips the first and last lines and returns the rest\n    get_lines(file, -4, -2)  # returns the fourth-, third- and second-last lines\n    get_lines(file, 1, -1, 6)  # returns every sixth line in the file\n    ```\n\n    Returns:\n        The lines from the `file`. Trailing whitespace is stripped from each line.\n            A [list][][[bytes][]] is returned if `file` is a file-like object\n            opened in binary mode, otherwise a [list][][[str][]] is returned.\n    \"\"\"\n    # want the \"stop\" line to be included\n    if (len(lines) &gt; 1) and (lines[1] is not None) and (lines[1] &lt; 0):\n        lines = (lines[0], None, *lines[2:]) if lines[1] == -1 else (lines[0], lines[1] + 1, *lines[2:])\n\n    # want the \"start\" line to be included\n    if (len(lines) &gt; 1) and (lines[0] is not None) and (lines[0] &gt; 0):\n        lines = (lines[0] - 1, *lines[1:])\n\n    result: list[bytes] | list[str]\n    # itertools.islice does not support negative indices, but want to allow\n    # getting the last \"N\" lines from a file.\n    if any(val &lt; 0 for val in lines if val):\n        if isinstance(file, (bytes, str, os.PathLike)):\n            with Path(os.fsdecode(file)).open(encoding=encoding, errors=errors) as f:\n                result = [line.rstrip() for line in f]\n        else:\n            position = file.tell()\n            result = [line.rstrip() for line in file]  # type: ignore[assignment]  # pyright: ignore[reportAssignmentType]\n            _ = file.seek(position)\n\n        assert lines  # noqa: S101\n        if len(lines) == 1:\n            result = result[lines[0] :]\n        elif len(lines) == 2:  # noqa: PLR2004\n            result = result[lines[0] : lines[1]]\n        else:\n            result = result[lines[0] : lines[1] : lines[2]]\n\n    else:\n        if not lines:\n            lines = (None,)\n\n        if isinstance(file, (bytes, str, os.PathLike)):\n            with Path(os.fsdecode(file)).open(encoding=encoding, errors=errors) as f:\n                result = [line.rstrip() for line in itertools.islice(f, *lines)]\n        else:\n            position = file.tell()\n            result = [line.rstrip() for line in itertools.islice(file, *lines)]  # type: ignore[attr-defined]  # pyright: ignore[reportAssignmentType]\n            _ = file.seek(position)\n\n    if remove_empty_lines:\n        return [line for line in result if line]  # type: ignore[return-value]  # pyright: ignore[reportReturnType]\n    return result\n</code></pre>"},{"location":"api/utils/#msl.io.utils.git_head","title":"git_head","text":"<pre><code>git_head(directory)\n</code></pre> <p>Get information about the HEAD of a repository.</p> <p>This function requires that git is installed and that it is available on the <code>PATH</code> environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>PathLike</code> <p>A directory that is under version control.</p> required <p>Returns:</p> Type Description <code>GitHead | None</code> <p>Information about the most recent commit on the current branch. If <code>directory</code> is not a directory that is under version control then returns <code>None</code>.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def git_head(directory: PathLike) -&gt; GitHead | None:\n    \"\"\"Get information about the [HEAD]{:target=\"_blank\"} of a repository.\n\n    This function requires that [git](https://git-scm.com/){:target=\"_blank\"} is installed and\n    that it is available on the `PATH` environment variable.\n\n    [HEAD]: https://git-scm.com/docs/gitglossary#def_HEAD\n\n    Args:\n        directory: A directory that is under version control.\n\n    Returns:\n        Information about the most recent commit on the current branch.\n            If `directory` is not a directory that is under version control\n            then returns `None`.\n    \"\"\"\n    cmd = [\"git\", \"show\", \"-s\", \"--format=%H %ct\", \"HEAD\"]\n    try:\n        out = subprocess.check_output(cmd, cwd=directory, stderr=subprocess.PIPE)  # noqa: S603\n    except subprocess.CalledProcessError:\n        return None\n\n    sha, timestamp = out.split()\n    return GitHead(hash=sha.decode(\"ascii\"), timestamp=datetime.fromtimestamp(int(timestamp)))  # noqa: DTZ006\n</code></pre>"},{"location":"api/utils/#msl.io.utils.is_admin","title":"is_admin","text":"<pre><code>is_admin()\n</code></pre> <p>Check if the current process is being run as an administrator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the current process is being run as an administrator, otherwise <code>False</code>.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def is_admin() -&gt; bool:\n    \"\"\"Check if the current process is being run as an administrator.\n\n    Returns:\n        `True` if the current process is being run as an administrator, otherwise `False`.\n    \"\"\"\n    import ctypes  # noqa: PLC0415\n\n    try:\n        is_admin: int = ctypes.windll.shell32.IsUserAnAdmin()\n    except AttributeError:\n        if sys.platform != \"win32\":\n            return os.geteuid() == 0\n        return False\n    else:\n        return is_admin == 1\n</code></pre>"},{"location":"api/utils/#msl.io.utils.is_dir_accessible","title":"is_dir_accessible","text":"<pre><code>is_dir_accessible(path, *, strict=False)\n</code></pre> <p>Check if a directory exists and is accessible.</p> <p>An accessible directory is one that the user has permission to access.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The directory to check.</p> required <code>strict</code> <code>bool</code> <p>Whether to raise an exception if the directory is not accessible.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the directory exists and is accessible.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def is_dir_accessible(path: PathLike, *, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a directory exists and is accessible.\n\n    An accessible directory is one that the user has permission to access.\n\n    Args:\n        path: The directory to check.\n        strict: Whether to raise an exception if the directory is not accessible.\n\n    Returns:\n        Whether the directory exists and is accessible.\n    \"\"\"\n    cwd = Path.cwd()\n    try:\n        os.chdir(path)\n    except (OSError, TypeError):\n        if strict:\n            raise\n        return False\n    else:\n        os.chdir(cwd)\n        return True\n</code></pre>"},{"location":"api/utils/#msl.io.utils.is_file_readable","title":"is_file_readable","text":"<pre><code>is_file_readable(file, *, strict=False)\n</code></pre> <p>Check if a file exists and is readable.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike</code> <p>The file to check.</p> required <code>strict</code> <code>bool</code> <p>Whether to raise an exception if the file does not exist or is not readable.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the file exists and is readable.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def is_file_readable(file: PathLike, *, strict: bool = False) -&gt; bool:\n    \"\"\"Check if a file exists and is readable.\n\n    Args:\n        file: The file to check.\n        strict: Whether to raise an exception if the file does not exist or is not readable.\n\n    Returns:\n        Whether the file exists and is readable.\n    \"\"\"\n    try:\n        with Path(os.fsdecode(file)).open(\"rb\"):\n            return True\n    except (OSError, TypeError):\n        if strict:\n            raise\n        return False\n</code></pre>"},{"location":"api/utils/#msl.io.utils.remove_write_permissions","title":"remove_write_permissions","text":"<pre><code>remove_write_permissions(path)\n</code></pre> <p>Remove all write permissions of a file.</p> <p>On Windows, this function will set the file attribute to be read only.</p> <p>On Linux and macOS, write permission is removed for the User, Group and Others. The read and execute permissions are preserved.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to remove the write permissions of.</p> required Source code in <code>src/msl/io/utils.py</code> <pre><code>def remove_write_permissions(path: PathLike) -&gt; None:\n    \"\"\"Remove all write permissions of a file.\n\n    On Windows, this function will set the file attribute to be read only.\n\n    On Linux and macOS, write permission is removed for the User,\n    Group and Others. The read and execute permissions are preserved.\n\n    Args:\n        path: The path to remove the write permissions of.\n    \"\"\"\n    import stat  # noqa: PLC0415\n\n    current_permissions = stat.S_IMODE(os.lstat(path).st_mode)\n    disable_writing = ~stat.S_IWUSR &amp; ~stat.S_IWGRP &amp; ~stat.S_IWOTH\n    os.chmod(path, current_permissions &amp; disable_writing)  # noqa: PTH101\n</code></pre>"},{"location":"api/utils/#msl.io.utils.run_as_admin","title":"run_as_admin","text":"<pre><code>run_as_admin(\n    args=None,\n    *,\n    executable=None,\n    cwd=None,\n    capture_stderr=False,\n    blocking=True,\n    show=False,\n    **kwargs,\n)\n</code></pre> <p>Run a process as an administrator and return its output.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>PathLike | Sequence[PathLike] | None</code> <p>A sequence of program arguments or else a command string. Providing a sequence of arguments is generally preferred, as it allows the subprocess to take care of any required escaping and quoting of arguments (e.g., to permit spaces in file names).</p> <code>None</code> <code>executable</code> <code>PathLike | None</code> <p>The executable to pass the <code>args</code> to.</p> <code>None</code> <code>cwd</code> <code>PathLike | None</code> <p>The working directory to use for the elevated process.</p> <code>None</code> <code>capture_stderr</code> <code>bool</code> <p>Whether to send the stderr stream to stdout.</p> <code>False</code> <code>blocking</code> <code>bool</code> <p>Whether to wait for the process to finish before returning to the calling program.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Whether to show the elevated console (Windows only). If <code>True</code>, the stdout stream of the process is not captured.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>If the current process already has admin privileges or if the operating system is not Windows then all additional keyword arguments are passed to subprocess.check_output. Otherwise, only a <code>timeout</code> keyword argument is used (Windows).</p> <code>{}</code> <p>Returns:</p> Type Description <code>int | bytes | Popen[Any]</code> <p>The returned object depends on whether the process is executed in blocking or non-blocking mode and whether Python is already running with admin privileges. If blocking, bytes are returned (the stdout stream of the process). If non-blocking, the returned object will either be the subprocess.Popen instance that is running the process (POSIX) or an int which is the process ID (Windows).</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def run_as_admin(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    args: PathLike | Sequence[PathLike] | None = None,\n    *,\n    executable: PathLike | None = None,\n    cwd: PathLike | None = None,\n    capture_stderr: bool = False,\n    blocking: bool = True,\n    show: bool = False,\n    **kwargs: Any,\n) -&gt; int | bytes | Popen[Any]:\n    \"\"\"Run a process as an administrator and return its output.\n\n    Args:\n        args: A sequence of program arguments or else a command string. Providing a sequence of\n            arguments is generally preferred, as it allows the subprocess to take care of any required\n            escaping and quoting of arguments (e.g., to permit spaces in file names).\n        executable: The executable to pass the `args` to.\n        cwd: The working directory to use for the elevated process.\n        capture_stderr: Whether to send the _stderr_ stream to _stdout_.\n        blocking: Whether to wait for the process to finish before returning to the calling program.\n        show: Whether to show the elevated console (Windows only). If `True`, the _stdout_ stream of\n            the process is not captured.\n        kwargs: If the current process already has admin privileges or if the operating system is\n            not Windows then all additional keyword arguments are passed to [subprocess.check_output][].\n            Otherwise, only a `timeout` keyword argument is used (Windows).\n\n    Returns:\n        The returned object depends on whether the process is executed in blocking or non-blocking mode\n            and whether Python is already running with admin privileges. If blocking, [bytes][] are returned\n            (the _stdout_ stream of the process). If non-blocking, the returned object will either be the\n            [subprocess.Popen][] instance that is running the process (POSIX) or an [int][] which is the\n            process ID (Windows).\n    \"\"\"\n    if not args and not executable:\n        msg = \"Must specify the args and/or an executable\"\n        raise ValueError(msg)\n\n    stderr = subprocess.STDOUT if capture_stderr else None\n    process = subprocess.check_output if blocking else subprocess.Popen\n\n    if is_admin():\n        if not args:\n            assert executable is not None  # noqa: S101\n            return process(executable, cwd=cwd, stderr=stderr, **kwargs)  # pyright: ignore[reportUnknownVariableType]\n        return process(args, executable=executable, cwd=cwd, stderr=stderr, **kwargs)  # pyright: ignore[reportUnknownVariableType]\n\n    exe = \"\" if executable is None else subprocess.list2cmdline([os.fsdecode(executable)])\n\n    if os.name != \"nt\":\n        if not args:\n            command = [\"sudo\", exe]\n        elif isinstance(args, (str, bytes, os.PathLike)):\n            command = [\"sudo\", exe, os.fsdecode(args)]\n        else:\n            command = [\"sudo\", exe, *list(map(os.fsdecode, args))]\n        return process(command, cwd=cwd, stderr=stderr, **kwargs)  # pyright: ignore[reportUnknownVariableType]\n\n    # Windows is more complicated\n\n    if args is None:\n        args = \"\"\n    elif isinstance(args, (bytes, os.PathLike)):\n        args = os.fsdecode(args)\n\n    if not isinstance(args, str):\n        args = subprocess.list2cmdline(args)\n\n    cwd = os.getcwd() if not cwd else os.fsdecode(cwd)  # noqa: PTH109\n\n    # the 'runas' verb starts in C:\\WINDOWS\\system32\n    cd = subprocess.list2cmdline([\"cd\", \"/d\", cwd, \"&amp;&amp;\"])\n\n    # check if a Python environment needs to be activated\n    activate = \"\"\n    if exe == sys.executable or args.startswith(sys.executable):\n        conda = os.getenv(\"CONDA_PREFIX\")  # conda\n        venv = os.getenv(\"VIRTUAL_ENV\")  # venv\n        if conda:\n            env = os.getenv(\"CONDA_DEFAULT_ENV\")\n            if not env:\n                msg = \"CONDA_DEFAULT_ENV environment variable does not exist\"\n                raise ValueError(msg)\n            if env == \"base\":\n                bat = Path(conda) / \"Scripts\" / \"activate.bat\"\n            else:\n                bat = Path(conda).parent.parent / \"Scripts\" / \"activate.bat\"\n            if not bat.is_file():\n                msg = f\"Cannot find {bat}\"\n                raise FileNotFoundError(msg)\n            activate = subprocess.list2cmdline([bat, env, \"&amp;&amp;\"])\n        elif venv:\n            bat = Path(venv) / \"Scripts\" / \"activate.bat\"\n            if not bat.is_file():\n                msg = f\"Cannot find {bat}\"\n                raise FileNotFoundError(msg)\n            activate = subprocess.list2cmdline([bat, \"&amp;&amp;\"])\n\n    # redirect stdout (stderr) to a file\n    redirect = None\n    stdout_file = None\n    if not show:\n        import tempfile  # noqa: PLC0415\n        import uuid  # noqa: PLC0415\n\n        stdout_file = Path(tempfile.gettempdir()) / str(uuid.uuid4())\n        r = [\"&gt;\", str(stdout_file)]\n        if capture_stderr:\n            r.append(\"2&gt;&amp;1\")\n        redirect = subprocess.list2cmdline(r)\n        if re.search(r\"\\d$\", args):\n            # this number is also considered as a file handle, so add a space\n            redirect = \" \" + redirect\n\n    # the string that is passed to cmd.exe\n    params = f'/S /C \"{cd} {activate} {exe} {args}\"{redirect}'\n\n    import ctypes  # noqa: PLC0415\n    from ctypes.wintypes import DWORD, HANDLE, HINSTANCE, HKEY, HWND, INT, LPCWSTR, ULONG  # noqa: PLC0415\n\n    class ShellExecuteInfoW(ctypes.Structure):\n        _fields_ = (  # pyright: ignore[reportUnannotatedClassAttribute]\n            (\"cbSize\", DWORD),\n            (\"fMask\", ULONG),\n            (\"hwnd\", HWND),\n            (\"lpVerb\", LPCWSTR),\n            (\"lpFile\", LPCWSTR),\n            (\"lpParameters\", LPCWSTR),\n            (\"lpDirectory\", LPCWSTR),\n            (\"nShow\", INT),\n            (\"hInstApp\", HINSTANCE),\n            (\"lpIDList\", ctypes.c_void_p),\n            (\"lpClass\", LPCWSTR),\n            (\"hkeyClass\", HKEY),\n            (\"dwHotKey\", DWORD),\n            (\"hIcon\", HANDLE),\n            (\"hProcess\", HANDLE),\n        )\n\n    sei = ShellExecuteInfoW()\n    sei.fMask = 0x00000040 | 0x00008000  # SEE_MASK_NOCLOSEPROCESS | SEE_MASK_NO_CONSOLE\n    sei.lpVerb = kwargs.get(\"verb\", \"runas\")  # change the verb when running the tests\n    sei.lpFile = \"cmd.exe\"\n    sei.lpParameters = params\n    sei.lpDirectory = f\"{cwd}\" if cwd else None\n    sei.nShow = int(show)\n    sei.cbSize = ctypes.sizeof(sei)\n    if not ctypes.windll.Shell32.ShellExecuteExW(ctypes.byref(sei)):\n        raise ctypes.WinError()\n\n    if not blocking:\n        return cast(\"int\", sei.hProcess)\n\n    kernel32 = ctypes.windll.kernel32\n    timeout = kwargs.get(\"timeout\", -1)  # INFINITE = -1\n    milliseconds = int(timeout * 1e3) if timeout &gt; 0 else timeout\n\n    ret = kernel32.WaitForSingleObject(sei.hProcess, milliseconds)\n    if ret == 0:  # WAIT_OBJECT_0\n        stdout = b\"\"\n        if stdout_file is not None and stdout_file.is_file():\n            stdout = stdout_file.read_bytes()\n            stdout_file.unlink()\n\n        code = DWORD()\n        if not kernel32.GetExitCodeProcess(sei.hProcess, ctypes.byref(code)):\n            raise ctypes.WinError()\n\n        if code.value != 0:\n            msg = ctypes.FormatError(code.value)\n            out_str = stdout.decode(\"utf-8\", \"ignore\").rstrip()\n            if show:\n                msg += \"\\nSet show=False to capture the stdout stream.\"\n            else:\n                if not capture_stderr:\n                    msg += \"\\nSet capture_stderr=True to see if more information is available.\"\n                if out_str:\n                    msg += f\"\\n{out_str}\"\n            raise ctypes.WinError(code.value, msg)\n\n        kernel32.CloseHandle(sei.hProcess)\n        return stdout\n\n    if ret == 0xFFFFFFFF:  # WAIT_FAILED  # noqa: PLR2004\n        raise ctypes.WinError()\n\n    if ret == 0x00000080:  # WAIT_ABANDONED  # noqa: PLR2004\n        msg = (\n            \"The specified object is a mutex object that was not \"\n            \"released by the thread that owned the mutex object before \"\n            \"the owning thread terminated. Ownership of the mutex \"\n            \"object is granted to the calling thread and the mutex state \"\n            \"is set to non-signalled. If the mutex was protecting persistent \"\n            \"state information, you should check it for consistency.\"\n        )\n    elif ret == 0x00000102:  # WAIT_TIMEOUT  # noqa: PLR2004\n        msg = f\"The timeout interval elapsed after {timeout} second(s) and the object's state is non-signalled.\"\n    else:\n        msg = f\"Unknown return value 0x{ret:x}\"\n\n    msg = f\"WaitForSingleObject: {msg}\"\n    raise OSError(msg)\n</code></pre>"},{"location":"api/utils/#msl.io.utils.search","title":"search","text":"<pre><code>search(\n    directory,\n    *,\n    depth=0,\n    include=None,\n    exclude=None,\n    flags=0,\n    ignore_os_error=True,\n    ignore_hidden_folders=True,\n    follow_symlinks=True,\n)\n</code></pre> <p>Search for files starting from a root directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>PathLike</code> <p>The root directory to begin searching for files.</p> required <code>depth</code> <code>int | None</code> <p>The number of sub-directories to recursively search for files. If <code>0</code>, only files in <code>directory</code> are searched, if <code>1</code> then files in <code>directory</code> and in one sub-directory are searched, etc. If <code>None</code>, search <code>directory</code> and recursively search all sub-directories.</p> <code>0</code> <code>include</code> <code>str | Pattern[str] | None</code> <p>A regular-expression pattern to use to include files. If <code>None</code>, no filtering is applied and all files are yielded (that are not <code>exclude</code>d). For example,</p> <ul> <li><code>r\"data\"</code> \u2192 find files with the word <code>data</code> in the file path</li> <li><code>r\"\\.png$\"</code> \u2192 find files with the extension <code>.png</code></li> <li><code>r\"\\.jpe*g$\"</code> \u2192 find files with the extension <code>.jpeg</code> or <code>.jpg</code></li> </ul> <code>None</code> <code>exclude</code> <code>str | Pattern[str] | None</code> <p>A regular-expression pattern to use to exclude files. The <code>exclude</code> pattern has precedence over the <code>include</code> pattern. For example,</p> <ul> <li><code>r\"bin\"</code> \u2192 exclude all files that contain the word <code>bin</code> in the file path</li> <li><code>r\"bin|lib\"</code> \u2192 exclude all files that contain the word <code>bin</code> or <code>lib</code> in the file path</li> </ul> <code>None</code> <code>flags</code> <code>int</code> <p>The flags to use to compile the regular-expression pattern (if it is a str type).</p> <code>0</code> <code>ignore_os_error</code> <code>bool</code> <p>Whether to ignore an OSError, if one occurs, while iterating through a directory. This type of error can occur if a directory does not have the appropriate read permission.</p> <code>True</code> <code>ignore_hidden_folders</code> <code>bool</code> <p>Whether to ignore a hidden directory from the search. A hidden directory starts with a <code>.</code> (a dot).</p> <code>True</code> <code>follow_symlinks</code> <code>bool</code> <p>Whether to search for files by following symbolic links.</p> <code>True</code> <p>Yields:</p> Type Description <code>Generator[Path]</code> <p>The path to a file.</p> Source code in <code>src/msl/io/utils.py</code> <pre><code>def search(  # noqa: C901, PLR0913\n    directory: PathLike,\n    *,\n    depth: int | None = 0,\n    include: str | re.Pattern[str] | None = None,\n    exclude: str | re.Pattern[str] | None = None,\n    flags: int = 0,\n    ignore_os_error: bool = True,\n    ignore_hidden_folders: bool = True,\n    follow_symlinks: bool = True,\n) -&gt; Generator[Path]:\n    r\"\"\"Search for files starting from a root directory.\n\n    Args:\n        directory: The root directory to begin searching for files.\n        depth: The number of sub-directories to recursively search for files.\n            If `0`, only files in `directory` are searched, if `1` then files in `directory`\n            and in one sub-directory are searched, etc. If `None`, search `directory` and\n            recursively search all sub-directories.\n        include: A regular-expression pattern to use to include files. If `None`, no filtering\n            is applied and all files are yielded (that are not `exclude`d). For example,\n\n            * `r\"data\"` &amp;#8594; find files with the word `data` in the file path\n            * `r\"\\.png$\"` &amp;#8594; find files with the extension `.png`\n            * `r\"\\.jpe*g$\"` &amp;#8594; find files with the extension `.jpeg` or `.jpg`\n\n        exclude: A regular-expression pattern to use to exclude files. The `exclude` pattern\n            has precedence over the `include` pattern. For example,\n\n            * `r\"bin\"` &amp;#8594; exclude all files that contain the word `bin` in the file path\n            * `r\"bin|lib\"` &amp;#8594; exclude all files that contain the word `bin` or `lib` in the file path\n\n        flags: The flags to use to compile the regular-expression pattern (if it is a [str][] type).\n        ignore_os_error: Whether to ignore an [OSError][], if one occurs, while iterating through a directory.\n            This type of error can occur if a directory does not have the appropriate read permission.\n        ignore_hidden_folders: Whether to ignore a hidden directory from the search. A hidden directory\n            starts with a `.` (a dot).\n        follow_symlinks: Whether to search for files by following symbolic links.\n\n    Yields:\n        The path to a file.\n    \"\"\"\n    if depth is not None and depth &lt; 0:\n        return\n\n    folder = Path(os.fsdecode(directory))\n\n    if ignore_hidden_folders and folder.name.startswith(\".\"):\n        logger.debug(\"search ignored hidden folder '%s'\", folder)\n        return\n\n    if isinstance(exclude, str):\n        exclude = re.compile(exclude, flags=flags)\n\n    if isinstance(include, str):\n        include = re.compile(include, flags=flags)\n\n    try:\n        with os.scandir(folder) as it:\n            for entry in it:\n                if entry.is_file():\n                    path = entry.path\n                    if exclude and exclude.search(path):\n                        logger.debug(\"search excluded file %r\", path)\n                    elif include is None or include.search(path):\n                        yield Path(path)\n                elif entry.is_dir(follow_symlinks=follow_symlinks):\n                    yield from search(\n                        entry,\n                        depth=None if depth is None else depth - 1,\n                        include=include,\n                        exclude=exclude,\n                        flags=flags,\n                        ignore_os_error=ignore_os_error,\n                        ignore_hidden_folders=ignore_hidden_folders,\n                        follow_symlinks=follow_symlinks,\n                    )\n    except OSError:\n        logger.debug(\"search raised OSError for '%s'\", folder)\n        if not ignore_os_error:\n            raise\n</code></pre>"},{"location":"api/utils/#msl.io.utils.send_email","title":"send_email","text":"<pre><code>send_email(\n    config, recipients, sender=None, subject=None, body=None\n)\n</code></pre> <p>Send an email.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PathLike | SupportsRead[AnyStr]</code> <p>An INI-style configuration file that contains information on how to send an email. There are two ways to send an email \u2014 Gmail API or SMTP server.</p> <p>An example INI file to use the Gmail API is the following (see GMail for more details). Although all key-value pairs are optional, a <code>[gmail]</code> section must exist to use the Gmail API. If a key is omitted, the value passed to GMail is <code>None</code></p> <pre><code>[gmail]\naccount = work\ncredentials = path/to/client_secrets.json\nscopes =\n    https://www.googleapis.com/auth/gmail.send\n    https://www.googleapis.com/auth/gmail.metadata\ndomain = @gmail.com\n</code></pre> <p>An example INI file for an SMTP server is the following. Only the <code>host</code> and <code>port</code> key-value pairs are required.</p> <pre><code>[smtp]\nhost = hostname or IP address of the SMTP server\nport = port number to connect to on the SMTP server (e.g., 25)\nstarttls = true|yes|1|on -or- false|no|0|off (default: false)\nusername = the username to authenticate with (default: None)\npassword = the password for username (default: None)\ndomain = @company.com (default: None)\n</code></pre> <p>Warning</p> <p>Since this information is specified in plain text in the configuration file, you should set the file permissions provided by your operating system to ensure that your authentication credentials are safe.</p> required <code>recipients</code> <code>str | list[str]</code> <p>The email address(es) of the recipient(s). Can omit the <code>@domain.com</code> part if a <code>domain</code> key is specified in the <code>config</code> file. Can use the value <code>'me'</code> if sending an email to yourself via Gmail.</p> required <code>sender</code> <code>str | None</code> <p>The email address of the sender. Can omit the <code>@domain.com</code> part if a <code>domain</code> key is specified in the <code>config</code> file. If <code>sender</code> is not specified, it becomes the value of the first <code>recipient</code> if using SMTP or the value <code>'me'</code> if using Gmail.</p> <code>None</code> <code>subject</code> <code>str | None</code> <p>The text to include in the subject field.</p> <code>None</code> <code>body</code> <code>str | None</code> <p>The text to include in the body of the email. The text can be enclosed in <code>&lt;html&gt;&lt;/html&gt;</code> tags to use HTML elements to format the message.</p> <code>None</code> Source code in <code>src/msl/io/utils.py</code> <pre><code>def send_email(\n    config: PathLike | SupportsRead[AnyStr],\n    recipients: str | list[str],\n    sender: str | None = None,\n    subject: str | None = None,\n    body: str | None = None,\n) -&gt; None:\n    \"\"\"Send an email.\n\n    Args:\n        config: An INI-style configuration file that contains information on how to send\n            an email. There are two ways to send an email &amp;mdash; Gmail API or SMTP server.\n\n            An example INI file to use the Gmail API is the following (see\n            [GMail][msl.io.google_api.GMail] for more details). Although all\n            key-value pairs are optional, a `[gmail]` section must exist to use\n            the Gmail API. If a key is omitted, the value passed to\n            [GMail][msl.io.google_api.GMail] is `None`\n\n            ```ini\n            [gmail]\n            account = work\n            credentials = path/to/client_secrets.json\n            scopes =\n                https://www.googleapis.com/auth/gmail.send\n                https://www.googleapis.com/auth/gmail.metadata\n            domain = @gmail.com\n            ```\n\n            An example INI file for an SMTP server is the following. Only the `host`\n            and `port` key-value pairs are required.\n\n            ```ini\n            [smtp]\n            host = hostname or IP address of the SMTP server\n            port = port number to connect to on the SMTP server (e.g., 25)\n            starttls = true|yes|1|on -or- false|no|0|off (default: false)\n            username = the username to authenticate with (default: None)\n            password = the password for username (default: None)\n            domain = @company.com (default: None)\n            ```\n\n            !!! warning\n                Since this information is specified in plain text in the configuration\n                file, you should set the file permissions provided by your operating\n                system to ensure that your authentication credentials are safe.\n\n        recipients: The email address(es) of the recipient(s). Can omit the `@domain.com`\n            part if a `domain` key is specified in the `config` file. Can use the value\n            `'me'` if sending an email to yourself via Gmail.\n        sender: The email address of the sender. Can omit the `@domain.com` part\n            if a `domain` key is specified in the `config` file. If `sender` is not\n            specified, it becomes the value of the first `recipient` if using SMTP\n            or the value `'me'` if using Gmail.\n        subject: The text to include in the subject field.\n        body: The text to include in the body of the email. The text can be enclosed\n            in `&lt;html&gt;&lt;/html&gt;` tags to use HTML elements to format the message.\n    \"\"\"\n    cfg = _prepare_email(config, recipients, sender)\n    if isinstance(cfg, _SMTPConfig):\n        from email.mime.multipart import MIMEMultipart  # noqa: PLC0415\n        from email.mime.text import MIMEText  # noqa: PLC0415\n        from smtplib import SMTP  # noqa: PLC0415\n\n        with SMTP(host=cfg.host, port=cfg.port) as server:\n            if cfg.starttls:\n                _ = server.ehlo()\n                _ = server.starttls()\n                _ = server.ehlo()\n            if cfg.username and cfg.password:\n                _ = server.login(cfg.username, cfg.password)\n            msg = MIMEMultipart()\n            msg[\"From\"] = cfg.frm\n            msg[\"To\"] = \", \".join(cfg.to)\n            msg[\"Subject\"] = subject or \"(no subject)\"\n            text = body or \"\"\n            subtype = \"html\" if text.startswith(\"&lt;html&gt;\") else \"plain\"\n            msg.attach(MIMEText(text, subtype))\n            _ = server.sendmail(cfg.frm, cfg.to, msg.as_string())\n    else:\n        with GMail(account=cfg.account, credentials=cfg.credentials, scopes=cfg.scopes) as gmail:\n            gmail.send(cfg.to, sender=cfg.frm, subject=subject, body=body)\n</code></pre>"},{"location":"items/attribute_access/","title":"Accessing Keys as Class Attributes","text":"<p>In order to access a dict key as a class attribute for a Group or a Metadata item or the fieldname of a numpy structured array for a Dataset, then the following naming rules must be followed:</p> <ul> <li> <p>the name matches the regular-expression pattern <code>^[a-zA-Z][a-zA-Z0-9_]*$</code> \u2014 which states that the name must begin with a letter and is followed by zero or more alphanumeric characters or underscores</p> </li> <li> <p>the name cannot be equal to any of the following:</p> <ul> <li>clear</li> <li>copy</li> <li>fromkeys</li> <li>get</li> <li>items</li> <li>keys</li> <li>pop</li> <li>popitem</li> <li>read_only</li> <li>setdefault</li> <li>update</li> <li>values</li> </ul> </li> </ul>"},{"location":"items/datasets/","title":"Datasets","text":"<p>A Dataset is analogous to a file in the file system used by an operating system and it is contained within a Group (analogous to a directory).</p> <p>A Dataset operates as a numpy ndarray with Metadata and it can be accessed in read-only mode or in read-write mode.</p> <p>Since a Dataset is a numpy ndarray, the attributes of ndarray are also valid for a Dataset. For example, suppose <code>my_dataset</code> is a Dataset</p> <pre><code>&gt;&gt;&gt; my_dataset\n&lt;Dataset '/my_dataset' shape=(5,) dtype='|V16' (2 metadata)&gt;\n&gt;&gt;&gt; print(my_dataset)\narray([(0.23, 1.27), (1.86, 2.74), (3.44, 2.91), (5.91, 1.83),\n       (8.73, 0.74)], dtype=[('x', '&lt;f8'), ('y', '&lt;f8')])\n</code></pre> <p>You can get the shape using</p> <pre><code>&gt;&gt;&gt; my_dataset.shape\n(5,)\n</code></pre> <p>or convert the data in the Dataset to a Python list using tolist</p> <pre><code>&gt;&gt;&gt; my_dataset.tolist()\n[(0.23, 1.27), (1.86, 2.74), (3.44, 2.91), (5.91, 1.83), (8.73, 0.74)]\n</code></pre> <p>To access the Metadata of a Dataset, you access the metadata attribute</p> <pre><code>&gt;&gt;&gt; my_dataset.metadata\n&lt;Metadata '/my_dataset' {'temperature': 20.13, 'humidity': 45.31}&gt;\n</code></pre> <p>You can access values of the Metadata as keys</p> <pre><code>&gt;&gt;&gt; my_dataset.metadata[\"humidity\"]\n45.31\n</code></pre> <p>or as attributes</p> <pre><code>&gt;&gt;&gt; my_dataset.metadata.temperature\n20.13\n</code></pre> <p>Depending on the dtype that was used to create the ndarray for the Dataset, the field names can also be accessed as class attributes. For example, you can access the fields in <code>my_dataset</code> as keys</p> <pre><code>&gt;&gt;&gt; my_dataset[\"x\"]\narray([0.23, 1.86, 3.44, 5.91, 8.73])\n</code></pre> <p>or as attributes</p> <pre><code>&gt;&gt;&gt; my_dataset.x\narray([0.23, 1.86, 3.44, 5.91, 8.73])\n</code></pre> <p>Note</p> <p>The returned object is a numpy ndarray and therefore does not contain Metadata.</p> <p>See Accessing Keys as Class Attributes for more information.</p> <p>You can also chain multiple attribute calls together. For example, to get the maximum <code>x</code> value in <code>my_dataset</code> you can use</p> <pre><code>&gt;&gt;&gt; print(my_dataset.x.max())\n8.73\n</code></pre>"},{"location":"items/datasets/#automatic-group-creation","title":"Automatic Group Creation","text":"<p>If you want to create a new Dataset and its parent Groups do not exist yet, the parent Groups are automatically created for you</p> <pre><code>&gt;&gt;&gt; voltages = root.create_dataset(\"a/b/c/voltages\", data=[3.2, 3.4, 3.3])\n&gt;&gt;&gt; root.a\n&lt;Group '/a' (2 groups, 1 datasets, 0 metadata)&gt;\n&gt;&gt;&gt; root.a.b\n&lt;Group '/a/b' (1 groups, 1 datasets, 0 metadata)&gt;\n&gt;&gt;&gt; root.a.b.c\n&lt;Group '/a/b/c' (0 groups, 1 datasets, 0 metadata)&gt;\n&gt;&gt;&gt; voltages\n&lt;Dataset '/a/b/c/voltages' shape=(3,) dtype='&lt;f8' (0 metadata)&gt;\n</code></pre>"},{"location":"items/datasets/#slicing-and-indexing","title":"Slicing and Indexing","text":"<p>Slicing and indexing a Dataset is a valid operation, but returns a numpy ndarray which does not contain Metadata.</p> <p>Consider <code>my_dataset</code> from above. You can slice it</p> <pre><code>&gt;&gt;&gt; my_dataset[::2]\narray([(0.23, 1.27), (3.44, 2.91), (8.73, 0.74)],\n      dtype=[('x', '&lt;f8'), ('y', '&lt;f8')])\n</code></pre> <p>or index it</p> <pre><code>&gt;&gt;&gt; print(my_dataset[2])\n(3.44, 2.91)\n</code></pre> <p>Since a numpy ndarray is returned, you are responsible for keeping track of the Metadata in slicing and indexing operations. For example, you can create a new Dataset from the subset by calling the create_dataset method</p> <pre><code>&gt;&gt;&gt; my_subset = root.create_dataset(\"my_subset\", data=my_dataset[::2], **my_dataset.metadata)\n&gt;&gt;&gt; my_subset\n&lt;Dataset '/my_subset' shape=(3,) dtype='|V16' (2 metadata)&gt;\n&gt;&gt;&gt; my_subset.data\narray([(0.23, 1.27), (3.44, 2.91), (8.73, 0.74)],\n      dtype=[('x', '&lt;f8'), ('y', '&lt;f8')])\n&gt;&gt;&gt; my_subset.metadata\n&lt;Metadata '/my_subset' {'temperature': 20.13, 'humidity': 45.31}&gt;\n</code></pre>"},{"location":"items/datasets/#arithmetic-operations","title":"Arithmetic Operations","text":"<p>Arithmetic operations are valid with a Dataset. The returned object is a Dataset with all Metadata copied and the name attribute updated to represent the operation that was performed.</p> <p>For example, consider a <code>temperatures</code> Dataset</p> <pre><code>&gt;&gt;&gt; temperatures\n&lt;Dataset '/temperatures' shape=(3,) dtype='&lt;f8' (1 metadata)&gt;\n&gt;&gt;&gt; temperatures.data\narray([19.8, 21.1, 20.5])\n&gt;&gt;&gt; temperatures.metadata.unit\n'\u00b0C'\n</code></pre> <p>and you wanted to add <code>1</code> to each temperature value, you can do the following</p> <pre><code>&gt;&gt;&gt; plus_1 = temperatures + 1\n&gt;&gt;&gt; plus_1\n&lt;Dataset 'add(/temperatures)' shape=(3,) dtype='&lt;f8' (1 metadata)&gt;\n&gt;&gt;&gt; plus_1.data\narray([20.8, 22.1, 21.5])\n&gt;&gt;&gt; plus_1.metadata.unit\n'\u00b0C'\n</code></pre> <p>Note</p> <p>The name attribute of the <code>plus_1</code> Dataset became <code>add(/temperatures)</code>.</p> <p>If the arithmetic operation involves multiple Datasets then the Metadata from the Datasets are merged into the resultant Dataset. Thus, if the Metadata for the individual Datasets have the same keys then only the key-value pair in the right-most Dataset in the operation will exist after the merger.</p> <p>For example, suppose you have two Datasets that contain the following information</p> <pre><code>&gt;&gt;&gt; dset1.data\narray([1., 2., 3.])\n&gt;&gt;&gt; dset1.metadata\n&lt;Metadata '/dset1' {'temperature': 20.3}&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; dset2.data\narray([4., 5., 6.])\n&gt;&gt;&gt; dset2.metadata\n&lt;Metadata '/dset2' {'temperature': 21.7}&gt;\n</code></pre> <p>You can add the Datasets, but the temperature value in <code>dset2</code> will be merged into the Metadata of <code>dset3</code> (since <code>dset2</code> is to the right of <code>dset1</code> in the addition operation)</p> <pre><code>&gt;&gt;&gt; dset3 = dset1 + dset2\n&gt;&gt;&gt; dset3\n&lt;Dataset 'add(/dset1,/dset2)' shape=(3,) dtype='&lt;f8' (1 metadata)&gt;\n&gt;&gt;&gt; dset3.metadata\n&lt;Metadata 'add(/dset1,/dset2)' {'temperature': 21.7}&gt;\n</code></pre> <p>If you want to preserve both temperature values, or change the resultant name, you can do so by explicitly creating a new Dataset</p> <pre><code>&gt;&gt;&gt; dset3 = root.create_dataset(\"dset3\", data=dset3, t1=dset1.metadata.temperature, t2=dset2.metadata.temperature)\n&gt;&gt;&gt; dset3\n&lt;Dataset '/dset3' shape=(3,) dtype='&lt;f8' (2 metadata)&gt;\n&gt;&gt;&gt; dset3.data\narray([5., 7., 9.])\n&gt;&gt;&gt; dset3.metadata\n&lt;Metadata '/dset3' {'t1': 20.3, 't2': 21.7}&gt;\n</code></pre>"},{"location":"items/datasets/#msl-io-dataset-logging","title":"Logging Records","text":"<p>The DatasetLogging class is a custom Dataset that is also a Handler which automatically appends logging records to the Dataset. See create_dataset_logging for more details.</p> <p>The following illustrates how to automatically append logging records to a Dataset</p> <pre><code>&gt;&gt;&gt; import logging\n&gt;&gt;&gt; from msl.io import JSONWriter\n&gt;&gt;&gt; logger = logging.getLogger(\"my_logger\")\n&gt;&gt;&gt; root = JSONWriter()\n&gt;&gt;&gt; log_dset = root.create_dataset_logging(\"log\")\n&gt;&gt;&gt; logger.info(\"hi\")\n&gt;&gt;&gt; logger.error(\"cannot do that!\")\n&gt;&gt;&gt; log_dset.data\narray([(..., 'INFO', 'my_logger', 'hi'),\n       (..., 'ERROR', 'my_logger', 'cannot do that!')],\n      dtype=[('asctime', 'O'), ('levelname', 'O'), ('name', 'O'), ('message', 'O')])\n</code></pre> <p>Get all <code>ERROR</code> logging records</p> <pre><code>&gt;&gt;&gt; errors = log_dset[log_dset[\"levelname\"] == \"ERROR\"]\n&gt;&gt;&gt; print(errors)\n[(..., 'ERROR', 'my_logger', 'cannot do that!')]\n</code></pre> <p>Stop the DatasetLogging instance from receiving logging records</p> <pre><code>&gt;&gt;&gt; log_dset.remove_handler()\n</code></pre> <p>Note</p> <p>When a file is read, it will load an object that was once a DatasetLogging as a Dataset (i.e., it will not be associated with new logging records that are emitted). If you want to convert the Dataset to be a DatasetLogging item again, so that logging records are once again appended to it when emitted, then you must call the require_dataset_logging method with the name argument equal to the value of the name attribute of the Dataset.</p>"},{"location":"items/groups/","title":"Groups","text":"<p>A Group is analogous to a directory in the file system used by an operating system. A Group can contain zero or more sub-Groups (sub-directories) and it can contain zero or more Datasets (analogous to a file). It uses a naming convention similar to UNIX file systems where every sub-directory is separated from its parent directory by the <code>/</code> character.</p> <p>From a Python perspective, a Group operates like a dict. The keys are the names of Group members, and the values are the members themselves (Group or Dataset objects).</p> <pre><code>&gt;&gt;&gt; print(root.tree())\n&lt;JSONWriter 'example.json' (3 groups, 1 datasets, 0 metadata)&gt;\n  &lt;Group '/a' (2 groups, 1 datasets, 0 metadata)&gt;\n    &lt;Group '/a/b' (1 groups, 1 datasets, 0 metadata)&gt;\n      &lt;Group '/a/b/c' (0 groups, 1 datasets, 0 metadata)&gt;\n        &lt;Dataset '/a/b/c/dset' shape=(100,) dtype='&lt;f8' (0 metadata)&gt;\n</code></pre> <p>A Group can either be in read-only mode</p> <pre><code>&gt;&gt;&gt; b.create_dataset('dset_b', data=[1, 2, 3, 4])\nTraceback (most recent call last):\n   ...\nValueError: Cannot modify &lt;Group '/a/b' (1 groups, 1 datasets, 0 metadata)&gt;. It is accessed in read-only mode.\n</code></pre> <p>or in read-write mode</p> <pre><code>&gt;&gt;&gt; b.read_only = False\n&gt;&gt;&gt; b.create_dataset('dset_b', data=[1, 2, 3, 4])\n&lt;Dataset '/a/b/dset_b' shape=(4,) dtype='&lt;f8' (0 metadata)&gt;\n</code></pre> <p>The items in a Group can be accessed as keys</p> <pre><code>&gt;&gt;&gt; root[\"a\"][\"b\"][\"c\"][\"dset\"]\n&lt;Dataset '/a/b/c/dset' shape=(100,) dtype='&lt;f8' (0 metadata)&gt;\n</code></pre> <p>or as class attributes</p> <pre><code>&gt;&gt;&gt; root.a.b.c.dset\n&lt;Dataset '/a/b/c/dset' shape=(100,) dtype='&lt;f8' (0 metadata)&gt;\n</code></pre> <p>See Accessing Keys as Class Attributes for more information.</p> <p>You can navigate through the tree by considering a Group to be an ancestor or descendant of other Groups</p> <pre><code>&gt;&gt;&gt; for ancestor in c.ancestors():\n...    print(ancestor)\n&lt;Group '/a/b' (1 groups, 2 datasets, 0 metadata)&gt;\n&lt;Group '/a' (2 groups, 2 datasets, 0 metadata)&gt;\n&lt;JSONWriter 'example.json' (3 groups, 2 datasets, 0 metadata)&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; for descendant in b.descendants():\n...    print(descendant)\n&lt;Group '/a/b/c' (0 groups, 1 datasets, 0 metadata)&gt;\n</code></pre> <p>If you want to create a new Group and its parent Groups do not exist yet, they are automatically created for you</p> <pre><code>&gt;&gt;&gt; root.read_only = False\n&gt;&gt;&gt; day = root.create_group(\"2025/07/28\")\n&gt;&gt;&gt; root[\"2025\"]\n&lt;Group '/2025' (2 groups, 0 datasets, 0 metadata)&gt;\n&gt;&gt;&gt; root[\"2025\"][\"07\"]\n&lt;Group '/2025/07' (1 groups, 0 datasets, 0 metadata)&gt;\n&gt;&gt;&gt; day\n&lt;Group '/2025/07/28' (0 groups, 0 datasets, 0 metadata)&gt;\n</code></pre>"},{"location":"items/metadata/","title":"Metadata","text":"<p>All Group and Dataset items contain Metadata. A Metadata item is a dict that can be made read only and it allows for accessing the keys of the dict as class attributes (see Accessing Keys as Class Attributes for more information).</p> <p>For example, suppose that a file is read that has the following Metadata</p> <pre><code>&gt;&gt;&gt; root.metadata\n&lt;Metadata '/' {'voltage': 1.2, 'voltage_unit': 'V'}&gt;\n</code></pre> <p>A value can be accessed by key</p> <pre><code>&gt;&gt;&gt; root.metadata[\"voltage\"]\n1.2\n</code></pre> <p>or as a class attribute</p> <pre><code>&gt;&gt;&gt; root.metadata.voltage\n1.2\n</code></pre>"},{"location":"items/metadata/#readwrite-mode","title":"Read/Write mode","text":"<p>When a file is read, the returned object is in read-only mode so you cannot modify the metadata</p> <pre><code>&gt;&gt;&gt; root.metadata.voltage = 7.64\nTraceback (most recent call last):\n    ...\nValueError: Cannot modify &lt;Metadata '/' {'voltage': 1.2, 'voltage_unit': 'V'}&gt;. It is accessed in read-only mode.\n</code></pre> <p>However, you can allow <code>metadata</code> to be modified by setting the read_only property to be <code>False</code></p> <pre><code>&gt;&gt;&gt; root.metadata.read_only = False\n</code></pre> <p>and then you can modify the values</p> <pre><code>&gt;&gt;&gt; root.metadata.voltage = 7.64\n&gt;&gt;&gt; root.add_metadata(current=10.3, current_unit=\"mA\")\n&gt;&gt;&gt; root.metadata\n&lt;Metadata '/' {'voltage': 7.64, 'voltage_unit': 'V', 'current': 10.3, 'current_unit': 'mA'}&gt;\n</code></pre>"},{"location":"items/metadata/#lists-tuples-and-arrays","title":"Lists, tuples and arrays","text":"<p>When the metadata value is a list, tuple or array, it will automatically be converted to a numpy ndarray. The dtype for a list, tuple will be object</p> <p><pre><code>&gt;&gt;&gt; root.metadata.temperatures = [20.1, 20.4, 19.8, 19.9]\n&gt;&gt;&gt; root.metadata.temperatures\narray([20.1, 20.4, 19.8, 19.9], dtype=object)\n</code></pre> <pre><code>&gt;&gt;&gt; root.metadata.humidities = (45.6, 46.1, 46.3, 44.7)\n&gt;&gt;&gt; root.metadata.humidities\narray([45.6, 46.1, 46.3, 44.7], dtype=object)\n</code></pre></p> <p>and the data type used by the array will be preserved</p> <pre><code>&gt;&gt;&gt; root.metadata.unsigned_integers = array.array(\"I\", [1, 2, 3, 4])\n&gt;&gt;&gt; root.metadata.unsigned_integers\narray([1, 2, 3, 4], dtype=uint32)\n</code></pre> <p>Setting the value to a numpy ndarray remains unchanged</p> <pre><code>&gt;&gt;&gt; root.metadata.eye = np.eye(2)\n&gt;&gt;&gt; root.metadata.eye\narray([[1., 0.],\n       [0., 1.]])\n</code></pre>"},{"location":"items/metadata/#dictionaries","title":"Dictionaries","text":"<p>When the metadata value is a dict it will be converted to a Metadata instance</p> <pre><code>&gt;&gt;&gt; root.metadata.nested = {\"one\": 1, \"two\": 2, \"three\": 3}\n&gt;&gt;&gt; root.metadata.nested\n&lt;Metadata '/' {'one': 1, 'two': 2, 'three': 3}&gt;\n&gt;&gt;&gt; root.metadata.nested.two\n2\n</code></pre>"},{"location":"readers/","title":"Readers","text":"<p>The following Readers are available</p> <ul> <li>DRSReader \u2014 Detector Responsivity System file format (MSL Light Standards)</li> <li>HDF5Reader \u2014 HDF5 file format</li> <li>JSONReader \u2014 Read a file created by JSONWriter</li> <li>RegularTransmittanceReader \u2014 Spectrophotometer transmittance file format (MSL Light Standards)</li> </ul>"},{"location":"readers/#msl.io.readers.detector_responsivity_system.DRSReader","title":"DRSReader","text":"<pre><code>DRSReader(file)\n</code></pre> <p>               Bases: <code>Reader</code></p> <p>Reader for the Detector Responsivity System in MSL Light Standards.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: ReadLike | str) -&gt; None:\n    \"\"\"Abstract base class for a [Reader][msl-io-readers].\n\n    Args:\n        file: The file to read.\n    \"\"\"\n    super().__init__(file)\n    self._file: ReadLike | str = file\n</code></pre>"},{"location":"readers/#msl.io.readers.detector_responsivity_system.DRSReader.can_read","title":"can_read  <code>staticmethod</code>","text":"<pre><code>can_read(file, **kwargs)\n</code></pre> <p>Checks if the first line starts with <code>DRS</code> and ends with <code>Shindo</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>ReadLike | str</code> <p>The file to check.</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are ignored.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether <code>file</code> was acquired in the DRS laboratory.</p> Source code in <code>src/msl/io/readers/detector_responsivity_system.py</code> <pre><code>@staticmethod\ndef can_read(file: ReadLike | str, **kwargs: Any) -&gt; bool:  # noqa: ARG004\n    \"\"\"Checks if the first line starts with `DRS` and ends with `Shindo`.\n\n    Args:\n        file: The file to check.\n        kwargs: All keyword arguments are ignored.\n\n    Returns:\n        Whether `file` was acquired in the DRS laboratory.\n    \"\"\"\n    if get_extension(file).lower() != \".dat\":\n        return False\n\n    line = get_lines(file, 1)[0]\n    if isinstance(line, bytes):\n        line = line.decode()\n\n    return line.startswith((\"DRS\", '\"DRS')) and line.endswith((\"Shindo\", 'Shindo\"'))\n</code></pre>"},{"location":"readers/#msl.io.readers.detector_responsivity_system.DRSReader.read","title":"read","text":"<pre><code>read(**kwargs)\n</code></pre> <p>Reads the <code>.DAT</code> and corresponding <code>.LOG</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>All keyword arguments are ignored.</p> <code>{}</code> Source code in <code>src/msl/io/readers/detector_responsivity_system.py</code> <pre><code>def read(self, **kwargs: Any) -&gt; None:  # noqa: ARG002\n    \"\"\"Reads the `.DAT` and corresponding `.LOG` file.\n\n    Args:\n        kwargs: All keyword arguments are ignored.\n    \"\"\"\n    self._default_alias: dict[str, str] = {\"l(nm)\": \"wavelength\"}\n\n    assert isinstance(self.file, str)  # noqa: S101\n    self._lines_dat: list[str] = get_lines(self.file, remove_empty_lines=True)\n    self._num_lines_dat: int = len(self._lines_dat)\n\n    self._lines_log: list[str] = get_lines(self.file[:-3] + \"LOG\", remove_empty_lines=True)\n    self._num_lines_log: int = len(self._lines_log)\n\n    num_runs = 0\n    self._index_dat: int = 0\n    self._index_log: int = 0\n    while self._index_dat &lt; self._num_lines_dat:\n        if \"DRS\" in self._lines_dat[self._index_dat]:\n            num_runs += 1\n            group = self.create_group(f\"run{num_runs}\")\n            self._read_run(group)\n        else:\n            self._index_dat += 1\n            self._index_log += 1\n</code></pre>"},{"location":"readers/#msl.io.readers.hdf5.HDF5Reader","title":"HDF5Reader","text":"<pre><code>HDF5Reader(file)\n</code></pre> <p>               Bases: <code>Reader</code></p> <p>Reader for the HDF5 file format.</p> <p>Info</p> <p>This Reader loads the entire HDF5 file in memory. If you need to use any of the more advanced features of an HDF5 file, it is best to directly load the file using h5py.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: ReadLike | str) -&gt; None:\n    \"\"\"Abstract base class for a [Reader][msl-io-readers].\n\n    Args:\n        file: The file to read.\n    \"\"\"\n    super().__init__(file)\n    self._file: ReadLike | str = file\n</code></pre>"},{"location":"readers/#msl.io.readers.hdf5.HDF5Reader.can_read","title":"can_read  <code>staticmethod</code>","text":"<pre><code>can_read(file, **kwargs)\n</code></pre> <p>Checks if the file has the HDF5 signature.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>ReadLike | str</code> <p>The file to check.</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are ignored.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the first 8 bytes are <code>\\x89HDF\\r\\n\\x1a\\n</code>.</p> Source code in <code>src/msl/io/readers/hdf5.py</code> <pre><code>@staticmethod\ndef can_read(file: ReadLike | str, **kwargs: Any) -&gt; bool:  # noqa: ARG004\n    r\"\"\"Checks if the file has the [HDF5 signature]{:target=\"_blank\"}.\n\n    [HDF5 signature]: https://support.hdfgroup.org/documentation/hdf5/latest/_f_m_t11.html#subsec_fmt11_boot_super\n\n    Args:\n        file: The file to check.\n        kwargs: All keyword arguments are ignored.\n\n    Returns:\n        Whether the first 8 bytes are `\\x89HDF\\r\\n\\x1a\\n`.\n    \"\"\"\n    if isinstance(file, (str, BufferedIOBase)):\n        return get_bytes(file, 8) == b\"\\x89HDF\\r\\n\\x1a\\n\"\n    return False\n</code></pre>"},{"location":"readers/#msl.io.readers.hdf5.HDF5Reader.read","title":"read","text":"<pre><code>read(**kwargs)\n</code></pre> <p>Reads the HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>All keyword arguments are passed to h5py.File.</p> <code>{}</code> Source code in <code>src/msl/io/readers/hdf5.py</code> <pre><code>def read(self, **kwargs: Any) -&gt; None:\n    \"\"\"Reads the [HDF5](https://www.hdfgroup.org/) file.\n\n    Args:\n        kwargs: All keyword arguments are passed to [h5py.File][]{:target=\"_blank\"}.\n    \"\"\"\n    if h5py is None:\n        msg = \"You must install h5py to read HDF5 files, run\\n  pip install h5py\"\n        raise ImportError(msg)\n\n    @no_type_check\n    def convert(name: str, obj: object) -&gt; None:\n        head, tail = os.path.split(name)\n        s = self[\"/\" + head] if head else self\n        if isinstance(obj, h5py.Dataset):\n            _ = s.create_dataset(tail, data=obj[:], **obj.attrs)\n        elif isinstance(obj, h5py.Group):\n            _ = s.create_group(tail, **obj.attrs)\n        else:\n            msg = f\"Should never get here, unhandled h5py object {obj}\"\n            raise TypeError(msg)\n\n    @no_type_check\n    def h5_open(f: BufferedIOBase) -&gt; None:\n        with h5py.File(f, mode=\"r\", **kwargs) as h5:\n            self.add_metadata(**h5.attrs)\n            h5.visititems(convert)  # cSpell: ignore visititems\n\n    # Calling h5py.File on a file on a mapped drive could raise\n    # an OSError. This occurred when a local folder was shared\n    # and then mapped on the same computer. Opening the file\n    # using open() and then passing in the file handle to\n    # h5py.File is more universal\n    if isinstance(self.file, BufferedIOBase):\n        h5_open(self.file)\n    elif isinstance(self.file, str):\n        with open(self.file, mode=\"rb\") as fp:  # noqa: PTH123\n            h5_open(fp)\n    else:\n        msg = f\"Should never get here, file type is {type(self.file)}\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"readers/#msl.io.readers.json_.JSONReader","title":"JSONReader","text":"<pre><code>JSONReader(file)\n</code></pre> <p>               Bases: <code>Reader</code></p> <p>Read a file that was created by JSONWriter.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: ReadLike | str) -&gt; None:\n    \"\"\"Abstract base class for a [Reader][msl-io-readers].\n\n    Args:\n        file: The file to read.\n    \"\"\"\n    super().__init__(file)\n    self._file: ReadLike | str = file\n</code></pre>"},{"location":"readers/#msl.io.readers.json_.JSONReader.can_read","title":"can_read  <code>staticmethod</code>","text":"<pre><code>can_read(file, **kwargs)\n</code></pre> <p>Checks if the file was created by JSONWriter.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>ReadLike | str</code> <p>The file to check.</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are passed to get_lines.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the text <code>MSL JSONWriter</code> is in the first line of the file.</p> Source code in <code>src/msl/io/readers/json_.py</code> <pre><code>@staticmethod\ndef can_read(file: ReadLike | str, **kwargs: Any) -&gt; bool:\n    \"\"\"Checks if the file was created by [JSONWriter][msl.io.writers.json_.JSONWriter].\n\n    Args:\n        file: The file to check.\n        kwargs: All keyword arguments are passed to [get_lines][msl.io.utils.get_lines].\n\n    Returns:\n        Whether the text `MSL JSONWriter` is in the first line of the file.\n    \"\"\"\n    text: bytes | str\n    if isinstance(file, (str, BufferedIOBase)):\n        text = get_bytes(file, 21, 34)\n    else:\n        text = get_lines(file, 1, **kwargs)[0][20:34]\n\n    if isinstance(text, str):\n        text = text.encode()\n    return text == b\"MSL JSONWriter\"\n</code></pre>"},{"location":"readers/#msl.io.readers.json_.JSONReader.read","title":"read","text":"<pre><code>read(**kwargs)\n</code></pre> <p>Read the file that was created by JSONWriter.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Accepts <code>encoding</code> and <code>errors</code> keyword arguments which are passed to open. The default <code>encoding</code> value is <code>utf-8</code> and the default <code>errors</code> value is <code>strict</code>. All additional keyword arguments are passed to json.loads.</p> <code>{}</code> Source code in <code>src/msl/io/readers/json_.py</code> <pre><code>def read(self, **kwargs: Any) -&gt; None:  # noqa: C901\n    \"\"\"Read the file that was created by [JSONWriter][msl.io.writers.json_.JSONWriter].\n\n    Args:\n        kwargs:  Accepts `encoding` and `errors` keyword arguments which are passed to\n            [open][]. The default `encoding` value is `utf-8` and the default\n            `errors` value is `strict`. All additional keyword arguments are passed to\n            [json.loads][].\n    \"\"\"\n    open_kwargs = {\n        \"encoding\": kwargs.get(\"encoding\", \"utf-8\"),\n        \"errors\": kwargs.pop(\"errors\", \"strict\"),\n    }\n\n    if isinstance(self.file, str):\n        with open(self.file, mode=\"rt\", **open_kwargs) as fp:  # noqa: PTH123, UP015\n            _ = fp.readline()  # skip the first line\n            dict_ = json.loads(fp.read(), **kwargs)\n    else:\n        _ = self.file.readline()  # skip the first line\n        data = self.file.read()\n        if isinstance(data, bytes):\n            data = data.decode(**open_kwargs)\n        dict_ = json.loads(data, **kwargs)\n\n    def create_group(parent: Group | None, name: str, node: dict[str, Any]) -&gt; None:\n        group = self if parent is None else parent.create_group(name)\n        for key, value in node.items():\n            if not isinstance(value, dict):  # Metadata\n                group.metadata[key] = value\n            elif \"dtype\" in value and \"data\" in value:  # Dataset\n                kws: dict[str, Any] = {}\n                for d_key, d_val in value.items():  # pyright: ignore[reportUnknownVariableType]\n                    if d_key == \"data\":\n                        pass  # handled in the 'dtype' check\n                    elif d_key == \"dtype\":\n                        if isinstance(d_val, list):\n                            kws[\"data\"] = np.asarray(\n                                [tuple(row) for row in value[\"data\"]],  # pyright: ignore[reportUnknownVariableType, reportUnknownArgumentType]\n                                dtype=[tuple(item) for item in d_val],  # pyright: ignore[reportUnknownVariableType, reportUnknownArgumentType]\n                            )\n                        else:\n                            kws[\"data\"] = np.asarray(value[\"data\"], dtype=d_val)  # pyright: ignore[reportUnknownArgumentType]\n                    else:  # Metadata\n                        kws[d_key] = d_val\n                _ = group.create_dataset(key, **kws)\n            else:  # use recursion to create a sub-Group\n                create_group(group, key, value)  # pyright: ignore[reportUnknownArgumentType]\n\n    # create the root group\n    create_group(None, \"\", dict_)\n</code></pre>"},{"location":"readers/#msl.io.readers.spectrophotometer_trans_reader.RegularTransmittanceReader","title":"RegularTransmittanceReader","text":"<pre><code>RegularTransmittanceReader(file)\n</code></pre> <p>               Bases: <code>Reader</code></p> <p>Reader for Trans files from Light Standards at MSL.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: ReadLike | str) -&gt; None:\n    \"\"\"Abstract base class for a [Reader][msl-io-readers].\n\n    Args:\n        file: The file to read.\n    \"\"\"\n    super().__init__(file)\n    self._file: ReadLike | str = file\n</code></pre>"},{"location":"readers/#msl.io.readers.spectrophotometer_trans_reader.RegularTransmittanceReader.can_read","title":"can_read  <code>staticmethod</code>","text":"<pre><code>can_read(file, **kwargs)\n</code></pre> <p>Checks is the file extension is .dat and the filename starts with 'Trans'.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>ReadLike | str</code> <p>file to be read</p> required <code>kwargs</code> <code>Any</code> <p>All keyword arguments are ignored.</p> <code>{}</code> Source code in <code>src/msl/io/readers/spectrophotometer_trans_reader.py</code> <pre><code>@staticmethod\ndef can_read(file: ReadLike | str, **kwargs: Any) -&gt; bool:  # noqa: ARG004\n    \"\"\"Checks is the file extension is .dat and the filename starts with 'Trans'.\n\n    Args:\n        file: file to be read\n        kwargs: All keyword arguments are ignored.\n    \"\"\"\n    filename = get_basename(file)\n    return filename.startswith(\"Trans_\") and filename.endswith((\".dat\", \".DAT\"))\n</code></pre>"},{"location":"readers/#msl.io.readers.spectrophotometer_trans_reader.RegularTransmittanceReader.read","title":"read","text":"<pre><code>read(**kwargs)\n</code></pre> <p>Reads the data in the corresponding log file.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>All keyword arguments are ignored.</p> <code>{}</code> Source code in <code>src/msl/io/readers/spectrophotometer_trans_reader.py</code> <pre><code>def read(self, **kwargs: Any) -&gt; None:  # noqa: ARG002, C901, PLR0915\n    \"\"\"Reads the data in the corresponding log file.\n\n    Args:\n        kwargs: All keyword arguments are ignored.\n    \"\"\"\n    assert isinstance(self.file, str)  # noqa: S101\n    lines_log = get_lines(self.file[:-3] + \"log\", remove_empty_lines=True)\n    num_lines_log = len(lines_log)\n    group = self.create_group(\"trans_data\")\n    meta: dict[str, Any] = {}\n\n    meta[\"start_time\"] = self._convert_time(lines_log[0].split(\",\")[1], lines_log[1])\n    try:\n        meta[\"end_time\"] = self._convert_time(lines_log[-2].split(\",\")[1], lines_log[-1])\n    except IndexError:\n        meta[\"note\"] = \"No end time recorded.\"\n    this_line = 2\n\n    # Read temperature data out of celsius file\n    meta[\"avg_temp\"] = self._read_celsius_file()\n\n    # skip lines until we get to the wavelength information and save comment\n    comment: list[str] = []\n    while not lines_log[this_line].startswith(\"Wavelengths Settings:\"):\n        comment.append(lines_log[this_line])\n        this_line += 1\n\n    meta[\"comment\"] = \"\\n\".join(comment)\n    this_line += 1\n\n    if lines_log[this_line].startswith(\"Wavelengths read\"):\n        meta[\"wavelength input\"] = \"Read from text file\"\n    else:\n        meta[\"wavelength input\"] = \"Manually input\"\n\n    while not lines_log[this_line].startswith(\"TEST MODULE:\"):\n        this_line += 1\n\n    this_line += 1\n    if lines_log[this_line].startswith(\"DVM INIT\"):\n        meta[\"dvm init\"] = lines_log[this_line]\n\n    this_line += 2\n    if lines_log[this_line].startswith(\"DELAY\"):\n        meta[\"delay\"] = float(lines_log[this_line].split()[1])\n\n    this_line += 2\n    if lines_log[this_line].startswith(\"NUM SAMPLES\"):\n        n_samples = int(lines_log[this_line].split()[2])\n        meta[\"n_samples\"] = n_samples\n\n    wavelengths: list[str] = []\n    dark_current: list[float] = []\n    dark_current_u: list[float] = []\n    dark_current_m: list[float] = []\n    dark_current_m_u: list[float] = []\n    i0: list[float] = []\n    i0_u: list[float] = []\n    m0: list[float] = []\n    m0_u: list[float] = []\n    corr: list[float] = []\n    is_: list[tuple[float, ...]] = []\n    is_u: list[tuple[float, ...]] = []\n    is_m: list[tuple[float, ...]] = []\n    is_m_u: list[tuple[float, ...]] = []\n    is_corr: list[tuple[float, ...]] = []\n\n    n_measurements = 0\n\n    while this_line &lt; num_lines_log - 1:\n        if lines_log[this_line].startswith(\"Wavelength = \"):\n            n_measurements += 1\n            wavelengths.append(lines_log[this_line].split()[2])\n            i0.append(float(re.split(\"=|\\t\", lines_log[this_line + 1])[1]))\n            i0_u.append(float(re.split(\"=|\\t\", lines_log[this_line + 1])[3]))\n            m0.append(float(re.split(\"=|\\t\", lines_log[this_line + 2])[1]))\n            m0_u.append(float(re.split(\"=|\\t\", lines_log[this_line + 2])[3]))\n            corr.append(float(lines_log[this_line + 3].split(\"=\")[1]))\n            dark_current.append(float(re.split(\"=|\\t\", lines_log[this_line + 4])[1]))\n            dark_current_u.append(float(re.split(\"=|\\t\", lines_log[this_line + 4])[3]))\n            dark_current_m.append(float(re.split(\"=|\\t\", lines_log[this_line + 5])[1]))\n            dark_current_m_u.append(float(re.split(\"=|\\t\", lines_log[this_line + 5])[3]))\n            is_.append(tuple([float(item) for item in lines_log[this_line + 7].split(\"|\")[1:] if item]))\n            is_u.append(tuple([float(item) for item in lines_log[this_line + 8].split(\"|\")[1:] if item]))\n            is_m.append(tuple([float(item) for item in lines_log[this_line + 9].split(\"|\")[1:] if item]))\n            is_m_u.append(tuple([float(item) for item in lines_log[this_line + 10].split(\"|\")[1:] if item]))\n            is_corr.append(tuple([float(item) for item in lines_log[this_line + 11].split(\"|\")[1:] if item]))\n\n            this_line += 10\n\n        else:\n            this_line += 1\n\n    fieldnames = [\n        \"wavelength\",\n        \"signal\",\n        \"u_signal\",\n        \"mon_signal\",\n        \"u_mon_signal\",\n        \"correlation\",\n        \"dark_current\",\n        \"u_dark_current\",\n        \"mon_dark\",\n        \"u_mon_dark\",\n    ]\n    data = np.array(\n        list(\n            zip(\n                wavelengths,\n                i0,\n                i0_u,\n                m0,\n                m0_u,\n                corr,\n                dark_current,\n                dark_current_u,\n                dark_current_m,\n                dark_current_m_u,\n            )\n        ),\n        dtype=[(name, float) for name in fieldnames],\n    )\n    _ = group.create_dataset(\"data\", data=data, **meta)\n\n    fieldnames = [\n        \"Transmitted signal\",\n        \"u(Transmitted signal)\",\n        \"Monitor transmitted\",\n        \"u(Monitor transmitted)\",\n        \"Transmitted correlation\",\n    ]\n    d_names = [\"Is\", \"Is_u\", \"Is_M\", \"Is_M_u\", \"Is_corr\"]\n    sample_data = np.array(\n        (list(zip(*is_)), list(zip(*is_u)), list(zip(*is_m)), list(zip(*is_m_u)), list(zip(*is_corr)))\n    )\n    ind = 1\n    for sample in sample_data:\n        _ = group.create_dataset(d_names[ind - 1], data=np.array(sample), **meta)\n        ind += 1\n</code></pre>"},{"location":"readers/create/","title":"Create a New Reader","text":"<p>When adding a new Reader to the repository the following steps should be performed.</p> <p>uv is used as the package and project manager for <code>msl-io</code> development, it is recommended to install it.</p> <p>Note</p> <p>If you do not want to contribute your new Reader to the repository then you only need to write the code shown in Step 2 to use your Reader in your own software. Once you import your module in your code, your Reader will be registered and it will be used to read your data files.</p> <ol> <li> <p>Create a fork of the repository.</p> </li> <li> <p>Create a new Reader by following this template. Save it in the <code>src/msl/io/readers</code> folder.</p> <pre><code>from __future__ import annotations\n\n# It's a good idea to provide type annotations in your code\nfrom typing import TYPE_CHECKING\n\n# Import the necessary msl-io object to subclass\nfrom msl.io import Reader\n\nif TYPE_CHECKING:\n    from typing import Any\n\n    from msl.io.types import ReadLike\n\n\n# Sub-classing Reader will tell msl-io that your MyReader exists\nclass MyReader(Reader):\n    \"\"\"Name your class to be whatever you want, i.e., change MyReader.\"\"\"\n\n    @staticmethod\n    def can_read(file: ReadLike | str, **kwargs: Any) -&gt; bool:\n        \"\"\"This method answers the following question:\n\n        Given a file-like object (e.g., a file stream or a buffered reader)\n        or a file path, can your Reader read this file?\n\n        You must perform all the necessary checks that *uniquely* answers\n        this question. For example, checking that the file extension is a\n        particular value may not be unique enough.\n\n        The optional kwargs can be passed in via the msl.io.read() function.\n\n        This method must return a boolean:\n        True (can read) or False (cannot read)\n        \"\"\"\n\n    def read(self, **kwargs: Any) -&gt; None:\n        \"\"\"This method reads the data file(s).\n\n        The optional kwargs can be passed in via the msl.io.read() function.\n\n        Your Reader class is a Root object.\n\n        The file to read is available at self.file\n\n        To add metadata to Root use self.add_metadata()\n\n        To create a Group in Root use self.create_group()\n\n        To create a Dataset in Root use self.create_dataset()\n\n        This method should return None.\n        \"\"\"\n</code></pre> </li> <li> <p>Import your Reader in the <code>src/msl/io/readers/__init__.py</code> module. Follow what is done for the other Readers.</p> </li> <li> <p>Add an example data file to the <code>tests/samples</code> directory and add a test case to the <code>tests</code> directory. Make sure that your Reader is returned by calling the read function, using your example data file as the input, and that the information in the returned object is correct. Run the tests using <code>uv run pytest</code>.</p> </li> <li> <p>Lint <code>uv run ruff check</code>, format <code>uv run ruff format</code> and type check <code>uv run basedpyright</code> the code.</p> </li> <li> <p>Add the new Reader, alphabetically, to <code>docs/readers/index.md</code>. Follow what is done for the other Readers.</p> </li> <li> <p>Update <code>CHANGELOG.md</code> stating that you added this new Reader.</p> </li> <li> <p>Build the documentation <code>uv run mkdocs serve</code> and check that your Reader renders correctly.</p> </li> <li> <p>If running the tests pass and linting, formatting, type checking and building the documentation do not show errors/warnings then create a pull request.</p> </li> </ol>"},{"location":"writers/","title":"Writers","text":"<p>The following Writers are available</p> <ul> <li>HDF5Writer \u2014 HDF5 file format</li> <li>JSONWriter \u2014 JSON file format</li> </ul>"},{"location":"writers/#msl.io.writers.hdf5.HDF5Writer","title":"HDF5Writer","text":"<pre><code>HDF5Writer(file=None, **metadata)\n</code></pre> <p>               Bases: <code>Writer</code></p> <p>Writer for the HDF5 file format.</p> <p>You can use this Writer as a context manager, for example,</p> <pre><code>with HDF5Writer(\"my_file.h5\") as root:\n    root.create_dataset(\"dset\", data=[1, 2, 3])\n</code></pre> <p>This will automatically write <code>root</code> to the specified file when the with block exits.</p> <p>Info</p> <p>This Writer requires the h5py package to be installed.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: PathLike | WriteLike | None = None, **metadata: Any) -&gt; None:\n    \"\"\"Abstract base class for a [Writer][msl-io-writers].\n\n    Args:\n        file: The file to write the data to. Can also be specified in the [write][msl.io.base.Writer.write] method.\n        metadata: All keyword arguments are used as [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    super().__init__(file, **metadata)\n    self._file: PathLike | WriteLike | None = file\n    self._context_kwargs: dict[str, Any] = {}\n</code></pre>"},{"location":"writers/#msl.io.writers.hdf5.HDF5Writer.write","title":"write","text":"<pre><code>write(file=None, root=None, **kwargs)\n</code></pre> <p>Write to a HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | WriteLike | None</code> <p>The file to write a root to. If <code>None</code> then uses the value of <code>file</code> that was specified when HDF5Writer was instantiated. If a file-like object, it must be open for writing in binary I/O and it must have <code>read</code>, <code>write</code>, <code>seek</code>, <code>tell</code>, <code>truncate</code> and <code>flush</code> methods.</p> <code>None</code> <code>root</code> <code>Group | None</code> <p>Write <code>root</code> in HDF5 format. If <code>None</code> then write the Groups and Datasets in the HDF5Writer instance. This argument is useful when converting between different file formats.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>All additional keyword arguments are passed to h5py.File.</p> <code>{}</code> Source code in <code>src/msl/io/writers/hdf5.py</code> <pre><code>def write(  # noqa: C901, PLR0912, PLR0915\n    self, file: PathLike | WriteLike | None = None, root: Group | None = None, **kwargs: Any\n) -&gt; None:\n    \"\"\"Write to a [HDF5](https://www.hdfgroup.org/){:target=\"_blank\"} file.\n\n    Args:\n        file: The file to write a *root* to. If `None` then uses the value of\n            `file` that was specified when [HDF5Writer][msl.io.writers.hdf5.HDF5Writer] was instantiated.\n            If a file-like object, it must be open for writing in binary I/O and it must have `read`, `write`,\n            `seek`, `tell`, `truncate` and `flush` methods.\n        root: Write `root` in [HDF5](https://www.hdfgroup.org/){:target=\"_blank\"} format.\n            If `None` then write the [Group][msl.io.node.Group]s and [Dataset][msl.io.node.Dataset]s\n            in the [HDF5Writer][msl.io.writers.hdf5.HDF5Writer] instance. This argument is useful when\n            converting between different file formats.\n        kwargs: All additional keyword arguments are passed to [h5py.File][]{:target=\"_blank\"}.\n    \"\"\"\n    if h5py is None:\n        msg = \"You must install h5py to write HDF5 files, run\\n  pip install h5py\"\n        raise ImportError(msg)\n\n    if file is None:\n        file = self.file\n    if not file:\n        msg = \"You must specify a file to write the root to\"\n        raise ValueError(msg)\n\n    if root is None:\n        root = self\n    elif not isinstance(root, Group):  # pyright: ignore[reportUnnecessaryIsInstance]\n        msg = \"The root parameter must be a Group object\"  # type: ignore[unreachable]  # pyright: ignore[reportUnreachable]\n        raise TypeError(msg)\n\n    if \"mode\" not in kwargs:\n        kwargs[\"mode\"] = \"x\"  # Create file, fail if exists\n\n    @no_type_check\n    def check_ndarray_dtype(obj: Any) -&gt; Any:  # noqa: C901, PLR0911, PLR0912\n        if not isinstance(obj, np.ndarray):\n            return obj\n\n        # h5py variable-length string\n        v_str = h5py.special_dtype(vlen=str)\n\n        if obj.dtype.names is not None:\n            convert, dtype = False, []\n            for n in obj.dtype.names:\n                typ = obj.dtype.fields[n][0]\n                if isinstance(obj[n].item(0), str):\n                    dtype.append((n, v_str))\n                    convert = True\n                else:\n                    dtype.append((n, typ))\n            if convert:\n                return obj.astype(dtype=dtype)\n            return obj\n        if obj.dtype.char == \"U\":\n            return obj.astype(dtype=v_str)\n        if obj.dtype.char == \"O\":\n            has_complex = False\n            for item in obj.flat:\n                if isinstance(item, str):\n                    return obj.astype(dtype=\"S\")\n                if isinstance(item, np.complexfloating):\n                    has_complex = True\n                elif item is None:\n                    return obj  # let h5py raise the error that HDF5 does not support NULL\n            if has_complex:\n                return obj.astype(dtype=complex)\n            return obj.astype(dtype=float)\n        return obj\n\n    def meta_to_dict(metadata: Metadata) -&gt; dict[str, dict[str, Any] | Any]:\n        return {\n            k: meta_to_dict(v) if isinstance(v, Metadata) else check_ndarray_dtype(v) for k, v in metadata.items()\n        }\n\n    @no_type_check\n    def h5_open(f: BufferedIOBase) -&gt; None:\n        with h5py.File(f, **kwargs) as h5:\n            h5.attrs.update(**meta_to_dict(root.metadata))\n            for name, value in root.items():\n                if self.is_dataset(value):\n                    try:\n                        vertex = h5.create_dataset(name, data=value.data)\n                    except TypeError:\n                        vertex = h5.create_dataset(name, data=check_ndarray_dtype(value.data))\n                else:\n                    vertex = h5.create_group(name)\n                vertex.attrs.update(**meta_to_dict(value.metadata))\n\n    # Calling h5py.File to write to a file on a mapped drive could raise\n    # an OSError. This occurred when a local folder was shared and then\n    # mapped on the same computer. Opening the file using open() and then\n    # passing in the file handle to h5py.File is more universal\n    if isinstance(file, (bytes, str, os.PathLike)):\n        m = kwargs[\"mode\"]\n        if m in [\"x\", \"w-\"]:\n            if os.path.isfile(file) or is_file_readable(file):  # noqa: PTH113\n                msg = f\"File exists {file!r}\\nSpecify mode='w' if you want to overwrite it.\"\n                raise FileExistsError(msg)\n        elif m == \"r+\":\n            if not (os.path.isfile(file) or is_file_readable(file)):  # noqa: PTH113\n                msg = f\"File does not exist {file!r}\"\n                raise FileNotFoundError(msg)\n        elif m not in [\"w\", \"a\"]:\n            msg = f\"Invalid mode {m!r}\"\n            raise ValueError(msg)\n\n        with open(file, mode=\"w+b\") as fp:  # noqa: PTH123\n            h5_open(fp)\n    else:\n        h5_open(file)\n</code></pre>"},{"location":"writers/#msl.io.writers.json_.JSONWriter","title":"JSONWriter","text":"<pre><code>JSONWriter(file=None, **metadata)\n</code></pre> <p>               Bases: <code>Writer</code></p> <p>Writer for a JSON file format.</p> <p>You can use this Writer as a context manager, for example,</p> <pre><code>with JSONWriter(\"my_file.json\") as root:\n    root.update_context_kwargs(indent=4)\n    dset = root.create_dataset(\"dset\", data=[1, 2, 3])\n</code></pre> <p>This will automatically write <code>root</code> to the specified file using four spaces as the indentation level (instead of the default value of two spaces) when the with block exits.</p> Source code in <code>src/msl/io/base.py</code> <pre><code>def __init__(self, file: PathLike | WriteLike | None = None, **metadata: Any) -&gt; None:\n    \"\"\"Abstract base class for a [Writer][msl-io-writers].\n\n    Args:\n        file: The file to write the data to. Can also be specified in the [write][msl.io.base.Writer.write] method.\n        metadata: All keyword arguments are used as [Metadata][msl.io.metadata.Metadata].\n    \"\"\"\n    super().__init__(file, **metadata)\n    self._file: PathLike | WriteLike | None = file\n    self._context_kwargs: dict[str, Any] = {}\n</code></pre>"},{"location":"writers/#msl.io.writers.json_.JSONWriter.write","title":"write","text":"<pre><code>write(file=None, root=None, **kwargs)\n</code></pre> <p>Write to a JSON file.</p> <p>The first line in the output file contains a description that the file was created by the JSONWriter. It begins with a <code>#</code> and contains a version number.</p> <p>Version 1.0 specifications:</p> <ul> <li> <p>Use the dtype and data keys to uniquely identify a   JSON object as a Dataset.</p> </li> <li> <p>If a Metadata key has a value that is a   Metadata object then the key becomes the name   of a Group and the value becomes   Metadata of that Group.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>PathLike | WriteLike | None</code> <p>The file to write a root to. If <code>None</code> then uses the value of <code>file</code> that was specified when JSONWriter was instantiated.</p> <code>None</code> <code>root</code> <code>Group | None</code> <p>Write <code>root</code> in JSON format. If <code>None</code> then write the Groups and Datasets in the JSONWriter instance. This argument is useful when converting between different file formats.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Accepts <code>mode</code>, <code>encoding</code> and <code>errors</code> keyword arguments which are passed to open. The default <code>encoding</code> value is <code>utf-8</code> and the default <code>errors</code> value is <code>strict</code>. All additional keyword arguments are passed to json.dump. The default indentation level is <code>2</code>.</p> <code>{}</code> Source code in <code>src/msl/io/writers/json_.py</code> <pre><code>def write(  # noqa: C901, PLR0912, PLR0915\n    self, file: PathLike | WriteLike | None = None, root: Group | None = None, **kwargs: Any\n) -&gt; None:\n    \"\"\"Write to a [JSON](https://www.json.org/){:target=\"_blank\"} file.\n\n    The first line in the output file contains a description that the file was created by the\n    [JSONWriter][msl.io.writers.json_.JSONWriter]. It begins with a `#` and contains a version number.\n\n    Version 1.0 specifications:\n\n    * Use the *dtype* and *data* keys to uniquely identify a\n      [JSON](https://www.json.org/){:target=\"_blank\"} object as a [Dataset][msl.io.node.Dataset].\n\n    * If a [Metadata][msl.io.metadata.Metadata] *key* has a *value* that is a\n      [Metadata][msl.io.metadata.Metadata] object then the *key* becomes the name\n      of a [Group][msl.io.node.Group] and the *value* becomes\n      [Metadata][msl.io.metadata.Metadata] of that [Group][msl.io.node.Group].\n\n    Args:\n        file: The file to write a *root* to. If `None` then uses the value of\n            `file` that was specified when [JSONWriter][msl.io.writers.json_.JSONWriter] was instantiated.\n        root: Write `root` in [JSON](https://www.json.org/){:target=\"_blank\"} format.\n            If `None` then write the [Group][msl.io.node.Group]s and [Dataset][msl.io.node.Dataset]s\n            in the [JSONWriter][msl.io.writers.json_.JSONWriter] instance. This argument is useful when\n            converting between different file formats.\n        kwargs: Accepts `mode`, `encoding` and `errors` keyword arguments which are passed\n            to [open][]{:target=\"_blank\"}. The default `encoding` value is `utf-8` and the default\n            `errors` value is `strict`. All additional keyword arguments are passed to\n            [json.dump][]{:target=\"_blank\"}. The default indentation level is `2`.\n    \"\"\"\n    if file is None:\n        file = self.file\n\n    if not file:\n        msg = \"You must specify a file to write the root to\"\n        raise ValueError(msg)\n\n    if root is None:\n        root = self\n    elif not isinstance(root, Group):  # pyright: ignore[reportUnnecessaryIsInstance]\n        msg = \"The root parameter must be a Group object\"  # type: ignore[unreachable]  # pyright: ignore[reportUnreachable]\n        raise TypeError(msg)\n\n    def add_dataset(d: dict[str, Any], dataset: Dataset) -&gt; None:\n        if dataset.dtype.fields:\n            d[\"dtype\"] = np.array([[name, str(dtype)] for name, (dtype, _) in dataset.dtype.fields.items()])\n        else:\n            d[\"dtype\"] = dataset.dtype.str\n        d[\"data\"] = dataset.data\n\n    def meta_to_dict(metadata: Metadata) -&gt; dict[str, dict[str, Any] | Any]:\n        return {k: meta_to_dict(v) if isinstance(v, Metadata) else v for k, v in metadata.items()}\n\n    dict_ = dict(**meta_to_dict(root.metadata))\n\n    for name, value in root.items():\n        nodes = name.split(\"/\")\n        root_key = nodes[1]\n\n        if root_key not in dict_:\n            dict_[root_key] = dict(**meta_to_dict(value.metadata))\n            if root.is_dataset(value):\n                add_dataset(dict_[root_key], value)  # type: ignore[arg-type]  # pyright: ignore[reportArgumentType]\n\n        if len(nodes) &gt; 2:  # noqa: PLR2004\n            vertex = dict_[root_key]\n            for key in nodes[2:-1]:\n                vertex = vertex[key]\n\n            leaf_key = nodes[-1]\n            if leaf_key not in vertex:\n                vertex[leaf_key] = dict(**meta_to_dict(value.metadata))\n                if root.is_dataset(value):\n                    add_dataset(vertex[leaf_key], value)  # type: ignore[arg-type]  # pyright: ignore[reportArgumentType]\n\n    open_kwargs = {\n        \"mode\": kwargs.pop(\"mode\", None),\n        \"encoding\": kwargs.pop(\"encoding\", \"utf-8\"),\n        \"errors\": kwargs.pop(\"errors\", \"strict\"),\n    }\n\n    if isinstance(file, (bytes, str, os.PathLike)):\n        if not open_kwargs[\"mode\"]:\n            open_kwargs[\"mode\"] = \"w\"\n            if os.path.isfile(file) or is_file_readable(file):  # noqa: PTH113\n                msg = f\"File exists {file!r}\\nSpecify mode='w' if you want to overwrite it.\"\n                raise FileExistsError(msg)\n        elif open_kwargs[\"mode\"] == \"r\":\n            msg = f\"Invalid mode {open_kwargs['mode']!r}\"\n            raise ValueError(msg)\n        elif open_kwargs[\"mode\"] == \"a\":\n            open_kwargs[\"mode\"] = \"w\"\n\n    if \"indent\" not in kwargs:\n        kwargs[\"indent\"] = 2\n    if \"cls\" not in kwargs:\n        kwargs[\"cls\"] = _NumpyEncoder\n\n    # header =&gt; f\"#File created with: MSL {self.__class__.__name__} version 1.0\\n\"\n    #\n    # Don't use the above definition of 'header' since JSONWriter could be sub-classed\n    # and therefore the value of self.__class__.__name__ would change. The\n    # JSONReader.can_read() method expects the text 'MSL JSONWriter' to be in a\n    # specific location on the first line in the file.\n    header = \"#File created with: MSL JSONWriter version 1.0\\n\"\n\n    if isinstance(file, (bytes, str, os.PathLike)):\n        with open(file, **open_kwargs) as fp:  # pyright: ignore[reportUnknownVariableType]  # noqa: PTH123\n            _ = fp.write(header)  # pyright: ignore[reportUnknownVariableType, reportUnknownMemberType]\n            json.dump(dict_, fp, **kwargs)  # pyright: ignore[reportUnknownArgumentType]\n    elif isinstance(file, BufferedIOBase):\n        encoding = open_kwargs[\"encoding\"]\n        _ = file.write(header.encode(encoding))  # pyright: ignore[reportArgumentType]\n        _ = file.write(json.dumps(dict_, **kwargs).encode(encoding))  # pyright: ignore[reportArgumentType]\n    else:\n        _ = file.write(header)  # type: ignore[arg-type]  # pyright: ignore[reportArgumentType]\n        json.dump(dict_, file, **kwargs)  # type: ignore[arg-type]  # pyright: ignore[reportArgumentType]\n</code></pre>"},{"location":"writers/create/","title":"Create a New Writer","text":"<p>When adding a new Writer to the repository the following steps should be performed. You will also need to Create a New Reader.</p> <p>uv is used as the package and project manager for <code>msl-io</code> development, it is recommended to install it.</p> <ol> <li> <p>Create a fork of the repository.</p> </li> <li> <p>Create a new Writer by following this template. Save it in the <code>src/msl/io/writers</code> folder.</p> <pre><code>from __future__ import annotations\n\n# It's a good idea to provide type annotations in your code\nfrom typing import TYPE_CHECKING\n\n# Import the necessary msl-io objects\nfrom msl.io import Writer\n\nif TYPE_CHECKING:\n    from typing import Any\n\n    from msl.io import Group\n    from msl.io.types import PathLike, WriteLike\n\n\nclass MyWriter(Writer):\n    \"\"\"Name your class to be whatever you want, i.e., change MyWriter.\"\"\"\n\n    def write(\n        self, file: PathLike | WriteLike | None = None, root: Group | None = None, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Implement your write method with the above signature.\n\n        Args:\n            file: The file to write to. If `None` then uses the value of\n                `file` that was specified when `MyWriter` was instantiated.\n            root: Write `root` to the file. If None then write the Groups\n                and Datasets that were created using `MyWriter`.\n            kwargs: Optional keyword arguments.\n        \"\"\"\n</code></pre> </li> <li> <p>Import your Writer in the <code>src/msl/io/writers/__init__.py</code> and <code>src/msl/io/__init__.py</code> modules. Follow what is done for the other Writers.</p> </li> <li> <p>Add test cases to the <code>tests</code> directory to make sure that your Writer works as expected. It is recommended to try converting a Root object between your Writer and other Writers that are available to verify different file-format conversions. Also, look at the test modules that begin with <code>test_writer</code> for more examples. Run the tests using <code>uv run pytest</code>.</p> </li> <li> <p>Lint <code>uv run ruff check</code>, format <code>uv run ruff format</code> and type check <code>uv run basedpyright</code> the code.</p> </li> <li> <p>Add the new Writer, alphabetically, to <code>docs/writers/index.md</code>. Follow what is done for the other Writers.</p> </li> <li> <p>Update <code>CHANGELOG.md</code> stating that you added this new Writer.</p> </li> <li> <p>Build the documentation <code>uv run mkdocs serve</code> and check that your Writer renders correctly.</p> </li> <li> <p>If running the tests pass and linting, formatting, type checking and building the documentation do not show errors/warnings then create a pull request.</p> </li> </ol>"}]}